timestamp,run_name,branch,commit,article,query,max_iterations,meta_json
2025-08-16T07:47:33Z,exp-001,main,4c9f022f108f6cb362d7ea2d3e77cbaab275573b,/home/avreymi/code/QFS-code-and-evaluation/Query-Focused-Summarization/articals/Attention-Is-All-You-Need.pdf,What are the core innovations of the Transformer architecture and how do they replace recurrence?,,{}
2025-08-16T07:47:33Z,exp-001,main,4c9f022f108f6cb362d7ea2d3e77cbaab275573b,/home/avreymi/code/QFS-code-and-evaluation/Query-Focused-Summarization/articals/The Linear Representation Hypothesis.pdf,Summarize the linear representation hypothesis and its implications for neural network interpretability.,,{}
2025-08-16T08:13:42Z,exp-002,main,44f67b3b8816d5393c6e95f8d9cdc8dfcd3fae19,/home/avreymi/code/QFS-code-and-evaluation/Query-Focused-Summarization/articals/Attention-Is-All-You-Need.pdf,What are the core innovations of the Transformer architecture and how do they replace recurrence?,,{}
2025-08-16T08:13:42Z,exp-002,main,44f67b3b8816d5393c6e95f8d9cdc8dfcd3fae19,/home/avreymi/code/QFS-code-and-evaluation/Query-Focused-Summarization/articals/The Linear Representation Hypothesis.pdf,Summarize the linear representation hypothesis and its implications for neural network interpretability.,,{}
2025-08-16T16:33:39Z,first_run,main,44f67b3b8816d5393c6e95f8d9cdc8dfcd3fae19,/home/avreymi/code/QFS-code-and-evaluation/QFS-evaluation/articles/Attention-Is-All-You-Need.md,What are the core innovations of the Transformer architecture and how do they replace recurrence?,,{}
2025-08-16T16:33:39Z,first_run,main,44f67b3b8816d5393c6e95f8d9cdc8dfcd3fae19,/home/avreymi/code/QFS-code-and-evaluation/QFS-evaluation/articles/The-Linear-Representation-Hypothesis.md,Summarize the linear representation hypothesis and its implications for neural network interpretability.,,{}
2025-08-16T16:33:39Z,first_run,main,44f67b3b8816d5393c6e95f8d9cdc8dfcd3fae19,/home/avreymi/code/QFS-code-and-evaluation/QFS-evaluation/articles/Forget-What-You-Know-about-LLMs.md,Discuss the limitations of traditional evaluation methods for LLMs and propose alternative approaches.,,{}
2025-08-16T16:33:39Z,first_run,main,44f67b3b8816d5393c6e95f8d9cdc8dfcd3fae19,/home/avreymi/code/QFS-code-and-evaluation/QFS-evaluation/articles/Hierarchical-Reasoning-Model.md,Explain the hierarchical reasoning model and its significance in understanding complex systems.,,{}
2025-08-17T07:58:59Z,secode_run,main,44f67b3b8816d5393c6e95f8d9cdc8dfcd3fae19,/home/avreymi/code/QFS-code-and-evaluation/QFS-evaluation/articles/Attention-Is-All-You-Need.md,What are the core innovations of the Transformer architecture and how do they replace recurrence?,,"{""meta"": ""{'key':'value', 'key2':'value'}""}"
2025-08-17T07:58:59Z,secode_run,main,44f67b3b8816d5393c6e95f8d9cdc8dfcd3fae19,/home/avreymi/code/QFS-code-and-evaluation/QFS-evaluation/articles/The-Linear-Representation-Hypothesis.md,Summarize the linear representation hypothesis and its implications for neural network interpretability.,,"{""meta"": ""{'key':'value', 'key2':'value'}""}"
2025-08-17T07:58:59Z,secode_run,main,44f67b3b8816d5393c6e95f8d9cdc8dfcd3fae19,/home/avreymi/code/QFS-code-and-evaluation/QFS-evaluation/articles/Forget-What-You-Know-about-LLMs.md,Discuss the limitations of traditional evaluation methods for LLMs and propose alternative approaches.,,"{""meta"": ""{'key':'value', 'key2':'value'}""}"
2025-08-17T07:58:59Z,secode_run,main,44f67b3b8816d5393c6e95f8d9cdc8dfcd3fae19,/home/avreymi/code/QFS-code-and-evaluation/QFS-evaluation/articles/Hierarchical-Reasoning-Model.md,Explain the hierarchical reasoning model and its significance in understanding complex systems.,,"{""meta"": ""{'key':'value', 'key2':'value'}""}"
2025-08-17T12:46:49Z,secode_run,main,44f67b3b8816d5393c6e95f8d9cdc8dfcd3fae19,/home/avreymi/code/QFS-code-and-evaluation/QFS-evaluation/articles/Attention-Is-All-You-Need.md,What are the core innovations of the Transformer architecture and how do they replace recurrence?,7,{}
2025-08-19T18:02:42Z,paid_key,dev-avreymi,44f67b3b8816d5393c6e95f8d9cdc8dfcd3fae19,/home/avreymi/code/QFS-code-and-evaluation/QFS-evaluation/articles/Attention-Is-All-You-Need.md,What are the core innovations of the Transformer architecture and how do they replace recurrence?,,{}
2025-08-19T18:02:42Z,paid_key,dev-avreymi,44f67b3b8816d5393c6e95f8d9cdc8dfcd3fae19,/home/avreymi/code/QFS-code-and-evaluation/QFS-evaluation/articles/The-Linear-Representation-Hypothesis.md,Summarize the linear representation hypothesis and its implications for neural network interpretability.,,{}
2025-08-19T18:02:42Z,paid_key,dev-avreymi,44f67b3b8816d5393c6e95f8d9cdc8dfcd3fae19,/home/avreymi/code/QFS-code-and-evaluation/QFS-evaluation/articles/Forget-What-You-Know-about-LLMs.md,Discuss the limitations of traditional evaluation methods for LLMs and propose alternative approaches.,,{}
2025-08-19T18:05:26Z,paid_key_it_10,dev-avreymi,44f67b3b8816d5393c6e95f8d9cdc8dfcd3fae19,/home/avreymi/code/QFS-code-and-evaluation/QFS-evaluation/articles/Attention-Is-All-You-Need.md,What are the core innovations of the Transformer architecture and how do they replace recurrence?,10,{}
2025-08-19T18:02:42Z,paid_key,dev-avreymi,44f67b3b8816d5393c6e95f8d9cdc8dfcd3fae19,/home/avreymi/code/QFS-code-and-evaluation/QFS-evaluation/articles/Hierarchical-Reasoning-Model.md,Explain the hierarchical reasoning model and its significance in understanding complex systems.,,{}
2025-08-19T18:05:26Z,paid_key_it_10,dev-avreymi,44f67b3b8816d5393c6e95f8d9cdc8dfcd3fae19,/home/avreymi/code/QFS-code-and-evaluation/QFS-evaluation/articles/The-Linear-Representation-Hypothesis.md,Summarize the linear representation hypothesis and its implications for neural network interpretability.,10,{}
2025-08-19T18:05:26Z,paid_key_it_10,dev-avreymi,44f67b3b8816d5393c6e95f8d9cdc8dfcd3fae19,/home/avreymi/code/QFS-code-and-evaluation/QFS-evaluation/articles/Forget-What-You-Know-about-LLMs.md,Discuss the limitations of traditional evaluation methods for LLMs and propose alternative approaches.,10,{}
2025-08-19T18:05:26Z,paid_key_it_10,dev-avreymi,44f67b3b8816d5393c6e95f8d9cdc8dfcd3fae19,/home/avreymi/code/QFS-code-and-evaluation/QFS-evaluation/articles/Hierarchical-Reasoning-Model.md,Explain the hierarchical reasoning model and its significance in understanding complex systems.,10,{}
2025-08-19T18:27:02Z,paid_key_10_without_limit,dev-avreymi,74ba08f05ba272dd9e26ffe0db92446668818b35,/home/avreymi/code/QFS-code-and-evaluation/QFS-evaluation/articles/Attention-Is-All-You-Need.md,What are the core innovations of the Transformer architecture and how do they replace recurrence?,10,{}
2025-08-19T18:27:02Z,paid_key_10_without_limit,dev-avreymi,74ba08f05ba272dd9e26ffe0db92446668818b35,/home/avreymi/code/QFS-code-and-evaluation/QFS-evaluation/articles/The-Linear-Representation-Hypothesis.md,Summarize the linear representation hypothesis and its implications for neural network interpretability.,10,{}
2025-08-19T18:27:02Z,paid_key_10_without_limit,dev-avreymi,74ba08f05ba272dd9e26ffe0db92446668818b35,/home/avreymi/code/QFS-code-and-evaluation/QFS-evaluation/articles/Forget-What-You-Know-about-LLMs.md,Discuss the limitations of traditional evaluation methods for LLMs and propose alternative approaches.,10,{}
2025-08-19T18:27:02Z,paid_key_10_without_limit,dev-avreymi,74ba08f05ba272dd9e26ffe0db92446668818b35,/home/avreymi/code/QFS-code-and-evaluation/QFS-evaluation/articles/Hierarchical-Reasoning-Model.md,Explain the hierarchical reasoning model and its significance in understanding complex systems.,10,{}
2025-08-19T19:04:06Z,concurrency-test,main,44f67b3b8816d5393c6e95f8d9cdc8dfcd3fae19,/home/avreymi/code/QFS-code-and-evaluation/QFS-evaluation/articles/Attention-Is-All-You-Need.md,In what ways does the multi-head attention mechanism allow the model to jointly attend to information from different representational subspaces at various positions?,1,{}
2025-08-19T19:04:06Z,concurrency-test,main,44f67b3b8816d5393c6e95f8d9cdc8dfcd3fae19,/home/avreymi/code/QFS-code-and-evaluation/QFS-evaluation/articles/Attention-Is-All-You-Need.md,training methodologies and hardware configurations,1,{}
2025-08-19T19:04:06Z,concurrency-test,main,44f67b3b8816d5393c6e95f8d9cdc8dfcd3fae19,/home/avreymi/code/QFS-code-and-evaluation/QFS-evaluation/articles/Forget-What-You-Know-about-LLMs.md,Model scale as an indicator of overfitting sensitivity.,1,{}
2025-08-19T19:04:06Z,concurrency-test,main,44f67b3b8816d5393c6e95f8d9cdc8dfcd3fae19,/home/avreymi/code/QFS-code-and-evaluation/QFS-evaluation/articles/The-Linear-Representation-Hypothesis.md,Summarize the linear representation hypothesis and its implications for neural network interpretability.,1,{}
2025-08-19T19:04:06Z,concurrency-test,main,44f67b3b8816d5393c6e95f8d9cdc8dfcd3fae19,/home/avreymi/code/QFS-code-and-evaluation/QFS-evaluation/articles/Forget-What-You-Know-about-LLMs.md,"Summarize the article's findings on the comparative resilience of different LLM families, such as Llama versus others, to textual perturbations.",1,{}
2025-08-19T19:04:06Z,concurrency-test,main,44f67b3b8816d5393c6e95f8d9cdc8dfcd3fae19,/home/avreymi/code/QFS-code-and-evaluation/QFS-evaluation/articles/Hierarchical-Reasoning-Model.md,Explain the hierarchical reasoning model and its significance in understanding complex systems.,1,{}
2025-08-19T19:07:54Z,concurrency-requstes-02,main,44f67b3b8816d5393c6e95f8d9cdc8dfcd3fae19,/home/avreymi/code/QFS-code-and-evaluation/QFS-evaluation/articles/Attention-Is-All-You-Need.md,In what ways does the multi-head attention mechanism allow the model to jointly attend to information from different representational subspaces at various positions?,5,{}
2025-08-19T19:07:54Z,concurrency-requstes-02,main,44f67b3b8816d5393c6e95f8d9cdc8dfcd3fae19,/home/avreymi/code/QFS-code-and-evaluation/QFS-evaluation/articles/Forget-What-You-Know-about-LLMs.md,"Summarize the article's findings on the comparative resilience of different LLM families, such as Llama versus others, to textual perturbations.",5,{}
2025-08-19T19:07:54Z,concurrency-requstes-02,main,44f67b3b8816d5393c6e95f8d9cdc8dfcd3fae19,/home/avreymi/code/QFS-code-and-evaluation/QFS-evaluation/articles/Attention-Is-All-You-Need.md,training methodologies and hardware configurations,5,{}
2025-08-19T19:07:54Z,concurrency-requstes-02,main,44f67b3b8816d5393c6e95f8d9cdc8dfcd3fae19,/home/avreymi/code/QFS-code-and-evaluation/QFS-evaluation/articles/Forget-What-You-Know-about-LLMs.md,Model scale as an indicator of overfitting sensitivity.,5,{}
2025-08-19T19:07:54Z,concurrency-requstes-02,main,44f67b3b8816d5393c6e95f8d9cdc8dfcd3fae19,/home/avreymi/code/QFS-code-and-evaluation/QFS-evaluation/articles/The-Linear-Representation-Hypothesis.md,Summarize the linear representation hypothesis and its implications for neural network interpretability.,5,{}
2025-08-19T19:07:54Z,concurrency-requstes-02,main,44f67b3b8816d5393c6e95f8d9cdc8dfcd3fae19,/home/avreymi/code/QFS-code-and-evaluation/QFS-evaluation/articles/Hierarchical-Reasoning-Model.md,Explain the hierarchical reasoning model and its significance in understanding complex systems.,5,{}
2025-08-20T19:02:50Z,new_output,dev-avreymi,3b3d05aaa5656b0af0aa88fd436d382ea8e7c670,/home/avreymi/code/QFS-code-and-evaluation/QFS-evaluation/articles/Attention-Is-All-You-Need.md,In what ways does the multi-head attention mechanism allow the model to jointly attend to information from different representational subspaces at various positions?,,{}
2025-08-20T19:02:50Z,new_output,dev-avreymi,3b3d05aaa5656b0af0aa88fd436d382ea8e7c670,/home/avreymi/code/QFS-code-and-evaluation/QFS-evaluation/articles/Attention-Is-All-You-Need.md,training methodologies and hardware configurations,,{}
2025-08-20T19:02:50Z,new_output,dev-avreymi,3b3d05aaa5656b0af0aa88fd436d382ea8e7c670,/home/avreymi/code/QFS-code-and-evaluation/QFS-evaluation/articles/Forget-What-You-Know-about-LLMs.md,Model scale as an indicator of overfitting sensitivity.,,{}
2025-08-20T19:02:50Z,new_output,dev-avreymi,3b3d05aaa5656b0af0aa88fd436d382ea8e7c670,/home/avreymi/code/QFS-code-and-evaluation/QFS-evaluation/articles/Forget-What-You-Know-about-LLMs.md,"Summarize the article's findings on the comparative resilience of different LLM families, such as Llama versus others, to textual perturbations.",,{}
2025-08-20T19:02:50Z,new_output,dev-avreymi,3b3d05aaa5656b0af0aa88fd436d382ea8e7c670,/home/avreymi/code/QFS-code-and-evaluation/QFS-evaluation/articles/Hierarchical-Reasoning-Model.md,Explain the hierarchical reasoning model and its significance in understanding complex systems.,,{}
2025-08-20T19:02:50Z,new_output,dev-avreymi,3b3d05aaa5656b0af0aa88fd436d382ea8e7c670,/home/avreymi/code/QFS-code-and-evaluation/QFS-evaluation/articles/The-Linear-Representation-Hypothesis.md,Summarize the linear representation hypothesis and its implications for neural network interpretability.,,{}
2025-08-27T05:20:33Z,new_format,ACU,fbab11fe8b1e01f39d817ed38d2a71960775b545,/home/user/studies/QFS-evaluation/articles/Attention-Is-All-You-Need.md,training methodologies and hardware configurations,,{}
2025-08-27T05:20:33Z,new_format,ACU,fbab11fe8b1e01f39d817ed38d2a71960775b545,/home/user/studies/QFS-evaluation/articles/Forget-What-You-Know-about-LLMs.md,"Summarize the article's findings on the comparative resilience of different LLM families, such as Llama versus others, to textual perturbations.",,{}
2025-08-27T05:20:33Z,new_format,ACU,fbab11fe8b1e01f39d817ed38d2a71960775b545,/home/user/studies/QFS-evaluation/articles/Forget-What-You-Know-about-LLMs.md,Model scale as an indicator of overfitting sensitivity.,,{}
2025-08-27T05:20:33Z,new_format,ACU,fbab11fe8b1e01f39d817ed38d2a71960775b545,/home/user/studies/QFS-evaluation/articles/Hierarchical-Reasoning-Model.md,Explain the hierarchical reasoning model and its significance in understanding complex systems.,,{}
2025-08-27T05:20:33Z,new_format,ACU,fbab11fe8b1e01f39d817ed38d2a71960775b545,/home/user/studies/QFS-evaluation/articles/Attention-Is-All-You-Need.md,In what ways does the multi-head attention mechanism allow the model to jointly attend to information from different representational subspaces at various positions?,,{}
2025-08-27T05:20:33Z,new_format,ACU,fbab11fe8b1e01f39d817ed38d2a71960775b545,/home/user/studies/QFS-evaluation/articles/The-Linear-Representation-Hypothesis.md,Summarize the linear representation hypothesis and its implications for neural network interpretability.,,{}
2025-08-27T08:07:37Z,all_section,sections,5f7e5593c4131cbe67d15207eaa6e54344a79e7e,/home/user/studies/QFS-evaluation/articles/Forget-What-You-Know-about-LLMs.md,Model scale as an indicator of overfitting sensitivity.,,{}
2025-08-27T08:07:37Z,all_section,sections,5f7e5593c4131cbe67d15207eaa6e54344a79e7e,/home/user/studies/QFS-evaluation/articles/Forget-What-You-Know-about-LLMs.md,"Summarize the article's findings on the comparative resilience of different LLM families, such as Llama versus others, to textual perturbations.",,{}
2025-08-27T08:07:37Z,all_section,sections,5f7e5593c4131cbe67d15207eaa6e54344a79e7e,/home/user/studies/QFS-evaluation/articles/Attention-Is-All-You-Need.md,training methodologies and hardware configurations,,{}
2025-08-27T08:07:37Z,all_section,sections,5f7e5593c4131cbe67d15207eaa6e54344a79e7e,/home/user/studies/QFS-evaluation/articles/Hierarchical-Reasoning-Model.md,Explain the hierarchical reasoning model and its significance in understanding complex systems.,,{}
2025-08-27T08:07:37Z,all_section,sections,5f7e5593c4131cbe67d15207eaa6e54344a79e7e,/home/user/studies/QFS-evaluation/articles/The-Linear-Representation-Hypothesis.md,Summarize the linear representation hypothesis and its implications for neural network interpretability.,,{}
2025-08-27T08:07:37Z,all_section,sections,5f7e5593c4131cbe67d15207eaa6e54344a79e7e,/home/user/studies/QFS-evaluation/articles/Attention-Is-All-You-Need.md,In what ways does the multi-head attention mechanism allow the model to jointly attend to information from different representational subspaces at various positions?,,{}
2025-08-27T08:10:43Z,all_section,sections,5f7e5593c4131cbe67d15207eaa6e54344a79e7e,/home/user/studies/QFS-evaluation/articles/Attention-Is-All-You-Need.md,In what ways does the multi-head attention mechanism allow the model to jointly attend to information from different representational subspaces at various positions?,10,{}
2025-08-27T08:10:43Z,all_section,sections,5f7e5593c4131cbe67d15207eaa6e54344a79e7e,/home/user/studies/QFS-evaluation/articles/The-Linear-Representation-Hypothesis.md,Summarize the linear representation hypothesis and its implications for neural network interpretability.,10,{}
2025-08-27T08:10:43Z,all_section,sections,5f7e5593c4131cbe67d15207eaa6e54344a79e7e,/home/user/studies/QFS-evaluation/articles/Hierarchical-Reasoning-Model.md,Explain the hierarchical reasoning model and its significance in understanding complex systems.,10,{}
2025-08-27T08:10:43Z,all_section,sections,5f7e5593c4131cbe67d15207eaa6e54344a79e7e,/home/user/studies/QFS-evaluation/articles/Forget-What-You-Know-about-LLMs.md,Model scale as an indicator of overfitting sensitivity.,10,{}
2025-08-27T08:10:43Z,all_section,sections,5f7e5593c4131cbe67d15207eaa6e54344a79e7e,/home/user/studies/QFS-evaluation/articles/Attention-Is-All-You-Need.md,training methodologies and hardware configurations,10,{}
2025-08-27T08:10:43Z,all_section,sections,5f7e5593c4131cbe67d15207eaa6e54344a79e7e,/home/user/studies/QFS-evaluation/articles/Forget-What-You-Know-about-LLMs.md,"Summarize the article's findings on the comparative resilience of different LLM families, such as Llama versus others, to textual perturbations.",10,{}
2025-08-27T18:59:03Z,Hille-0.0,,,/home/user/studies/QFS-evaluation/articles/Attention-Is-All-You-Need.md,In what ways does the multi-head attention mechanism allow the model to jointly attend to information from different representational subspaces at various positions?,10,{}
2025-08-27T18:59:03Z,Hille-0.0,,,/home/user/studies/QFS-evaluation/articles/Forget-What-You-Know-about-LLMs.md,"Summarize the article's findings on the comparative resilience of different LLM families, such as Llama versus others, to textual perturbations.",10,{}
2025-08-27T18:59:03Z,Hille-0.0,,,/home/user/studies/QFS-evaluation/articles/Forget-What-You-Know-about-LLMs.md,Model scale as an indicator of overfitting sensitivity.,10,{}
2025-08-27T18:59:03Z,Hille-0.0,,,/home/user/studies/QFS-evaluation/articles/The-Linear-Representation-Hypothesis.md,Summarize the linear representation hypothesis and its implications for neural network interpretability.,10,{}
2025-08-27T18:59:03Z,Hille-0.0,,,/home/user/studies/QFS-evaluation/articles/Attention-Is-All-You-Need.md,training methodologies and hardware configurations,10,{}
2025-08-27T18:59:03Z,Hille-0.0,,,/home/user/studies/QFS-evaluation/articles/Hierarchical-Reasoning-Model.md,Explain the hierarchical reasoning model and its significance in understanding complex systems.,10,{}
2025-08-27T19:49:06Z,Hille-0-2-update-gemini-and-Q-prompt,,,/home/user/studies/QFS-evaluation/articles/Attention-Is-All-You-Need.md,The practical challenges and solutions for stabilizing dot-product attention.,20,"{""QuestionGeneratorModel"": ""gemini-2.5-pro""}"
2025-08-27T19:49:06Z,Hille-0-2-update-gemini-and-Q-prompt,,,/home/user/studies/QFS-evaluation/articles/Attention-Is-All-You-Need.md,Methods for encoding sequential order in non-recurrent architectures.,20,"{""QuestionGeneratorModel"": ""gemini-2.5-pro""}"
2025-08-27T19:49:06Z,Hille-0-2-update-gemini-and-Q-prompt,,,/home/user/studies/QFS-evaluation/articles/Evolutionary Computation LLM.md,The dual role of the LLM-EA synergy in the context of model and code security.,20,"{""QuestionGeneratorModel"": ""gemini-2.5-pro""}"
2025-08-27T19:49:06Z,Hille-0-2-update-gemini-and-Q-prompt,,,/home/user/studies/QFS-evaluation/articles/Evolutionary Computation LLM.md,Neural Architecture Search (NAS) as both a target for optimization and a domain for synergy.,20,"{""QuestionGeneratorModel"": ""gemini-2.5-pro""}"
2025-08-27T19:49:06Z,Hille-0-2-update-gemini-and-Q-prompt,,,/home/user/studies/QFS-evaluation/articles/Attention-Is-All-You-Need.md,Computational efficiency and theoretical limits of sequence modeling.,20,"{""QuestionGeneratorModel"": ""gemini-2.5-pro""}"
2025-08-27T19:49:06Z,Hille-0-2-update-gemini-and-Q-prompt,,,/home/user/studies/QFS-evaluation/articles/Forget-What-You-Know-about-LLMs.md,The paradox of high accuracy as an indicator of fragility.,20,"{""QuestionGeneratorModel"": ""gemini-2.5-pro""}"
2025-08-27T19:49:06Z,Hille-0-2-update-gemini-and-Q-prompt,,,/home/user/studies/QFS-evaluation/articles/Forget-What-You-Know-about-LLMs.md,Differential robustness across distinct LLM architectural families.,20,"{""QuestionGeneratorModel"": ""gemini-2.5-pro""}"
2025-08-27T19:49:06Z,Hille-0-2-update-gemini-and-Q-prompt,,,/home/user/studies/QFS-evaluation/articles/Forget-What-You-Know-about-LLMs.md,The paper's critique of a leaderboard-centric evaluation culture in NLP.,20,"{""QuestionGeneratorModel"": ""gemini-2.5-pro""}"
2025-08-27T19:49:06Z,Hille-0-2-update-gemini-and-Q-prompt,,,/home/user/studies/QFS-evaluation/articles/Forget-What-You-Know-about-LLMs.md,The methodology of using generative models to create adversarial evaluation datasets.,20,"{""QuestionGeneratorModel"": ""gemini-2.5-pro""}"
2025-08-27T19:49:06Z,Hille-0-2-update-gemini-and-Q-prompt,,,/home/user/studies/QFS-evaluation/articles/Attention-Is-All-You-Need.md,"Enhancing representational power through parallel, subspace-specific attention.",20,"{""QuestionGeneratorModel"": ""gemini-2.5-pro""}"
2025-08-27T19:49:06Z,Hille-0-2-update-gemini-and-Q-prompt,,,/home/user/studies/QFS-evaluation/articles/Evolutionary Computation LLM.md,The LLM as a substitute for evolutionary operators.,20,"{""QuestionGeneratorModel"": ""gemini-2.5-pro""}"
2025-08-27T19:49:06Z,Hille-0-2-update-gemini-and-Q-prompt,,,/home/user/studies/QFS-evaluation/articles/Learn Beyond The Answer.md,The role and limitations of proprietary expert models in synthetic data generation for LLM research.,20,"{""QuestionGeneratorModel"": ""gemini-2.5-pro""}"
2025-08-27T19:49:06Z,Hille-0-2-update-gemini-and-Q-prompt,,,/home/user/studies/QFS-evaluation/articles/Evolutionary Computation LLM.md,The application of multi-objective evolutionary principles throughout the surveyed research.,20,"{""QuestionGeneratorModel"": ""gemini-2.5-pro""}"
2025-08-27T19:49:06Z,Hille-0-2-update-gemini-and-Q-prompt,,,/home/user/studies/QFS-evaluation/articles/Attention-Is-All-You-Need.md,The role of regularization and optimization schemes in training large-scale models.,20,"{""QuestionGeneratorModel"": ""gemini-2.5-pro""}"
2025-08-27T19:49:06Z,Hille-0-2-update-gemini-and-Q-prompt,,,/home/user/studies/QFS-evaluation/articles/Hierarchical-Reasoning-Model.md,Emergent dimensionality hierarchy in a trained reasoning model as a parallel to the functional organization of the mouse cortex.,20,"{""QuestionGeneratorModel"": ""gemini-2.5-pro""}"
2025-08-27T19:49:06Z,Hille-0-2-update-gemini-and-Q-prompt,,,/home/user/studies/QFS-evaluation/articles/Evolutionary Computation LLM.md,The article's perspective on the theme of black-box optimization.,20,"{""QuestionGeneratorModel"": ""gemini-2.5-pro""}"
2025-08-27T19:49:06Z,Hille-0-2-update-gemini-and-Q-prompt,,,/home/user/studies/QFS-evaluation/articles/Forget-What-You-Know-about-LLMs.md,The impact of model scale on overfitting vulnerability.,20,"{""QuestionGeneratorModel"": ""gemini-2.5-pro""}"
2025-08-27T19:49:06Z,Hille-0-2-update-gemini-and-Q-prompt,,,/home/user/studies/QFS-evaluation/articles/Learn Beyond The Answer.md,"A framework for categorizing mathematical reasoning skills in language models, from basic forward reasoning to complex reflective capabilities.",20,"{""QuestionGeneratorModel"": ""gemini-2.5-pro""}"
2025-08-27T19:49:06Z,Hille-0-2-update-gemini-and-Q-prompt,,,/home/user/studies/QFS-evaluation/articles/Learn Beyond The Answer.md,Comparing instance-dimension vs. sequence-dimension data augmentation for enhancing LLM reasoning.,20,"{""QuestionGeneratorModel"": ""gemini-2.5-pro""}"
2025-08-27T19:49:06Z,Hille-0-2-update-gemini-and-Q-prompt,,,/home/user/studies/QFS-evaluation/articles/MemOS.md,"The role of structured metadata in enabling the scheduling, governance, and evolution of memory.",20,"{""QuestionGeneratorModel"": ""gemini-2.5-pro""}"
2025-08-27T19:49:06Z,Hille-0-2-update-gemini-and-Q-prompt,,,/home/user/studies/QFS-evaluation/articles/Hierarchical-Reasoning-Model.md,"An argument against the architectural and computational limitations of the Transformer/Chain-of-Thought paradigm for genuine, latent algorithmic reasoning.",20,"{""QuestionGeneratorModel"": ""gemini-2.5-pro""}"
2025-08-27T19:49:06Z,Hille-0-2-update-gemini-and-Q-prompt,,,/home/user/studies/QFS-evaluation/articles/Learn Beyond The Answer.md,Efficiency trade-offs in LLM data augmentation strategies.,20,"{""QuestionGeneratorModel"": ""gemini-2.5-pro""}"
2025-08-27T19:49:06Z,Hille-0-2-update-gemini-and-Q-prompt,,,/home/user/studies/QFS-evaluation/articles/Hierarchical-Reasoning-Model.md,"Novel training strategies for recurrent architectures, combining BPTT-free approximate gradients, deep supervision, and reinforcement learning for adaptive computation.",20,"{""QuestionGeneratorModel"": ""gemini-2.5-pro""}"
2025-08-27T19:49:06Z,Hille-0-2-update-gemini-and-Q-prompt,,,/home/user/studies/QFS-evaluation/articles/MemOS.md,The vision for a collaborative and decentralized memory sharing ecosystem.,20,"{""QuestionGeneratorModel"": ""gemini-2.5-pro""}"
2025-08-27T19:49:06Z,Hille-0-2-update-gemini-and-Q-prompt,,,/home/user/studies/QFS-evaluation/articles/Hierarchical-Reasoning-Model.md,"Mechanisms for achieving and leveraging effective computational depth, using ""hierarchical convergence"" to overcome the vanishing gradient and premature convergence problems in recurrent systems.",20,"{""QuestionGeneratorModel"": ""gemini-2.5-pro""}"
2025-08-27T19:49:06Z,Hille-0-2-update-gemini-and-Q-prompt,,,/home/user/studies/QFS-evaluation/articles/Hierarchical-Reasoning-Model.md,"Applying principles of hierarchical processing, temporal separation, and local credit assignment from neuroscience to engineer stable, deep reasoning in artificial neural networks.",20,"{""QuestionGeneratorModel"": ""gemini-2.5-pro""}"
2025-08-27T19:49:06Z,Hille-0-2-update-gemini-and-Q-prompt,,,/home/user/studies/QFS-evaluation/articles/Random Teachers are Good Teachers.md,How random teacher distillation pre-conditions a network for supervised training.,20,"{""QuestionGeneratorModel"": ""gemini-2.5-pro""}"
2025-08-27T19:49:06Z,Hille-0-2-update-gemini-and-Q-prompt,,,/home/user/studies/QFS-evaluation/articles/Random Teachers are Good Teachers.md,The impact of student initialization proximity to the teacher.,20,"{""QuestionGeneratorModel"": ""gemini-2.5-pro""}"
2025-08-27T19:49:06Z,Hille-0-2-update-gemini-and-Q-prompt,,,/home/user/studies/QFS-evaluation/articles/Random Teachers are Good Teachers.md,The nature of implicit regularization induced by the teacher-student dynamics.,20,"{""QuestionGeneratorModel"": ""gemini-2.5-pro""}"
2025-08-27T19:49:06Z,Hille-0-2-update-gemini-and-Q-prompt,,,/home/user/studies/QFS-evaluation/articles/Learn Beyond The Answer.md,"The interplay between data quantity, quality, and complexity in fine-tuning language models for mathematical tasks.",20,"{""QuestionGeneratorModel"": ""gemini-2.5-pro""}"
2025-08-27T19:49:06Z,Hille-0-2-update-gemini-and-Q-prompt,,,/home/user/studies/QFS-evaluation/articles/MemOS.md,Parallels between the MemOS architecture and cognitive models of memory.,20,"{""QuestionGeneratorModel"": ""gemini-2.5-pro""}"
2025-08-27T19:49:06Z,Hille-0-2-update-gemini-and-Q-prompt,,,/home/user/studies/QFS-evaluation/articles/MemOS.md,The framework for accountable and governed memory in multi-agent systems.,20,"{""QuestionGeneratorModel"": ""gemini-2.5-pro""}"
2025-08-27T19:49:06Z,Hille-0-2-update-gemini-and-Q-prompt,,,/home/user/studies/QFS-evaluation/articles/MemOS.md,Mechanisms for the lifecycle and cross-type transformation of memory.,20,"{""QuestionGeneratorModel"": ""gemini-2.5-pro""}"
2025-08-27T19:49:06Z,Hille-0-2-update-gemini-and-Q-prompt,,,/home/user/studies/QFS-evaluation/articles/Random Teachers are Good Teachers.md,The asymmetric nature of the loss landscape around a random initialization.,20,"{""QuestionGeneratorModel"": ""gemini-2.5-pro""}"
2025-08-27T19:49:06Z,Hille-0-2-update-gemini-and-Q-prompt,,,/home/user/studies/QFS-evaluation/articles/Reinforcement Learning for Reasoning in Small LLMs.md,The impact of output length constraints on the stability and efficacy of training LLMs for complex reasoning tasks.,20,"{""QuestionGeneratorModel"": ""gemini-2.5-pro""}"
2025-08-27T19:49:06Z,Hille-0-2-update-gemini-and-Q-prompt,,,/home/user/studies/QFS-evaluation/articles/Rethinking Training Signals in RLVR.md,"A critique of model-specific conclusions in RL research, using the Qwen model family as a case study for how easily performance gains can be achieved with spurious signals.",20,"{""QuestionGeneratorModel"": ""gemini-2.5-pro""}"
2025-08-27T19:49:06Z,Hille-0-2-update-gemini-and-Q-prompt,,,/home/user/studies/QFS-evaluation/articles/Random Teachers are Good Teachers.md,The role of the input data distribution in shaping the learned representations.,20,"{""QuestionGeneratorModel"": ""gemini-2.5-pro""}"
2025-08-27T19:49:06Z,Hille-0-2-update-gemini-and-Q-prompt,,,/home/user/studies/QFS-evaluation/articles/Rethinking Training Signals in RLVR.md,"The influence of a model's pretrained latent abilities (like code reasoning) on the outcomes of reinforcement learning, irrespective of the reward signal's quality.",20,"{""QuestionGeneratorModel"": ""gemini-2.5-pro""}"
2025-08-27T19:49:06Z,Hille-0-2-update-gemini-and-Q-prompt,,,/home/user/studies/QFS-evaluation/articles/Reinforcement Learning for Reasoning in Small LLMs.md,Causes of optimization instability and performance degradation during RL fine-tuning of small language models.,20,"{""QuestionGeneratorModel"": ""gemini-2.5-pro""}"
2025-08-27T19:49:06Z,Hille-0-2-update-gemini-and-Q-prompt,,,/home/user/studies/QFS-evaluation/articles/Reinforcement Learning for Reasoning in Small LLMs.md,A comparative economic analysis of achieving state-of-the-art reasoning performance in small vs. large language models.,20,"{""QuestionGeneratorModel"": ""gemini-2.5-pro""}"
2025-08-27T19:49:06Z,Hille-0-2-update-gemini-and-Q-prompt,,,/home/user/studies/QFS-evaluation/articles/Rethinking Training Signals in RLVR.md,"The role of optimization algorithm artifacts, specifically the clipping mechanism in GRPO, in creating a directional training signal from pure noise.",20,"{""QuestionGeneratorModel"": ""gemini-2.5-pro""}"
2025-08-27T19:49:06Z,Hille-0-2-update-gemini-and-Q-prompt,,,/home/user/studies/QFS-evaluation/articles/Rethinking Training Signals in RLVR.md,"The divergent behaviors of different model families (Qwen, Llama, OLMo) under identical RLVR training, highlighting a fundamental lack of technique generalization.",20,"{""QuestionGeneratorModel"": ""gemini-2.5-pro""}"
2025-08-27T19:49:06Z,Hille-0-2-update-gemini-and-Q-prompt,,,/home/user/studies/QFS-evaluation/articles/Task Singular Vectors.md,The application of Orthogonal Procrustes analysis to decorrelate task-specific feature bases.,20,"{""QuestionGeneratorModel"": ""gemini-2.5-pro""}"
2025-08-27T19:49:06Z,Hille-0-2-update-gemini-and-Q-prompt,,,/home/user/studies/QFS-evaluation/articles/Task Singular Vectors.md,The paper's empirical argument for eliminating the scaling coefficient hyperparameter.,20,"{""QuestionGeneratorModel"": ""gemini-2.5-pro""}"
2025-08-27T19:49:06Z,Hille-0-2-update-gemini-and-Q-prompt,,,/home/user/studies/QFS-evaluation/articles/Task Singular Vectors.md,The counter-intuitive necessity of low-rank approximation for effective interference reduction.,20,"{""QuestionGeneratorModel"": ""gemini-2.5-pro""}"
2025-08-27T19:49:06Z,Hille-0-2-update-gemini-and-Q-prompt,,,/home/user/studies/QFS-evaluation/articles/The-Linear-Representation-Hypothesis.md,How the proposed 'causal inner product' serves as a bridge to unify the conceptually separate input (embedding) and output (unembedding) representation spaces.,20,"{""QuestionGeneratorModel"": ""gemini-2.5-pro""}"
2025-08-27T19:49:06Z,Hille-0-2-update-gemini-and-Q-prompt,,,/home/user/studies/QFS-evaluation/articles/Reinforcement Learning for Reasoning in Small LLMs.md,Strategies for curating compact datasets by balancing problem difficulty to enhance RL training efficiency under resource constraints.,20,"{""QuestionGeneratorModel"": ""gemini-2.5-pro""}"
2025-08-27T19:49:06Z,Hille-0-2-update-gemini-and-Q-prompt,,,/home/user/studies/QFS-evaluation/articles/Rethinking Training Signals in RLVR.md,"An analysis of how superficial interventions, such as prompting or rewarding simple syntactic patterns, can elicit complex reasoning behaviors in certain models.",20,"{""QuestionGeneratorModel"": ""gemini-2.5-pro""}"
2025-08-27T19:49:06Z,Hille-0-2-update-gemini-and-Q-prompt,,,/home/user/studies/QFS-evaluation/articles/Reinforcement Learning for Reasoning in Small LLMs.md,The challenge of managing multilingual language drift when fine-tuning a foundational model for a monolingual reasoning task.,20,"{""QuestionGeneratorModel"": ""gemini-2.5-pro""}"
2025-08-27T19:49:06Z,Hille-0-2-update-gemini-and-Q-prompt,,,/home/user/studies/QFS-evaluation/articles/distillation of SOTA embedding models.md,The progressive constraint strategy in knowledge distillation.,20,"{""QuestionGeneratorModel"": ""gemini-2.5-pro""}"
2025-08-27T19:49:06Z,Hille-0-2-update-gemini-and-Q-prompt,,,/home/user/studies/QFS-evaluation/articles/Task Singular Vectors.md,An argument for structure-aware model manipulation over flattened parameter approaches.,20,"{""QuestionGeneratorModel"": ""gemini-2.5-pro""}"
2025-08-27T19:49:06Z,Hille-0-2-update-gemini-and-Q-prompt,,,/home/user/studies/QFS-evaluation/articles/distillation of SOTA embedding models.md,Self-distillation as a mechanism for post-hoc modality alignment.,20,"{""QuestionGeneratorModel"": ""gemini-2.5-pro""}"
2025-08-27T19:49:06Z,Hille-0-2-update-gemini-and-Q-prompt,,,/home/user/studies/QFS-evaluation/articles/The-Linear-Representation-Hypothesis.md,"Providing a single geometric framework that formally connects the subspace hypothesis (e.g., word2vec analogies) with the practical techniques of linear probing and activation steering.",20,"{""QuestionGeneratorModel"": ""gemini-2.5-pro""}"
2025-08-27T19:49:06Z,Hille-0-2-update-gemini-and-Q-prompt,,,/home/user/studies/QFS-evaluation/articles/distillation of SOTA embedding models.md,The paper's dual approach to the engineering problem of embedding dimensionality.,20,"{""QuestionGeneratorModel"": ""gemini-2.5-pro""}"
2025-08-27T19:49:06Z,Hille-0-2-update-gemini-and-Q-prompt,,,/home/user/studies/QFS-evaluation/articles/The-Linear-Representation-Hypothesis.md,"The derivation of a semantically meaningful geometry from the statistical properties of the model's full vocabulary, rather than from its training data distribution.",20,"{""QuestionGeneratorModel"": ""gemini-2.5-pro""}"
2025-08-27T19:49:06Z,Hille-0-2-update-gemini-and-Q-prompt,,,/home/user/studies/QFS-evaluation/articles/Task Singular Vectors.md,The geometric interpretation of task interference through singular vector alignment.,20,"{""QuestionGeneratorModel"": ""gemini-2.5-pro""}"
2025-08-27T19:49:06Z,Hille-0-2-update-gemini-and-Q-prompt,,,/home/user/studies/QFS-evaluation/articles/The-Linear-Representation-Hypothesis.md,"The paper as a solution to the geometric unidentifiability of representation spaces, resolved by imposing a causal structure.",20,"{""QuestionGeneratorModel"": ""gemini-2.5-pro""}"
2025-08-27T19:49:06Z,Hille-0-2-update-gemini-and-Q-prompt,,,/home/user/studies/QFS-evaluation/articles/distillation of SOTA embedding models.md,Rationale for the staged training methodology.,20,"{""QuestionGeneratorModel"": ""gemini-2.5-pro""}"
2025-08-27T19:49:06Z,Hille-0-2-update-gemini-and-Q-prompt,,,/home/user/studies/QFS-evaluation/articles/rStar-Math.md,The role of code execution as a verifier for synthetic reasoning data.,20,"{""QuestionGeneratorModel"": ""gemini-2.5-pro""}"
2025-08-27T19:49:06Z,Hille-0-2-update-gemini-and-Q-prompt,,,/home/user/studies/QFS-evaluation/articles/rStar-Math.md,The viability of self-evolution as an alternative to knowledge distillation for creating state-of-the-art training datasets.,20,"{""QuestionGeneratorModel"": ""gemini-2.5-pro""}"
2025-08-27T19:49:06Z,Hille-0-2-update-gemini-and-Q-prompt,,,/home/user/studies/QFS-evaluation/articles/The-Linear-Representation-Hypothesis.md,The article's use of counterfactuals as a formal language to ground and connect disparate intuitions about linear representations.,20,"{""QuestionGeneratorModel"": ""gemini-2.5-pro""}"
2025-08-27T19:49:06Z,Hille-0-2-update-gemini-and-Q-prompt,,,/home/user/studies/QFS-evaluation/articles/distillation of SOTA embedding models.md,Methodology for fusing knowledge from heterogeneous teacher models.,20,"{""QuestionGeneratorModel"": ""gemini-2.5-pro""}"
2025-08-27T19:49:06Z,Hille-0-2-update-gemini-and-Q-prompt,,,/home/user/studies/QFS-evaluation/articles/rStar-Math.md,The emergence of self-correction capabilities as an intrinsic byproduct of the MCTS-based reasoning process.,20,"{""QuestionGeneratorModel"": ""gemini-2.5-pro""}"
2025-08-27T19:49:06Z,Hille-0-2-update-gemini-and-Q-prompt,,,/home/user/studies/QFS-evaluation/articles/rStar-Math.md,Novel training methodologies for process reward models that bypass noisy score annotation.,20,"{""QuestionGeneratorModel"": ""gemini-2.5-pro""}"
2025-08-27T20:35:56Z,Hille-0-3-update-gemini-and-Q-prompt-shared-querys,,,/home/user/studies/QFS-evaluation/articles/Learn Beyond The Answer.md,"The interplay between data quantity, quality, and complexity in fine-tuning language models for mathematical tasks.",10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro""}"
2025-08-27T20:35:56Z,Hille-0-3-update-gemini-and-Q-prompt-shared-querys,,,/home/user/studies/QFS-evaluation/articles/Hierarchical-Reasoning-Model.md,A comparative economic analysis of achieving state-of-the-art reasoning performance in small vs. large language models.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro""}"
2025-08-27T20:35:56Z,Hille-0-3-update-gemini-and-Q-prompt-shared-querys,,,/home/user/studies/QFS-evaluation/articles/Random Teachers are Good Teachers.md,"An analysis of how superficial interventions, such as prompting or rewarding simple syntactic patterns, can elicit complex reasoning behaviors in certain models.",10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro""}"
2025-08-27T20:35:56Z,Hille-0-3-update-gemini-and-Q-prompt-shared-querys,,,/home/user/studies/QFS-evaluation/articles/Evolutionary Computation LLM.md,The methodology of using generative models to create adversarial evaluation datasets.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro""}"
2025-08-27T20:35:56Z,Hille-0-3-update-gemini-and-Q-prompt-shared-querys,,,/home/user/studies/QFS-evaluation/articles/Evolutionary Computation LLM.md,"An analysis of how superficial interventions, such as prompting or rewarding simple syntactic patterns, can elicit complex reasoning behaviors in certain models.",10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro""}"
2025-08-27T20:35:56Z,Hille-0-3-update-gemini-and-Q-prompt-shared-querys,,,/home/user/studies/QFS-evaluation/articles/Learn Beyond The Answer.md,A comparative economic analysis of achieving state-of-the-art reasoning performance in small vs. large language models.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro""}"
2025-08-27T20:35:56Z,Hille-0-3-update-gemini-and-Q-prompt-shared-querys,,,/home/user/studies/QFS-evaluation/articles/Learn Beyond The Answer.md,The role and limitations of proprietary expert models in synthetic data generation for LLM research.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro""}"
2025-08-27T20:35:56Z,Hille-0-3-update-gemini-and-Q-prompt-shared-querys,,,/home/user/studies/QFS-evaluation/articles/Reinforcement Learning for Reasoning in Small LLMs.md,A comparative economic analysis of achieving state-of-the-art reasoning performance in small vs. large language models.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro""}"
2025-08-27T20:35:56Z,Hille-0-3-update-gemini-and-Q-prompt-shared-querys,,,/home/user/studies/QFS-evaluation/articles/Reinforcement Learning for Reasoning in Small LLMs.md,The impact of output length constraints on the stability and efficacy of training LLMs for complex reasoning tasks.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro""}"
2025-08-27T20:35:56Z,Hille-0-3-update-gemini-and-Q-prompt-shared-querys,,,/home/user/studies/QFS-evaluation/articles/Reinforcement Learning for Reasoning in Small LLMs.md,The challenge of managing multilingual language drift when fine-tuning a foundational model for a monolingual reasoning task.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro""}"
2025-08-27T20:35:56Z,Hille-0-3-update-gemini-and-Q-prompt-shared-querys,,,/home/user/studies/QFS-evaluation/articles/MemOS.md,The framework for accountable and governed memory in multi-agent systems.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro""}"
2025-08-27T20:35:56Z,Hille-0-3-update-gemini-and-Q-prompt-shared-querys,,,/home/user/studies/QFS-evaluation/articles/MemOS.md,The vision for a collaborative and decentralized memory sharing ecosystem.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro""}"
2025-08-27T20:35:56Z,Hille-0-3-update-gemini-and-Q-prompt-shared-querys,,,/home/user/studies/QFS-evaluation/articles/Random Teachers are Good Teachers.md,"A reframing of non-contrastive self-supervised learning where implicit regularization from the teacher-student optimization dynamic, rather than explicit data augmentation, serves as the primary driver of feature learning.",10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro""}"
2025-08-27T20:35:56Z,Hille-0-3-update-gemini-and-Q-prompt-shared-querys,,,/home/user/studies/QFS-evaluation/articles/Attention-Is-All-You-Need.md,"Mechanisms for achieving and leveraging effective computational depth, using ""hierarchical convergence"" to overcome the vanishing gradient and premature convergence problems in recurrent systems.",10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro""}"
2025-08-27T20:35:56Z,Hille-0-3-update-gemini-and-Q-prompt-shared-querys,,,/home/user/studies/QFS-evaluation/articles/Attention-Is-All-You-Need.md,A comparative economic analysis of achieving state-of-the-art reasoning performance in small vs. large language models.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro""}"
2025-08-27T20:35:56Z,Hille-0-3-update-gemini-and-Q-prompt-shared-querys,,,/home/user/studies/QFS-evaluation/articles/Forget-What-You-Know-about-LLMs.md,"An analysis of how superficial interventions, such as prompting or rewarding simple syntactic patterns, can elicit complex reasoning behaviors in certain models.",10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro""}"
2025-08-27T20:35:56Z,Hille-0-3-update-gemini-and-Q-prompt-shared-querys,,,/home/user/studies/QFS-evaluation/articles/Forget-What-You-Know-about-LLMs.md,"The divergent behaviors of different model families (Qwen, Llama, OLMo) under identical RLVR training, highlighting a fundamental lack of technique generalization.",10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro""}"
2025-08-27T20:35:56Z,Hille-0-3-update-gemini-and-Q-prompt-shared-querys,,,/home/user/studies/QFS-evaluation/articles/Evolutionary Computation LLM.md,The role and limitations of proprietary expert models in synthetic data generation for LLM research.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro""}"
2025-08-27T20:35:56Z,Hille-0-3-update-gemini-and-Q-prompt-shared-querys,,,/home/user/studies/QFS-evaluation/articles/Attention-Is-All-You-Need.md,"An analysis of how superficial interventions, such as prompting or rewarding simple syntactic patterns, can elicit complex reasoning behaviors in certain models.",10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro""}"
2025-08-27T20:35:56Z,Hille-0-3-update-gemini-and-Q-prompt-shared-querys,,,/home/user/studies/QFS-evaluation/articles/Hierarchical-Reasoning-Model.md,"An analysis of how superficial interventions, such as prompting or rewarding simple syntactic patterns, can elicit complex reasoning behaviors in certain models.",10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro""}"
2025-08-27T20:35:56Z,Hille-0-3-update-gemini-and-Q-prompt-shared-querys,,,/home/user/studies/QFS-evaluation/articles/Reinforcement Learning for Reasoning in Small LLMs.md,Causes of optimization instability and performance degradation during RL fine-tuning of small language models.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro""}"
2025-08-27T20:35:56Z,Hille-0-3-update-gemini-and-Q-prompt-shared-querys,,,/home/user/studies/QFS-evaluation/articles/Random Teachers are Good Teachers.md,The mechanism by which initializing a student network in close proximity to a random teacher allows it to escape the teacher's poor minimum by navigating the flatter side of an asymmetric valley in the loss landscape.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro""}"
2025-08-27T20:35:56Z,Hille-0-3-update-gemini-and-Q-prompt-shared-querys,,,/home/user/studies/QFS-evaluation/articles/Task Singular Vectors.md,"Mechanisms for achieving and leveraging effective computational depth, using ""hierarchical convergence"" to overcome the vanishing gradient and premature convergence problems in recurrent systems.",10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro""}"
2025-08-27T20:35:56Z,Hille-0-3-update-gemini-and-Q-prompt-shared-querys,,,/home/user/studies/QFS-evaluation/articles/Rethinking Training Signals in RLVR.md,"The divergent behaviors of different model families (Qwen, Llama, OLMo) under identical RLVR training, highlighting a fundamental lack of technique generalization.",10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro""}"
2025-08-27T20:35:56Z,Hille-0-3-update-gemini-and-Q-prompt-shared-querys,,,/home/user/studies/QFS-evaluation/articles/Hierarchical-Reasoning-Model.md,"Mechanisms for achieving and leveraging effective computational depth, using ""hierarchical convergence"" to overcome the vanishing gradient and premature convergence problems in recurrent systems.",10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro""}"
2025-08-27T20:35:56Z,Hille-0-3-update-gemini-and-Q-prompt-shared-querys,,,/home/user/studies/QFS-evaluation/articles/Reinforcement Learning for Reasoning in Small LLMs.md,Strategies for curating compact datasets by balancing problem difficulty to enhance RL training efficiency under resource constraints.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro""}"
2025-08-27T20:35:56Z,Hille-0-3-update-gemini-and-Q-prompt-shared-querys,,,/home/user/studies/QFS-evaluation/articles/Forget-What-You-Know-about-LLMs.md,"A critique of model-specific conclusions in RL research, using the Qwen model family as a case study for how easily performance gains can be achieved with spurious signals.",10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro""}"
2025-08-27T20:35:56Z,Hille-0-3-update-gemini-and-Q-prompt-shared-querys,,,/home/user/studies/QFS-evaluation/articles/The-Linear-Representation-Hypothesis.md,"The derivation of a semantically meaningful geometry from the statistical properties of the model's full vocabulary, rather than from its training data distribution.",10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro""}"
2025-08-27T20:35:56Z,Hille-0-3-update-gemini-and-Q-prompt-shared-querys,,,/home/user/studies/QFS-evaluation/articles/Rethinking Training Signals in RLVR.md,"An analysis of how superficial interventions, such as prompting or rewarding simple syntactic patterns, can elicit complex reasoning behaviors in certain models.",10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro""}"
2025-08-27T20:35:56Z,Hille-0-3-update-gemini-and-Q-prompt-shared-querys,,,/home/user/studies/QFS-evaluation/articles/Rethinking Training Signals in RLVR.md,"The influence of a model's pretrained latent abilities (like code reasoning) on the outcomes of reinforcement learning, irrespective of the reward signal's quality.",10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro""}"
2025-08-27T20:35:56Z,Hille-0-3-update-gemini-and-Q-prompt-shared-querys,,,/home/user/studies/QFS-evaluation/articles/distillation of SOTA embedding models.md,A comparative economic analysis of achieving state-of-the-art reasoning performance in small vs. large language models.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro""}"
2025-08-27T20:35:56Z,Hille-0-3-update-gemini-and-Q-prompt-shared-querys,,,/home/user/studies/QFS-evaluation/articles/Task Singular Vectors.md,"The divergent behaviors of different model families (Qwen, Llama, OLMo) under identical RLVR training, highlighting a fundamental lack of technique generalization.",10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro""}"
2025-08-27T20:35:56Z,Hille-0-3-update-gemini-and-Q-prompt-shared-querys,,,/home/user/studies/QFS-evaluation/articles/distillation of SOTA embedding models.md,The role and limitations of proprietary expert models in synthetic data generation for LLM research.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro""}"
2025-08-27T20:35:56Z,Hille-0-3-update-gemini-and-Q-prompt-shared-querys,,,/home/user/studies/QFS-evaluation/articles/Task Singular Vectors.md,"The role of optimization algorithm artifacts, specifically the clipping mechanism in GRPO, in creating a directional training signal from pure noise.",10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro""}"
2025-08-27T20:35:56Z,Hille-0-3-update-gemini-and-Q-prompt-shared-querys,,,/home/user/studies/QFS-evaluation/articles/Rethinking Training Signals in RLVR.md,"The role of optimization algorithm artifacts, specifically the clipping mechanism in GRPO, in creating a directional training signal from pure noise.",10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro""}"
2025-08-27T20:35:56Z,Hille-0-3-update-gemini-and-Q-prompt-shared-querys,,,/home/user/studies/QFS-evaluation/articles/distillation of SOTA embedding models.md,"The interplay between data quantity, quality, and complexity in fine-tuning language models for mathematical tasks.",10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro""}"
2025-08-27T20:35:56Z,Hille-0-3-update-gemini-and-Q-prompt-shared-querys,,,/home/user/studies/QFS-evaluation/articles/The-Linear-Representation-Hypothesis.md,The article's use of counterfactuals as a formal language to ground and connect disparate intuitions about linear representations.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro""}"
2025-08-27T20:35:56Z,Hille-0-3-update-gemini-and-Q-prompt-shared-querys,,,/home/user/studies/QFS-evaluation/articles/The-Linear-Representation-Hypothesis.md,"The paper as a solution to the geometric unidentifiability of representation spaces, resolved by imposing a causal structure.",10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro""}"
2025-08-28T07:11:43Z,Hille-0-3-update-gemini-and-Q-prompt-shared-querys,,,/home/user/studies/QFS-evaluation/articles/Learn Beyond The Answer.md,The role and limitations of proprietary expert models in synthetic data generation for LLM research.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""delete ACU, remove the number of quastions.add the artical in end of prompt""}"
2025-08-28T07:11:43Z,Hille-0-3-update-gemini-and-Q-prompt-shared-querys,,,/home/user/studies/QFS-evaluation/articles/MemOS.md,The framework for accountable and governed memory in multi-agent systems.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""delete ACU, remove the number of quastions.add the artical in end of prompt""}"
2025-08-28T07:11:43Z,Hille-0-3-update-gemini-and-Q-prompt-shared-querys,,,/home/user/studies/QFS-evaluation/articles/Evolutionary Computation LLM.md,"An analysis of how superficial interventions, such as prompting or rewarding simple syntactic patterns, can elicit complex reasoning behaviors in certain models.",10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""delete ACU, remove the number of quastions.add the artical in end of prompt""}"
2025-08-28T07:11:43Z,Hille-0-3-update-gemini-and-Q-prompt-shared-querys,,,/home/user/studies/QFS-evaluation/articles/MemOS.md,The vision for a collaborative and decentralized memory sharing ecosystem.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""delete ACU, remove the number of quastions.add the artical in end of prompt""}"
2025-08-28T07:11:43Z,Hille-0-3-update-gemini-and-Q-prompt-shared-querys,,,/home/user/studies/QFS-evaluation/articles/Attention-Is-All-You-Need.md,"An analysis of how superficial interventions, such as prompting or rewarding simple syntactic patterns, can elicit complex reasoning behaviors in certain models.",10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""delete ACU, remove the number of quastions.add the artical in end of prompt""}"
2025-08-28T07:11:43Z,Hille-0-3-update-gemini-and-Q-prompt-shared-querys,,,/home/user/studies/QFS-evaluation/articles/Learn Beyond The Answer.md,A comparative economic analysis of achieving state-of-the-art reasoning performance in small vs. large language models.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""delete ACU, remove the number of quastions.add the artical in end of prompt""}"
2025-08-28T07:11:43Z,Hille-0-3-update-gemini-and-Q-prompt-shared-querys,,,/home/user/studies/QFS-evaluation/articles/Forget-What-You-Know-about-LLMs.md,"The divergent behaviors of different model families (Qwen, Llama, OLMo) under identical RLVR training, highlighting a fundamental lack of technique generalization.",10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""delete ACU, remove the number of quastions.add the artical in end of prompt""}"
2025-08-28T07:11:43Z,Hille-0-3-update-gemini-and-Q-prompt-shared-querys,,,/home/user/studies/QFS-evaluation/articles/Hierarchical-Reasoning-Model.md,"An analysis of how superficial interventions, such as prompting or rewarding simple syntactic patterns, can elicit complex reasoning behaviors in certain models.",10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""delete ACU, remove the number of quastions.add the artical in end of prompt""}"
2025-08-28T07:11:43Z,Hille-0-3-update-gemini-and-Q-prompt-shared-querys,,,/home/user/studies/QFS-evaluation/articles/Evolutionary Computation LLM.md,The methodology of using generative models to create adversarial evaluation datasets.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""delete ACU, remove the number of quastions.add the artical in end of prompt""}"
2025-08-28T07:11:43Z,Hille-0-3-update-gemini-and-Q-prompt-shared-querys,,,/home/user/studies/QFS-evaluation/articles/Evolutionary Computation LLM.md,The role and limitations of proprietary expert models in synthetic data generation for LLM research.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""delete ACU, remove the number of quastions.add the artical in end of prompt""}"
2025-08-28T07:11:43Z,Hille-0-3-update-gemini-and-Q-prompt-shared-querys,,,/home/user/studies/QFS-evaluation/articles/Hierarchical-Reasoning-Model.md,A comparative economic analysis of achieving state-of-the-art reasoning performance in small vs. large language models.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""delete ACU, remove the number of quastions.add the artical in end of prompt""}"
2025-08-28T07:11:43Z,Hille-0-3-update-gemini-and-Q-prompt-shared-querys,,,/home/user/studies/QFS-evaluation/articles/Forget-What-You-Know-about-LLMs.md,"An analysis of how superficial interventions, such as prompting or rewarding simple syntactic patterns, can elicit complex reasoning behaviors in certain models.",10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""delete ACU, remove the number of quastions.add the artical in end of prompt""}"
2025-08-28T07:11:43Z,Hille-0-3-update-gemini-and-Q-prompt-shared-querys,,,/home/user/studies/QFS-evaluation/articles/Attention-Is-All-You-Need.md,A comparative economic analysis of achieving state-of-the-art reasoning performance in small vs. large language models.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""delete ACU, remove the number of quastions.add the artical in end of prompt""}"
2025-08-28T07:11:43Z,Hille-0-3-update-gemini-and-Q-prompt-shared-querys,,,/home/user/studies/QFS-evaluation/articles/Reinforcement Learning for Reasoning in Small LLMs.md,A comparative economic analysis of achieving state-of-the-art reasoning performance in small vs. large language models.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""delete ACU, remove the number of quastions.add the artical in end of prompt""}"
2025-08-28T07:11:43Z,Hille-0-3-update-gemini-and-Q-prompt-shared-querys,,,/home/user/studies/QFS-evaluation/articles/Rethinking Training Signals in RLVR.md,"The role of optimization algorithm artifacts, specifically the clipping mechanism in GRPO, in creating a directional training signal from pure noise.",10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""delete ACU, remove the number of quastions.add the artical in end of prompt""}"
2025-08-28T07:11:43Z,Hille-0-3-update-gemini-and-Q-prompt-shared-querys,,,/home/user/studies/QFS-evaluation/articles/Reinforcement Learning for Reasoning in Small LLMs.md,Strategies for curating compact datasets by balancing problem difficulty to enhance RL training efficiency under resource constraints.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""delete ACU, remove the number of quastions.add the artical in end of prompt""}"
2025-08-28T07:11:43Z,Hille-0-3-update-gemini-and-Q-prompt-shared-querys,,,/home/user/studies/QFS-evaluation/articles/The-Linear-Representation-Hypothesis.md,"The derivation of a semantically meaningful geometry from the statistical properties of the model's full vocabulary, rather than from its training data distribution.",10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""delete ACU, remove the number of quastions.add the artical in end of prompt""}"
2025-08-28T07:11:43Z,Hille-0-3-update-gemini-and-Q-prompt-shared-querys,,,/home/user/studies/QFS-evaluation/articles/Task Singular Vectors.md,"The divergent behaviors of different model families (Qwen, Llama, OLMo) under identical RLVR training, highlighting a fundamental lack of technique generalization.",10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""delete ACU, remove the number of quastions.add the artical in end of prompt""}"
2025-08-28T07:11:43Z,Hille-0-3-update-gemini-and-Q-prompt-shared-querys,,,/home/user/studies/QFS-evaluation/articles/Task Singular Vectors.md,"Mechanisms for achieving and leveraging effective computational depth, using ""hierarchical convergence"" to overcome the vanishing gradient and premature convergence problems in recurrent systems.",10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""delete ACU, remove the number of quastions.add the artical in end of prompt""}"
2025-08-28T07:11:43Z,Hille-0-3-update-gemini-and-Q-prompt-shared-querys,,,/home/user/studies/QFS-evaluation/articles/Hierarchical-Reasoning-Model.md,"Mechanisms for achieving and leveraging effective computational depth, using ""hierarchical convergence"" to overcome the vanishing gradient and premature convergence problems in recurrent systems.",10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""delete ACU, remove the number of quastions.add the artical in end of prompt""}"
2025-08-28T07:11:43Z,Hille-0-3-update-gemini-and-Q-prompt-shared-querys,,,/home/user/studies/QFS-evaluation/articles/Attention-Is-All-You-Need.md,"Mechanisms for achieving and leveraging effective computational depth, using ""hierarchical convergence"" to overcome the vanishing gradient and premature convergence problems in recurrent systems.",10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""delete ACU, remove the number of quastions.add the artical in end of prompt""}"
2025-08-28T07:11:43Z,Hille-0-3-update-gemini-and-Q-prompt-shared-querys,,,/home/user/studies/QFS-evaluation/articles/Reinforcement Learning for Reasoning in Small LLMs.md,The challenge of managing multilingual language drift when fine-tuning a foundational model for a monolingual reasoning task.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""delete ACU, remove the number of quastions.add the artical in end of prompt""}"
2025-08-28T07:11:43Z,Hille-0-3-update-gemini-and-Q-prompt-shared-querys,,,/home/user/studies/QFS-evaluation/articles/Rethinking Training Signals in RLVR.md,"The divergent behaviors of different model families (Qwen, Llama, OLMo) under identical RLVR training, highlighting a fundamental lack of technique generalization.",10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""delete ACU, remove the number of quastions.add the artical in end of prompt""}"
2025-08-28T07:11:43Z,Hille-0-3-update-gemini-and-Q-prompt-shared-querys,,,/home/user/studies/QFS-evaluation/articles/Reinforcement Learning for Reasoning in Small LLMs.md,Causes of optimization instability and performance degradation during RL fine-tuning of small language models.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""delete ACU, remove the number of quastions.add the artical in end of prompt""}"
2025-08-28T07:11:43Z,Hille-0-3-update-gemini-and-Q-prompt-shared-querys,,,/home/user/studies/QFS-evaluation/articles/MemOS.md,Mechanisms for the lifecycle and cross-type transformation of memory.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""delete ACU, remove the number of quastions.add the artical in end of prompt""}"
2025-08-28T07:11:43Z,Hille-0-3-update-gemini-and-Q-prompt-shared-querys,,,/home/user/studies/QFS-evaluation/articles/Random Teachers are Good Teachers.md,The mechanism by which initializing a student network in close proximity to a random teacher allows it to escape the teacher's poor minimum by navigating the flatter side of an asymmetric valley in the loss landscape.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""delete ACU, remove the number of quastions.add the artical in end of prompt""}"
2025-08-28T07:11:43Z,Hille-0-3-update-gemini-and-Q-prompt-shared-querys,,,/home/user/studies/QFS-evaluation/articles/distillation of SOTA embedding models.md,The role and limitations of proprietary expert models in synthetic data generation for LLM research.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""delete ACU, remove the number of quastions.add the artical in end of prompt""}"
2025-08-28T07:11:43Z,Hille-0-3-update-gemini-and-Q-prompt-shared-querys,,,/home/user/studies/QFS-evaluation/articles/The-Linear-Representation-Hypothesis.md,The article's use of counterfactuals as a formal language to ground and connect disparate intuitions about linear representations.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""delete ACU, remove the number of quastions.add the artical in end of prompt""}"
2025-08-28T07:11:43Z,Hille-0-3-update-gemini-and-Q-prompt-shared-querys,,,/home/user/studies/QFS-evaluation/articles/distillation of SOTA embedding models.md,A comparative economic analysis of achieving state-of-the-art reasoning performance in small vs. large language models.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""delete ACU, remove the number of quastions.add the artical in end of prompt""}"
2025-08-28T07:11:43Z,Hille-0-3-update-gemini-and-Q-prompt-shared-querys,,,/home/user/studies/QFS-evaluation/articles/Rethinking Training Signals in RLVR.md,"A critique of model-specific conclusions in RL research, using the Qwen model family as a case study for how easily performance gains can be achieved with spurious signals.",10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""delete ACU, remove the number of quastions.add the artical in end of prompt""}"
2025-08-28T07:11:43Z,Hille-0-3-update-gemini-and-Q-prompt-shared-querys,,,/home/user/studies/QFS-evaluation/articles/Forget-What-You-Know-about-LLMs.md,"A critique of model-specific conclusions in RL research, using the Qwen model family as a case study for how easily performance gains can be achieved with spurious signals.",10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""delete ACU, remove the number of quastions.add the artical in end of prompt""}"
2025-08-28T07:11:43Z,Hille-0-3-update-gemini-and-Q-prompt-shared-querys,,,/home/user/studies/QFS-evaluation/articles/Reinforcement Learning for Reasoning in Small LLMs.md,The impact of output length constraints on the stability and efficacy of training LLMs for complex reasoning tasks.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""delete ACU, remove the number of quastions.add the artical in end of prompt""}"
2025-08-28T07:11:43Z,Hille-0-3-update-gemini-and-Q-prompt-shared-querys,,,/home/user/studies/QFS-evaluation/articles/Learn Beyond The Answer.md,"The interplay between data quantity, quality, and complexity in fine-tuning language models for mathematical tasks.",10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""delete ACU, remove the number of quastions.add the artical in end of prompt""}"
2025-08-28T07:11:43Z,Hille-0-3-update-gemini-and-Q-prompt-shared-querys,,,/home/user/studies/QFS-evaluation/articles/Random Teachers are Good Teachers.md,"A reframing of non-contrastive self-supervised learning where implicit regularization from the teacher-student optimization dynamic, rather than explicit data augmentation, serves as the primary driver of feature learning.",10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""delete ACU, remove the number of quastions.add the artical in end of prompt""}"
2025-08-28T07:11:43Z,Hille-0-3-update-gemini-and-Q-prompt-shared-querys,,,/home/user/studies/QFS-evaluation/articles/Rethinking Training Signals in RLVR.md,"An analysis of how superficial interventions, such as prompting or rewarding simple syntactic patterns, can elicit complex reasoning behaviors in certain models.",10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""delete ACU, remove the number of quastions.add the artical in end of prompt""}"
2025-08-28T07:11:43Z,Hille-0-3-update-gemini-and-Q-prompt-shared-querys,,,/home/user/studies/QFS-evaluation/articles/Task Singular Vectors.md,"The role of optimization algorithm artifacts, specifically the clipping mechanism in GRPO, in creating a directional training signal from pure noise.",10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""delete ACU, remove the number of quastions.add the artical in end of prompt""}"
2025-08-28T07:11:43Z,Hille-0-3-update-gemini-and-Q-prompt-shared-querys,,,/home/user/studies/QFS-evaluation/articles/Rethinking Training Signals in RLVR.md,"The influence of a model's pretrained latent abilities (like code reasoning) on the outcomes of reinforcement learning, irrespective of the reward signal's quality.",10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""delete ACU, remove the number of quastions.add the artical in end of prompt""}"
2025-08-28T07:11:43Z,Hille-0-3-update-gemini-and-Q-prompt-shared-querys,,,/home/user/studies/QFS-evaluation/articles/The-Linear-Representation-Hypothesis.md,"The paper as a solution to the geometric unidentifiability of representation spaces, resolved by imposing a causal structure.",10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""delete ACU, remove the number of quastions.add the artical in end of prompt""}"
2025-08-28T07:11:43Z,Hille-0-3-update-gemini-and-Q-prompt-shared-querys,,,/home/user/studies/QFS-evaluation/articles/distillation of SOTA embedding models.md,"The interplay between data quantity, quality, and complexity in fine-tuning language models for mathematical tasks.",10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""delete ACU, remove the number of quastions.add the artical in end of prompt""}"
2025-08-28T07:22:36Z,Hille-0-4-update-gemini-and-Q-prompt-shared-querys-without-ACU,,,/home/user/studies/QFS-evaluation/articles/Hierarchical-Reasoning-Model.md,"An analysis of how superficial interventions, such as prompting or rewarding simple syntactic patterns, can elicit complex reasoning behaviors in certain models.",10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""delete ACU, remove the number of quastions. add the artical in end of prompt""}"
2025-08-28T07:22:36Z,Hille-0-4-update-gemini-and-Q-prompt-shared-querys-without-ACU,,,/home/user/studies/QFS-evaluation/articles/Forget-What-You-Know-about-LLMs.md,"A critique of model-specific conclusions in RL research, using the Qwen model family as a case study for how easily performance gains can be achieved with spurious signals.",10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""delete ACU, remove the number of quastions. add the artical in end of prompt""}"
2025-08-28T07:22:36Z,Hille-0-4-update-gemini-and-Q-prompt-shared-querys-without-ACU,,,/home/user/studies/QFS-evaluation/articles/Evolutionary Computation LLM.md,The methodology of using generative models to create adversarial evaluation datasets.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""delete ACU, remove the number of quastions. add the artical in end of prompt""}"
2025-08-28T07:22:36Z,Hille-0-4-update-gemini-and-Q-prompt-shared-querys-without-ACU,,,/home/user/studies/QFS-evaluation/articles/Random Teachers are Good Teachers.md,"An analysis of how superficial interventions, such as prompting or rewarding simple syntactic patterns, can elicit complex reasoning behaviors in certain models.",10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""delete ACU, remove the number of quastions. add the artical in end of prompt""}"
2025-08-28T07:22:36Z,Hille-0-4-update-gemini-and-Q-prompt-shared-querys-without-ACU,,,/home/user/studies/QFS-evaluation/articles/Forget-What-You-Know-about-LLMs.md,"The divergent behaviors of different model families (Qwen, Llama, OLMo) under identical RLVR training, highlighting a fundamental lack of technique generalization.",10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""delete ACU, remove the number of quastions. add the artical in end of prompt""}"
2025-08-28T07:22:36Z,Hille-0-4-update-gemini-and-Q-prompt-shared-querys-without-ACU,,,/home/user/studies/QFS-evaluation/articles/Random Teachers are Good Teachers.md,The mechanism by which initializing a student network in close proximity to a random teacher allows it to escape the teacher's poor minimum by navigating the flatter side of an asymmetric valley in the loss landscape.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""delete ACU, remove the number of quastions. add the artical in end of prompt""}"
2025-08-28T07:22:36Z,Hille-0-4-update-gemini-and-Q-prompt-shared-querys-without-ACU,,,/home/user/studies/QFS-evaluation/articles/Learn Beyond The Answer.md,The role and limitations of proprietary expert models in synthetic data generation for LLM research.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""delete ACU, remove the number of quastions. add the artical in end of prompt""}"
2025-08-28T07:22:36Z,Hille-0-4-update-gemini-and-Q-prompt-shared-querys-without-ACU,,,/home/user/studies/QFS-evaluation/articles/Hierarchical-Reasoning-Model.md,A comparative economic analysis of achieving state-of-the-art reasoning performance in small vs. large language models.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""delete ACU, remove the number of quastions. add the artical in end of prompt""}"
2025-08-28T07:22:36Z,Hille-0-4-update-gemini-and-Q-prompt-shared-querys-without-ACU,,,/home/user/studies/QFS-evaluation/articles/MemOS.md,Mechanisms for the lifecycle and cross-type transformation of memory.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""delete ACU, remove the number of quastions. add the artical in end of prompt""}"
2025-08-28T07:22:36Z,Hille-0-4-update-gemini-and-Q-prompt-shared-querys-without-ACU,,,/home/user/studies/QFS-evaluation/articles/MemOS.md,The vision for a collaborative and decentralized memory sharing ecosystem.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""delete ACU, remove the number of quastions. add the artical in end of prompt""}"
2025-08-28T07:22:36Z,Hille-0-4-update-gemini-and-Q-prompt-shared-querys-without-ACU,,,/home/user/studies/QFS-evaluation/articles/Reinforcement Learning for Reasoning in Small LLMs.md,A comparative economic analysis of achieving state-of-the-art reasoning performance in small vs. large language models.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""delete ACU, remove the number of quastions. add the artical in end of prompt""}"
2025-08-28T07:22:36Z,Hille-0-4-update-gemini-and-Q-prompt-shared-querys-without-ACU,,,/home/user/studies/QFS-evaluation/articles/MemOS.md,The framework for accountable and governed memory in multi-agent systems.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""delete ACU, remove the number of quastions. add the artical in end of prompt""}"
2025-08-28T07:22:36Z,Hille-0-4-update-gemini-and-Q-prompt-shared-querys-without-ACU,,,/home/user/studies/QFS-evaluation/articles/Reinforcement Learning for Reasoning in Small LLMs.md,The impact of output length constraints on the stability and efficacy of training LLMs for complex reasoning tasks.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""delete ACU, remove the number of quastions. add the artical in end of prompt""}"
2025-08-28T07:22:36Z,Hille-0-4-update-gemini-and-Q-prompt-shared-querys-without-ACU,,,/home/user/studies/QFS-evaluation/articles/Evolutionary Computation LLM.md,"An analysis of how superficial interventions, such as prompting or rewarding simple syntactic patterns, can elicit complex reasoning behaviors in certain models.",10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""delete ACU, remove the number of quastions. add the artical in end of prompt""}"
2025-08-28T07:22:36Z,Hille-0-4-update-gemini-and-Q-prompt-shared-querys-without-ACU,,,/home/user/studies/QFS-evaluation/articles/Reinforcement Learning for Reasoning in Small LLMs.md,Strategies for curating compact datasets by balancing problem difficulty to enhance RL training efficiency under resource constraints.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""delete ACU, remove the number of quastions. add the artical in end of prompt""}"
2025-08-28T07:22:36Z,Hille-0-4-update-gemini-and-Q-prompt-shared-querys-without-ACU,,,/home/user/studies/QFS-evaluation/articles/Rethinking Training Signals in RLVR.md,"The role of optimization algorithm artifacts, specifically the clipping mechanism in GRPO, in creating a directional training signal from pure noise.",10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""delete ACU, remove the number of quastions. add the artical in end of prompt""}"
2025-08-28T07:22:36Z,Hille-0-4-update-gemini-and-Q-prompt-shared-querys-without-ACU,,,/home/user/studies/QFS-evaluation/articles/Attention-Is-All-You-Need.md,A comparative economic analysis of achieving state-of-the-art reasoning performance in small vs. large language models.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""delete ACU, remove the number of quastions. add the artical in end of prompt""}"
2025-08-28T07:22:36Z,Hille-0-4-update-gemini-and-Q-prompt-shared-querys-without-ACU,,,/home/user/studies/QFS-evaluation/articles/Forget-What-You-Know-about-LLMs.md,"An analysis of how superficial interventions, such as prompting or rewarding simple syntactic patterns, can elicit complex reasoning behaviors in certain models.",10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""delete ACU, remove the number of quastions. add the artical in end of prompt""}"
2025-08-28T07:22:36Z,Hille-0-4-update-gemini-and-Q-prompt-shared-querys-without-ACU,,,/home/user/studies/QFS-evaluation/articles/Random Teachers are Good Teachers.md,"A reframing of non-contrastive self-supervised learning where implicit regularization from the teacher-student optimization dynamic, rather than explicit data augmentation, serves as the primary driver of feature learning.",10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""delete ACU, remove the number of quastions. add the artical in end of prompt""}"
2025-08-28T07:22:36Z,Hille-0-4-update-gemini-and-Q-prompt-shared-querys-without-ACU,,,/home/user/studies/QFS-evaluation/articles/Reinforcement Learning for Reasoning in Small LLMs.md,The challenge of managing multilingual language drift when fine-tuning a foundational model for a monolingual reasoning task.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""delete ACU, remove the number of quastions. add the artical in end of prompt""}"
2025-08-28T07:22:36Z,Hille-0-4-update-gemini-and-Q-prompt-shared-querys-without-ACU,,,/home/user/studies/QFS-evaluation/articles/Hierarchical-Reasoning-Model.md,"Mechanisms for achieving and leveraging effective computational depth, using ""hierarchical convergence"" to overcome the vanishing gradient and premature convergence problems in recurrent systems.",10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""delete ACU, remove the number of quastions. add the artical in end of prompt""}"
2025-08-28T07:22:36Z,Hille-0-4-update-gemini-and-Q-prompt-shared-querys-without-ACU,,,/home/user/studies/QFS-evaluation/articles/Attention-Is-All-You-Need.md,"Mechanisms for achieving and leveraging effective computational depth, using ""hierarchical convergence"" to overcome the vanishing gradient and premature convergence problems in recurrent systems.",10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""delete ACU, remove the number of quastions. add the artical in end of prompt""}"
2025-08-28T07:22:36Z,Hille-0-4-update-gemini-and-Q-prompt-shared-querys-without-ACU,,,/home/user/studies/QFS-evaluation/articles/Attention-Is-All-You-Need.md,"An analysis of how superficial interventions, such as prompting or rewarding simple syntactic patterns, can elicit complex reasoning behaviors in certain models.",10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""delete ACU, remove the number of quastions. add the artical in end of prompt""}"
2025-08-28T07:22:36Z,Hille-0-4-update-gemini-and-Q-prompt-shared-querys-without-ACU,,,/home/user/studies/QFS-evaluation/articles/Learn Beyond The Answer.md,"The interplay between data quantity, quality, and complexity in fine-tuning language models for mathematical tasks.",10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""delete ACU, remove the number of quastions. add the artical in end of prompt""}"
2025-08-28T07:22:36Z,Hille-0-4-update-gemini-and-Q-prompt-shared-querys-without-ACU,,,/home/user/studies/QFS-evaluation/articles/Learn Beyond The Answer.md,A comparative economic analysis of achieving state-of-the-art reasoning performance in small vs. large language models.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""delete ACU, remove the number of quastions. add the artical in end of prompt""}"
2025-08-28T07:22:36Z,Hille-0-4-update-gemini-and-Q-prompt-shared-querys-without-ACU,,,/home/user/studies/QFS-evaluation/articles/distillation of SOTA embedding models.md,"The interplay between data quantity, quality, and complexity in fine-tuning language models for mathematical tasks.",10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""delete ACU, remove the number of quastions. add the artical in end of prompt""}"
2025-08-28T07:22:36Z,Hille-0-4-update-gemini-and-Q-prompt-shared-querys-without-ACU,,,/home/user/studies/QFS-evaluation/articles/Task Singular Vectors.md,"Mechanisms for achieving and leveraging effective computational depth, using ""hierarchical convergence"" to overcome the vanishing gradient and premature convergence problems in recurrent systems.",10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""delete ACU, remove the number of quastions. add the artical in end of prompt""}"
2025-08-28T07:22:36Z,Hille-0-4-update-gemini-and-Q-prompt-shared-querys-without-ACU,,,/home/user/studies/QFS-evaluation/articles/The-Linear-Representation-Hypothesis.md,"The derivation of a semantically meaningful geometry from the statistical properties of the model's full vocabulary, rather than from its training data distribution.",10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""delete ACU, remove the number of quastions. add the artical in end of prompt""}"
2025-08-28T07:22:36Z,Hille-0-4-update-gemini-and-Q-prompt-shared-querys-without-ACU,,,/home/user/studies/QFS-evaluation/articles/distillation of SOTA embedding models.md,A comparative economic analysis of achieving state-of-the-art reasoning performance in small vs. large language models.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""delete ACU, remove the number of quastions. add the artical in end of prompt""}"
2025-08-28T07:22:36Z,Hille-0-4-update-gemini-and-Q-prompt-shared-querys-without-ACU,,,/home/user/studies/QFS-evaluation/articles/distillation of SOTA embedding models.md,The role and limitations of proprietary expert models in synthetic data generation for LLM research.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""delete ACU, remove the number of quastions. add the artical in end of prompt""}"
2025-08-28T07:22:36Z,Hille-0-4-update-gemini-and-Q-prompt-shared-querys-without-ACU,,,/home/user/studies/QFS-evaluation/articles/The-Linear-Representation-Hypothesis.md,"The paper as a solution to the geometric unidentifiability of representation spaces, resolved by imposing a causal structure.",10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""delete ACU, remove the number of quastions. add the artical in end of prompt""}"
2025-08-28T07:22:36Z,Hille-0-4-update-gemini-and-Q-prompt-shared-querys-without-ACU,,,/home/user/studies/QFS-evaluation/articles/Rethinking Training Signals in RLVR.md,"The influence of a model's pretrained latent abilities (like code reasoning) on the outcomes of reinforcement learning, irrespective of the reward signal's quality.",10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""delete ACU, remove the number of quastions. add the artical in end of prompt""}"
2025-08-28T07:22:36Z,Hille-0-4-update-gemini-and-Q-prompt-shared-querys-without-ACU,,,/home/user/studies/QFS-evaluation/articles/Reinforcement Learning for Reasoning in Small LLMs.md,Causes of optimization instability and performance degradation during RL fine-tuning of small language models.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""delete ACU, remove the number of quastions. add the artical in end of prompt""}"
2025-08-28T07:22:36Z,Hille-0-4-update-gemini-and-Q-prompt-shared-querys-without-ACU,,,/home/user/studies/QFS-evaluation/articles/Task Singular Vectors.md,"The divergent behaviors of different model families (Qwen, Llama, OLMo) under identical RLVR training, highlighting a fundamental lack of technique generalization.",10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""delete ACU, remove the number of quastions. add the artical in end of prompt""}"
2025-08-28T07:22:36Z,Hille-0-4-update-gemini-and-Q-prompt-shared-querys-without-ACU,,,/home/user/studies/QFS-evaluation/articles/The-Linear-Representation-Hypothesis.md,The article's use of counterfactuals as a formal language to ground and connect disparate intuitions about linear representations.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""delete ACU, remove the number of quastions. add the artical in end of prompt""}"
2025-08-28T07:22:36Z,Hille-0-4-update-gemini-and-Q-prompt-shared-querys-without-ACU,,,/home/user/studies/QFS-evaluation/articles/Evolutionary Computation LLM.md,The role and limitations of proprietary expert models in synthetic data generation for LLM research.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""delete ACU, remove the number of quastions. add the artical in end of prompt""}"
2025-08-28T07:22:36Z,Hille-0-4-update-gemini-and-Q-prompt-shared-querys-without-ACU,,,/home/user/studies/QFS-evaluation/articles/Rethinking Training Signals in RLVR.md,"A critique of model-specific conclusions in RL research, using the Qwen model family as a case study for how easily performance gains can be achieved with spurious signals.",10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""delete ACU, remove the number of quastions. add the artical in end of prompt""}"
2025-08-28T07:22:36Z,Hille-0-4-update-gemini-and-Q-prompt-shared-querys-without-ACU,,,/home/user/studies/QFS-evaluation/articles/Rethinking Training Signals in RLVR.md,"An analysis of how superficial interventions, such as prompting or rewarding simple syntactic patterns, can elicit complex reasoning behaviors in certain models.",10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""delete ACU, remove the number of quastions. add the artical in end of prompt""}"
2025-08-28T07:22:36Z,Hille-0-4-update-gemini-and-Q-prompt-shared-querys-without-ACU,,,/home/user/studies/QFS-evaluation/articles/Task Singular Vectors.md,"The role of optimization algorithm artifacts, specifically the clipping mechanism in GRPO, in creating a directional training signal from pure noise.",10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""delete ACU, remove the number of quastions. add the artical in end of prompt""}"
2025-08-28T07:44:11Z,Hille-0-5-update-gemini-without-ACU-fix-judge,,,/home/user/studies/QFS-evaluation/articles/Hierarchical-Reasoning-Model.md,"Novel training strategies for recurrent architectures, combining BPTT-free approximate gradients, deep supervision, and reinforcement learning for adaptive computation.",10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""delete ACU, remove the number of quastions,fix judge""}"
2025-08-28T07:44:11Z,Hille-0-5-update-gemini-without-ACU-fix-judge,,,/home/user/studies/QFS-evaluation/articles/Hierarchical-Reasoning-Model.md,Emergent dimensionality hierarchy in a trained reasoning model as a parallel to the functional organization of the mouse cortex.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""delete ACU, remove the number of quastions,fix judge""}"
2025-08-28T07:44:11Z,Hille-0-5-update-gemini-without-ACU-fix-judge,,,/home/user/studies/QFS-evaluation/articles/Forget-What-You-Know-about-LLMs.md,The paper's critique of a leaderboard-centric evaluation culture in NLP.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""delete ACU, remove the number of quastions,fix judge""}"
2025-08-28T07:44:11Z,Hille-0-5-update-gemini-without-ACU-fix-judge,,,/home/user/studies/QFS-evaluation/articles/Attention-Is-All-You-Need.md,The role of regularization and optimization schemes in training large-scale models.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""delete ACU, remove the number of quastions,fix judge""}"
2025-08-28T07:44:11Z,Hille-0-5-update-gemini-without-ACU-fix-judge,,,/home/user/studies/QFS-evaluation/articles/Forget-What-You-Know-about-LLMs.md,Differential robustness across distinct LLM architectural families.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""delete ACU, remove the number of quastions,fix judge""}"
2025-08-28T07:44:11Z,Hille-0-5-update-gemini-without-ACU-fix-judge,,,/home/user/studies/QFS-evaluation/articles/Attention-Is-All-You-Need.md,Computational efficiency and theoretical limits of sequence modeling.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""delete ACU, remove the number of quastions,fix judge""}"
2025-08-28T07:44:11Z,Hille-0-5-update-gemini-without-ACU-fix-judge,,,/home/user/studies/QFS-evaluation/articles/Learn Beyond The Answer.md,"A framework for categorizing mathematical reasoning skills in language models, from basic forward reasoning to complex reflective capabilities.",10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""delete ACU, remove the number of quastions,fix judge""}"
2025-08-28T07:44:11Z,Hille-0-5-update-gemini-without-ACU-fix-judge,,,/home/user/studies/QFS-evaluation/articles/Attention-Is-All-You-Need.md,The practical challenges and solutions for stabilizing dot-product attention.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""delete ACU, remove the number of quastions,fix judge""}"
2025-08-28T07:44:11Z,Hille-0-5-update-gemini-without-ACU-fix-judge,,,/home/user/studies/QFS-evaluation/articles/Learn Beyond The Answer.md,"The interplay between data quantity, quality, and complexity in fine-tuning language models for mathematical tasks.",10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""delete ACU, remove the number of quastions,fix judge""}"
2025-08-28T07:44:11Z,Hille-0-5-update-gemini-without-ACU-fix-judge,,,/home/user/studies/QFS-evaluation/articles/MemOS.md,The framework for accountable and governed memory in multi-agent systems.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""delete ACU, remove the number of quastions,fix judge""}"
2025-08-28T07:44:11Z,Hille-0-5-update-gemini-without-ACU-fix-judge,,,/home/user/studies/QFS-evaluation/articles/MemOS.md,"The role of structured metadata in enabling the scheduling, governance, and evolution of memory.",10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""delete ACU, remove the number of quastions,fix judge""}"
2025-08-28T07:44:11Z,Hille-0-5-update-gemini-without-ACU-fix-judge,,,/home/user/studies/QFS-evaluation/articles/MemOS.md,The vision for a collaborative and decentralized memory sharing ecosystem.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""delete ACU, remove the number of quastions,fix judge""}"
2025-08-28T07:44:11Z,Hille-0-5-update-gemini-without-ACU-fix-judge,,,/home/user/studies/QFS-evaluation/articles/MemOS.md,Mechanisms for the lifecycle and cross-type transformation of memory.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""delete ACU, remove the number of quastions,fix judge""}"
2025-08-28T07:44:11Z,Hille-0-5-update-gemini-without-ACU-fix-judge,,,/home/user/studies/QFS-evaluation/articles/Learn Beyond The Answer.md,Comparing instance-dimension vs. sequence-dimension data augmentation for enhancing LLM reasoning.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""delete ACU, remove the number of quastions,fix judge""}"
2025-08-28T07:44:11Z,Hille-0-5-update-gemini-without-ACU-fix-judge,,,/home/user/studies/QFS-evaluation/articles/Hierarchical-Reasoning-Model.md,"An argument against the architectural and computational limitations of the Transformer/Chain-of-Thought paradigm for genuine, latent algorithmic reasoning.",10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""delete ACU, remove the number of quastions,fix judge""}"
2025-08-28T07:44:11Z,Hille-0-5-update-gemini-without-ACU-fix-judge,,,/home/user/studies/QFS-evaluation/articles/Hierarchical-Reasoning-Model.md,"Applying principles of hierarchical processing, temporal separation, and local credit assignment from neuroscience to engineer stable, deep reasoning in artificial neural networks.",10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""delete ACU, remove the number of quastions,fix judge""}"
2025-08-28T07:44:11Z,Hille-0-5-update-gemini-without-ACU-fix-judge,,,/home/user/studies/QFS-evaluation/articles/Forget-What-You-Know-about-LLMs.md,The methodology of using generative models to create adversarial evaluation datasets.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""delete ACU, remove the number of quastions,fix judge""}"
2025-08-28T07:44:11Z,Hille-0-5-update-gemini-without-ACU-fix-judge,,,/home/user/studies/QFS-evaluation/articles/Attention-Is-All-You-Need.md,"Enhancing representational power through parallel, subspace-specific attention.",10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""delete ACU, remove the number of quastions,fix judge""}"
2025-08-28T07:44:11Z,Hille-0-5-update-gemini-without-ACU-fix-judge,,,/home/user/studies/QFS-evaluation/articles/Evolutionary Computation LLM.md,The article's perspective on the theme of black-box optimization.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""delete ACU, remove the number of quastions,fix judge""}"
2025-08-28T07:44:11Z,Hille-0-5-update-gemini-without-ACU-fix-judge,,,/home/user/studies/QFS-evaluation/articles/Random Teachers are Good Teachers.md,The role of the input data distribution in shaping the learned representations.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""delete ACU, remove the number of quastions,fix judge""}"
2025-08-28T07:44:11Z,Hille-0-5-update-gemini-without-ACU-fix-judge,,,/home/user/studies/QFS-evaluation/articles/Learn Beyond The Answer.md,The role and limitations of proprietary expert models in synthetic data generation for LLM research.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""delete ACU, remove the number of quastions,fix judge""}"
2025-08-28T07:44:11Z,Hille-0-5-update-gemini-without-ACU-fix-judge,,,/home/user/studies/QFS-evaluation/articles/Evolutionary Computation LLM.md,The dual role of the LLM-EA synergy in the context of model and code security.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""delete ACU, remove the number of quastions,fix judge""}"
2025-08-28T07:44:11Z,Hille-0-5-update-gemini-without-ACU-fix-judge,,,/home/user/studies/QFS-evaluation/articles/Evolutionary Computation LLM.md,The application of multi-objective evolutionary principles throughout the surveyed research.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""delete ACU, remove the number of quastions,fix judge""}"
2025-08-28T07:44:11Z,Hille-0-5-update-gemini-without-ACU-fix-judge,,,/home/user/studies/QFS-evaluation/articles/Evolutionary Computation LLM.md,Neural Architecture Search (NAS) as both a target for optimization and a domain for synergy.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""delete ACU, remove the number of quastions,fix judge""}"
2025-08-28T07:44:11Z,Hille-0-5-update-gemini-without-ACU-fix-judge,,,/home/user/studies/QFS-evaluation/articles/Reinforcement Learning for Reasoning in Small LLMs.md,The impact of output length constraints on the stability and efficacy of training LLMs for complex reasoning tasks.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""delete ACU, remove the number of quastions,fix judge""}"
2025-08-28T07:44:11Z,Hille-0-5-update-gemini-without-ACU-fix-judge,,,/home/user/studies/QFS-evaluation/articles/Forget-What-You-Know-about-LLMs.md,The impact of model scale on overfitting vulnerability.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""delete ACU, remove the number of quastions,fix judge""}"
2025-08-28T07:44:11Z,Hille-0-5-update-gemini-without-ACU-fix-judge,,,/home/user/studies/QFS-evaluation/articles/Random Teachers are Good Teachers.md,The impact of student initialization proximity to the teacher.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""delete ACU, remove the number of quastions,fix judge""}"
2025-08-28T07:44:11Z,Hille-0-5-update-gemini-without-ACU-fix-judge,,,/home/user/studies/QFS-evaluation/articles/Evolutionary Computation LLM.md,The LLM as a substitute for evolutionary operators.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""delete ACU, remove the number of quastions,fix judge""}"
2025-08-28T07:44:11Z,Hille-0-5-update-gemini-without-ACU-fix-judge,,,/home/user/studies/QFS-evaluation/articles/Learn Beyond The Answer.md,Efficiency trade-offs in LLM data augmentation strategies.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""delete ACU, remove the number of quastions,fix judge""}"
2025-08-28T07:44:11Z,Hille-0-5-update-gemini-without-ACU-fix-judge,,,/home/user/studies/QFS-evaluation/articles/Reinforcement Learning for Reasoning in Small LLMs.md,Strategies for curating compact datasets by balancing problem difficulty to enhance RL training efficiency under resource constraints.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""delete ACU, remove the number of quastions,fix judge""}"
2025-08-28T07:44:11Z,Hille-0-5-update-gemini-without-ACU-fix-judge,,,/home/user/studies/QFS-evaluation/articles/Attention-Is-All-You-Need.md,Methods for encoding sequential order in non-recurrent architectures.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""delete ACU, remove the number of quastions,fix judge""}"
2025-08-28T07:44:11Z,Hille-0-5-update-gemini-without-ACU-fix-judge,,,/home/user/studies/QFS-evaluation/articles/Hierarchical-Reasoning-Model.md,"Mechanisms for achieving and leveraging effective computational depth, using ""hierarchical convergence"" to overcome the vanishing gradient and premature convergence problems in recurrent systems.",10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""delete ACU, remove the number of quastions,fix judge""}"
2025-08-28T07:44:11Z,Hille-0-5-update-gemini-without-ACU-fix-judge,,,/home/user/studies/QFS-evaluation/articles/Random Teachers are Good Teachers.md,The asymmetric nature of the loss landscape around a random initialization.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""delete ACU, remove the number of quastions,fix judge""}"
2025-08-28T07:44:11Z,Hille-0-5-update-gemini-without-ACU-fix-judge,,,/home/user/studies/QFS-evaluation/articles/MemOS.md,Parallels between the MemOS architecture and cognitive models of memory.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""delete ACU, remove the number of quastions,fix judge""}"
2025-08-28T07:44:11Z,Hille-0-5-update-gemini-without-ACU-fix-judge,,,/home/user/studies/QFS-evaluation/articles/Reinforcement Learning for Reasoning in Small LLMs.md,The challenge of managing multilingual language drift when fine-tuning a foundational model for a monolingual reasoning task.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""delete ACU, remove the number of quastions,fix judge""}"
2025-08-28T07:44:11Z,Hille-0-5-update-gemini-without-ACU-fix-judge,,,/home/user/studies/QFS-evaluation/articles/Random Teachers are Good Teachers.md,The nature of implicit regularization induced by the teacher-student dynamics.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""delete ACU, remove the number of quastions,fix judge""}"
2025-08-28T07:44:11Z,Hille-0-5-update-gemini-without-ACU-fix-judge,,,/home/user/studies/QFS-evaluation/articles/The-Linear-Representation-Hypothesis.md,"Providing a single geometric framework that formally connects the subspace hypothesis (e.g., word2vec analogies) with the practical techniques of linear probing and activation steering.",10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""delete ACU, remove the number of quastions,fix judge""}"
2025-08-28T07:44:11Z,Hille-0-5-update-gemini-without-ACU-fix-judge,,,/home/user/studies/QFS-evaluation/articles/Task Singular Vectors.md,The counter-intuitive necessity of low-rank approximation for effective interference reduction.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""delete ACU, remove the number of quastions,fix judge""}"
2025-08-28T07:44:11Z,Hille-0-5-update-gemini-without-ACU-fix-judge,,,/home/user/studies/QFS-evaluation/articles/Rethinking Training Signals in RLVR.md,"An analysis of how superficial interventions, such as prompting or rewarding simple syntactic patterns, can elicit complex reasoning behaviors in certain models.",10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""delete ACU, remove the number of quastions,fix judge""}"
2025-08-28T07:44:11Z,Hille-0-5-update-gemini-without-ACU-fix-judge,,,/home/user/studies/QFS-evaluation/articles/Forget-What-You-Know-about-LLMs.md,The paradox of high accuracy as an indicator of fragility.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""delete ACU, remove the number of quastions,fix judge""}"
2025-08-28T07:44:11Z,Hille-0-5-update-gemini-without-ACU-fix-judge,,,/home/user/studies/QFS-evaluation/articles/Reinforcement Learning for Reasoning in Small LLMs.md,Causes of optimization instability and performance degradation during RL fine-tuning of small language models.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""delete ACU, remove the number of quastions,fix judge""}"
2025-08-28T07:44:11Z,Hille-0-5-update-gemini-without-ACU-fix-judge,,,/home/user/studies/QFS-evaluation/articles/The-Linear-Representation-Hypothesis.md,The article's use of counterfactuals as a formal language to ground and connect disparate intuitions about linear representations.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""delete ACU, remove the number of quastions,fix judge""}"
2025-08-28T07:44:11Z,Hille-0-5-update-gemini-without-ACU-fix-judge,,,/home/user/studies/QFS-evaluation/articles/Task Singular Vectors.md,The application of Orthogonal Procrustes analysis to decorrelate task-specific feature bases.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""delete ACU, remove the number of quastions,fix judge""}"
2025-08-28T07:44:11Z,Hille-0-5-update-gemini-without-ACU-fix-judge,,,/home/user/studies/QFS-evaluation/articles/The-Linear-Representation-Hypothesis.md,"The paper as a solution to the geometric unidentifiability of representation spaces, resolved by imposing a causal structure.",10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""delete ACU, remove the number of quastions,fix judge""}"
2025-08-28T07:44:11Z,Hille-0-5-update-gemini-without-ACU-fix-judge,,,/home/user/studies/QFS-evaluation/articles/Random Teachers are Good Teachers.md,How random teacher distillation pre-conditions a network for supervised training.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""delete ACU, remove the number of quastions,fix judge""}"
2025-08-28T07:44:11Z,Hille-0-5-update-gemini-without-ACU-fix-judge,,,/home/user/studies/QFS-evaluation/articles/Task Singular Vectors.md,The paper's empirical argument for eliminating the scaling coefficient hyperparameter.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""delete ACU, remove the number of quastions,fix judge""}"
2025-08-28T07:44:11Z,Hille-0-5-update-gemini-without-ACU-fix-judge,,,/home/user/studies/QFS-evaluation/articles/rStar-Math.md,The emergence of self-correction capabilities as an intrinsic byproduct of the MCTS-based reasoning process.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""delete ACU, remove the number of quastions,fix judge""}"
2025-08-28T07:44:11Z,Hille-0-5-update-gemini-without-ACU-fix-judge,,,/home/user/studies/QFS-evaluation/articles/The-Linear-Representation-Hypothesis.md,How the proposed 'causal inner product' serves as a bridge to unify the conceptually separate input (embedding) and output (unembedding) representation spaces.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""delete ACU, remove the number of quastions,fix judge""}"
2025-08-28T07:44:11Z,Hille-0-5-update-gemini-without-ACU-fix-judge,,,/home/user/studies/QFS-evaluation/articles/Rethinking Training Signals in RLVR.md,"A critique of model-specific conclusions in RL research, using the Qwen model family as a case study for how easily performance gains can be achieved with spurious signals.",10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""delete ACU, remove the number of quastions,fix judge""}"
2025-08-28T07:44:11Z,Hille-0-5-update-gemini-without-ACU-fix-judge,,,/home/user/studies/QFS-evaluation/articles/distillation of SOTA embedding models.md,The progressive constraint strategy in knowledge distillation.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""delete ACU, remove the number of quastions,fix judge""}"
2025-08-28T07:44:11Z,Hille-0-5-update-gemini-without-ACU-fix-judge,,,/home/user/studies/QFS-evaluation/articles/rStar-Math.md,Novel training methodologies for process reward models that bypass noisy score annotation.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""delete ACU, remove the number of quastions,fix judge""}"
2025-08-28T07:44:11Z,Hille-0-5-update-gemini-without-ACU-fix-judge,,,/home/user/studies/QFS-evaluation/articles/Task Singular Vectors.md,The geometric interpretation of task interference through singular vector alignment.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""delete ACU, remove the number of quastions,fix judge""}"
2025-08-28T07:44:11Z,Hille-0-5-update-gemini-without-ACU-fix-judge,,,/home/user/studies/QFS-evaluation/articles/Reinforcement Learning for Reasoning in Small LLMs.md,A comparative economic analysis of achieving state-of-the-art reasoning performance in small vs. large language models.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""delete ACU, remove the number of quastions,fix judge""}"
2025-08-28T07:44:11Z,Hille-0-5-update-gemini-without-ACU-fix-judge,,,/home/user/studies/QFS-evaluation/articles/Rethinking Training Signals in RLVR.md,"The divergent behaviors of different model families (Qwen, Llama, OLMo) under identical RLVR training, highlighting a fundamental lack of technique generalization.",10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""delete ACU, remove the number of quastions,fix judge""}"
2025-08-28T07:44:11Z,Hille-0-5-update-gemini-without-ACU-fix-judge,,,/home/user/studies/QFS-evaluation/articles/The-Linear-Representation-Hypothesis.md,"The derivation of a semantically meaningful geometry from the statistical properties of the model's full vocabulary, rather than from its training data distribution.",10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""delete ACU, remove the number of quastions,fix judge""}"
2025-08-28T07:44:11Z,Hille-0-5-update-gemini-without-ACU-fix-judge,,,/home/user/studies/QFS-evaluation/articles/Task Singular Vectors.md,An argument for structure-aware model manipulation over flattened parameter approaches.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""delete ACU, remove the number of quastions,fix judge""}"
2025-08-28T07:44:11Z,Hille-0-5-update-gemini-without-ACU-fix-judge,,,/home/user/studies/QFS-evaluation/articles/distillation of SOTA embedding models.md,Methodology for fusing knowledge from heterogeneous teacher models.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""delete ACU, remove the number of quastions,fix judge""}"
2025-08-28T07:44:11Z,Hille-0-5-update-gemini-without-ACU-fix-judge,,,/home/user/studies/QFS-evaluation/articles/rStar-Math.md,The viability of self-evolution as an alternative to knowledge distillation for creating state-of-the-art training datasets.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""delete ACU, remove the number of quastions,fix judge""}"
2025-08-28T07:44:11Z,Hille-0-5-update-gemini-without-ACU-fix-judge,,,/home/user/studies/QFS-evaluation/articles/distillation of SOTA embedding models.md,Rationale for the staged training methodology.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""delete ACU, remove the number of quastions,fix judge""}"
2025-08-28T07:44:11Z,Hille-0-5-update-gemini-without-ACU-fix-judge,,,/home/user/studies/QFS-evaluation/articles/rStar-Math.md,The role of code execution as a verifier for synthetic reasoning data.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""delete ACU, remove the number of quastions,fix judge""}"
2025-08-28T07:44:11Z,Hille-0-5-update-gemini-without-ACU-fix-judge,,,/home/user/studies/QFS-evaluation/articles/distillation of SOTA embedding models.md,The paper's dual approach to the engineering problem of embedding dimensionality.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""delete ACU, remove the number of quastions,fix judge""}"
2025-08-28T07:44:11Z,Hille-0-5-update-gemini-without-ACU-fix-judge,,,/home/user/studies/QFS-evaluation/articles/distillation of SOTA embedding models.md,Self-distillation as a mechanism for post-hoc modality alignment.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""delete ACU, remove the number of quastions,fix judge""}"
2025-08-28T07:44:11Z,Hille-0-5-update-gemini-without-ACU-fix-judge,,,/home/user/studies/QFS-evaluation/articles/Rethinking Training Signals in RLVR.md,"The influence of a model's pretrained latent abilities (like code reasoning) on the outcomes of reinforcement learning, irrespective of the reward signal's quality.",10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""delete ACU, remove the number of quastions,fix judge""}"
2025-08-28T07:55:22Z,sections-0-0-update-gemini,,,/home/user/studies/QFS-evaluation/articles/Attention-Is-All-You-Need.md,The role of regularization and optimization schemes in training large-scale models.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": """"}"
2025-08-28T07:55:22Z,sections-0-0-update-gemini,,,/home/user/studies/QFS-evaluation/articles/Forget-What-You-Know-about-LLMs.md,Differential robustness across distinct LLM architectural families.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": """"}"
2025-08-28T07:55:22Z,sections-0-0-update-gemini,,,/home/user/studies/QFS-evaluation/articles/Forget-What-You-Know-about-LLMs.md,The impact of model scale on overfitting vulnerability.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": """"}"
2025-08-28T07:55:22Z,sections-0-0-update-gemini,,,/home/user/studies/QFS-evaluation/articles/Evolutionary Computation LLM.md,The article's perspective on the theme of black-box optimization.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": """"}"
2025-08-28T07:55:22Z,sections-0-0-update-gemini,,,/home/user/studies/QFS-evaluation/articles/Attention-Is-All-You-Need.md,"Enhancing representational power through parallel, subspace-specific attention.",10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": """"}"
2025-08-28T07:55:22Z,sections-0-0-update-gemini,,,/home/user/studies/QFS-evaluation/articles/Forget-What-You-Know-about-LLMs.md,The paper's critique of a leaderboard-centric evaluation culture in NLP.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": """"}"
2025-08-28T07:55:22Z,sections-0-0-update-gemini,,,/home/user/studies/QFS-evaluation/articles/Forget-What-You-Know-about-LLMs.md,The methodology of using generative models to create adversarial evaluation datasets.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": """"}"
2025-08-28T07:55:22Z,sections-0-0-update-gemini,,,/home/user/studies/QFS-evaluation/articles/Evolutionary Computation LLM.md,The dual role of the LLM-EA synergy in the context of model and code security.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": """"}"
2025-08-28T07:55:22Z,sections-0-0-update-gemini,,,/home/user/studies/QFS-evaluation/articles/Attention-Is-All-You-Need.md,Methods for encoding sequential order in non-recurrent architectures.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": """"}"
2025-08-28T07:55:22Z,sections-0-0-update-gemini,,,/home/user/studies/QFS-evaluation/articles/Attention-Is-All-You-Need.md,Computational efficiency and theoretical limits of sequence modeling.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": """"}"
2025-08-28T07:55:22Z,sections-0-0-update-gemini,,,/home/user/studies/QFS-evaluation/articles/Evolutionary Computation LLM.md,The application of multi-objective evolutionary principles throughout the surveyed research.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": """"}"
2025-08-28T07:55:22Z,sections-0-0-update-gemini,,,/home/user/studies/QFS-evaluation/articles/Forget-What-You-Know-about-LLMs.md,The paradox of high accuracy as an indicator of fragility.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": """"}"
2025-08-28T07:55:22Z,sections-0-0-update-gemini,,,/home/user/studies/QFS-evaluation/articles/Learn Beyond The Answer.md,The role and limitations of proprietary expert models in synthetic data generation for LLM research.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": """"}"
2025-08-28T07:55:22Z,sections-0-0-update-gemini,,,/home/user/studies/QFS-evaluation/articles/Evolutionary Computation LLM.md,Neural Architecture Search (NAS) as both a target for optimization and a domain for synergy.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": """"}"
2025-08-28T07:55:22Z,sections-0-0-update-gemini,,,/home/user/studies/QFS-evaluation/articles/MemOS.md,The framework for accountable and governed memory in multi-agent systems.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": """"}"
2025-08-28T07:55:22Z,sections-0-0-update-gemini,,,/home/user/studies/QFS-evaluation/articles/Learn Beyond The Answer.md,Efficiency trade-offs in LLM data augmentation strategies.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": """"}"
2025-08-28T07:55:22Z,sections-0-0-update-gemini,,,/home/user/studies/QFS-evaluation/articles/Learn Beyond The Answer.md,"A framework for categorizing mathematical reasoning skills in language models, from basic forward reasoning to complex reflective capabilities.",10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": """"}"
2025-08-28T07:55:22Z,sections-0-0-update-gemini,,,/home/user/studies/QFS-evaluation/articles/Hierarchical-Reasoning-Model.md,"An argument against the architectural and computational limitations of the Transformer/Chain-of-Thought paradigm for genuine, latent algorithmic reasoning.",10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": """"}"
2025-08-28T07:55:22Z,sections-0-0-update-gemini,,,/home/user/studies/QFS-evaluation/articles/Hierarchical-Reasoning-Model.md,"Applying principles of hierarchical processing, temporal separation, and local credit assignment from neuroscience to engineer stable, deep reasoning in artificial neural networks.",10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": """"}"
2025-08-28T07:55:22Z,sections-0-0-update-gemini,,,/home/user/studies/QFS-evaluation/articles/Hierarchical-Reasoning-Model.md,"Novel training strategies for recurrent architectures, combining BPTT-free approximate gradients, deep supervision, and reinforcement learning for adaptive computation.",10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": """"}"
2025-08-28T07:55:22Z,sections-0-0-update-gemini,,,/home/user/studies/QFS-evaluation/articles/Learn Beyond The Answer.md,"The interplay between data quantity, quality, and complexity in fine-tuning language models for mathematical tasks.",10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": """"}"
2025-08-28T07:55:22Z,sections-0-0-update-gemini,,,/home/user/studies/QFS-evaluation/articles/MemOS.md,Mechanisms for the lifecycle and cross-type transformation of memory.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": """"}"
2025-08-28T07:55:22Z,sections-0-0-update-gemini,,,/home/user/studies/QFS-evaluation/articles/MemOS.md,The vision for a collaborative and decentralized memory sharing ecosystem.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": """"}"
2025-08-28T07:55:22Z,sections-0-0-update-gemini,,,/home/user/studies/QFS-evaluation/articles/Hierarchical-Reasoning-Model.md,"Mechanisms for achieving and leveraging effective computational depth, using ""hierarchical convergence"" to overcome the vanishing gradient and premature convergence problems in recurrent systems.",10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": """"}"
2025-08-28T07:55:22Z,sections-0-0-update-gemini,,,/home/user/studies/QFS-evaluation/articles/Reinforcement Learning for Reasoning in Small LLMs.md,Causes of optimization instability and performance degradation during RL fine-tuning of small language models.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": """"}"
2025-08-28T07:55:22Z,sections-0-0-update-gemini,,,/home/user/studies/QFS-evaluation/articles/Random Teachers are Good Teachers.md,The role of the input data distribution in shaping the learned representations.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": """"}"
2025-08-28T07:55:22Z,sections-0-0-update-gemini,,,/home/user/studies/QFS-evaluation/articles/MemOS.md,"The role of structured metadata in enabling the scheduling, governance, and evolution of memory.",10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": """"}"
2025-08-28T07:55:22Z,sections-0-0-update-gemini,,,/home/user/studies/QFS-evaluation/articles/Random Teachers are Good Teachers.md,The asymmetric nature of the loss landscape around a random initialization.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": """"}"
2025-08-28T07:55:22Z,sections-0-0-update-gemini,,,/home/user/studies/QFS-evaluation/articles/Random Teachers are Good Teachers.md,The impact of student initialization proximity to the teacher.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": """"}"
2025-08-28T07:55:22Z,sections-0-0-update-gemini,,,/home/user/studies/QFS-evaluation/articles/Random Teachers are Good Teachers.md,The nature of implicit regularization induced by the teacher-student dynamics.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": """"}"
2025-08-28T07:55:22Z,sections-0-0-update-gemini,,,/home/user/studies/QFS-evaluation/articles/Reinforcement Learning for Reasoning in Small LLMs.md,The impact of output length constraints on the stability and efficacy of training LLMs for complex reasoning tasks.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": """"}"
2025-08-28T07:55:22Z,sections-0-0-update-gemini,,,/home/user/studies/QFS-evaluation/articles/Random Teachers are Good Teachers.md,How random teacher distillation pre-conditions a network for supervised training.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": """"}"
2025-08-28T07:55:22Z,sections-0-0-update-gemini,,,/home/user/studies/QFS-evaluation/articles/Rethinking Training Signals in RLVR.md,"The divergent behaviors of different model families (Qwen, Llama, OLMo) under identical RLVR training, highlighting a fundamental lack of technique generalization.",10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": """"}"
2025-08-28T07:55:22Z,sections-0-0-update-gemini,,,/home/user/studies/QFS-evaluation/articles/Reinforcement Learning for Reasoning in Small LLMs.md,A comparative economic analysis of achieving state-of-the-art reasoning performance in small vs. large language models.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": """"}"
2025-08-28T07:55:22Z,sections-0-0-update-gemini,,,/home/user/studies/QFS-evaluation/articles/Reinforcement Learning for Reasoning in Small LLMs.md,The challenge of managing multilingual language drift when fine-tuning a foundational model for a monolingual reasoning task.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": """"}"
2025-08-28T07:55:22Z,sections-0-0-update-gemini,,,/home/user/studies/QFS-evaluation/articles/Task Singular Vectors.md,An argument for structure-aware model manipulation over flattened parameter approaches.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": """"}"
2025-08-28T07:55:22Z,sections-0-0-update-gemini,,,/home/user/studies/QFS-evaluation/articles/Rethinking Training Signals in RLVR.md,"A critique of model-specific conclusions in RL research, using the Qwen model family as a case study for how easily performance gains can be achieved with spurious signals.",10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": """"}"
2025-08-28T07:55:22Z,sections-0-0-update-gemini,,,/home/user/studies/QFS-evaluation/articles/Rethinking Training Signals in RLVR.md,"An analysis of how superficial interventions, such as prompting or rewarding simple syntactic patterns, can elicit complex reasoning behaviors in certain models.",10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": """"}"
2025-08-28T07:55:22Z,sections-0-0-update-gemini,,,/home/user/studies/QFS-evaluation/articles/Rethinking Training Signals in RLVR.md,"The role of optimization algorithm artifacts, specifically the clipping mechanism in GRPO, in creating a directional training signal from pure noise.",10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": """"}"
2025-08-28T07:55:22Z,sections-0-0-update-gemini,,,/home/user/studies/QFS-evaluation/articles/Task Singular Vectors.md,The application of Orthogonal Procrustes analysis to decorrelate task-specific feature bases.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": """"}"
2025-08-28T07:55:22Z,sections-0-0-update-gemini,,,/home/user/studies/QFS-evaluation/articles/Rethinking Training Signals in RLVR.md,"The influence of a model's pretrained latent abilities (like code reasoning) on the outcomes of reinforcement learning, irrespective of the reward signal's quality.",10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": """"}"
2025-08-28T07:55:22Z,sections-0-0-update-gemini,,,/home/user/studies/QFS-evaluation/articles/MemOS.md,Parallels between the MemOS architecture and cognitive models of memory.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": """"}"
2025-08-28T07:55:22Z,sections-0-0-update-gemini,,,/home/user/studies/QFS-evaluation/articles/Task Singular Vectors.md,The counter-intuitive necessity of low-rank approximation for effective interference reduction.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": """"}"
2025-08-28T07:55:22Z,sections-0-0-update-gemini,,,/home/user/studies/QFS-evaluation/articles/Task Singular Vectors.md,The geometric interpretation of task interference through singular vector alignment.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": """"}"
2025-08-28T07:55:22Z,sections-0-0-update-gemini,,,/home/user/studies/QFS-evaluation/articles/distillation of SOTA embedding models.md,Self-distillation as a mechanism for post-hoc modality alignment.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": """"}"
2025-08-28T07:55:22Z,sections-0-0-update-gemini,,,/home/user/studies/QFS-evaluation/articles/distillation of SOTA embedding models.md,The progressive constraint strategy in knowledge distillation.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": """"}"
2025-08-28T07:55:22Z,sections-0-0-update-gemini,,,/home/user/studies/QFS-evaluation/articles/Task Singular Vectors.md,The paper's empirical argument for eliminating the scaling coefficient hyperparameter.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": """"}"
2025-08-28T07:55:22Z,sections-0-0-update-gemini,,,/home/user/studies/QFS-evaluation/articles/The-Linear-Representation-Hypothesis.md,How the proposed 'causal inner product' serves as a bridge to unify the conceptually separate input (embedding) and output (unembedding) representation spaces.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": """"}"
2025-08-28T07:55:22Z,sections-0-0-update-gemini,,,/home/user/studies/QFS-evaluation/articles/distillation of SOTA embedding models.md,Methodology for fusing knowledge from heterogeneous teacher models.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": """"}"
2025-08-28T07:55:22Z,sections-0-0-update-gemini,,,/home/user/studies/QFS-evaluation/articles/distillation of SOTA embedding models.md,Rationale for the staged training methodology.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": """"}"
2025-08-28T07:55:22Z,sections-0-0-update-gemini,,,/home/user/studies/QFS-evaluation/articles/distillation of SOTA embedding models.md,The paper's dual approach to the engineering problem of embedding dimensionality.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": """"}"
2025-08-28T07:55:22Z,sections-0-0-update-gemini,,,/home/user/studies/QFS-evaluation/articles/The-Linear-Representation-Hypothesis.md,The article's use of counterfactuals as a formal language to ground and connect disparate intuitions about linear representations.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": """"}"
2025-08-28T07:55:22Z,sections-0-0-update-gemini,,,/home/user/studies/QFS-evaluation/articles/The-Linear-Representation-Hypothesis.md,"Providing a single geometric framework that formally connects the subspace hypothesis (e.g., word2vec analogies) with the practical techniques of linear probing and activation steering.",10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": """"}"
2025-08-28T07:55:22Z,sections-0-0-update-gemini,,,/home/user/studies/QFS-evaluation/articles/rStar-Math.md,The role of code execution as a verifier for synthetic reasoning data.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": """"}"
2025-08-28T07:55:22Z,sections-0-0-update-gemini,,,/home/user/studies/QFS-evaluation/articles/rStar-Math.md,The viability of self-evolution as an alternative to knowledge distillation for creating state-of-the-art training datasets.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": """"}"
2025-08-28T07:55:22Z,sections-0-0-update-gemini,,,/home/user/studies/QFS-evaluation/articles/rStar-Math.md,The emergence of self-correction capabilities as an intrinsic byproduct of the MCTS-based reasoning process.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": """"}"
2025-08-28T07:55:22Z,sections-0-0-update-gemini,,,/home/user/studies/QFS-evaluation/articles/The-Linear-Representation-Hypothesis.md,"The derivation of a semantically meaningful geometry from the statistical properties of the model's full vocabulary, rather than from its training data distribution.",10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": """"}"
2025-08-28T07:55:22Z,sections-0-0-update-gemini,,,/home/user/studies/QFS-evaluation/articles/rStar-Math.md,Novel training methodologies for process reward models that bypass noisy score annotation.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": """"}"
2025-08-28T08:06:29Z,sections-0-1-update-SummarizerModel,,,/home/user/studies/QFS-evaluation/articles/Attention-Is-All-You-Need.md,The practical challenges and solutions for stabilizing dot-product attention.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""SummarizerModel"": ""gemini-2.5-flash"", ""commitMessage"": ""make the quastions easy""}"
2025-08-28T08:06:29Z,sections-0-1-update-SummarizerModel,,,/home/user/studies/QFS-evaluation/articles/Attention-Is-All-You-Need.md,Methods for encoding sequential order in non-recurrent architectures.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""SummarizerModel"": ""gemini-2.5-flash"", ""commitMessage"": ""make the quastions easy""}"
2025-08-28T08:06:29Z,sections-0-1-update-SummarizerModel,,,/home/user/studies/QFS-evaluation/articles/Forget-What-You-Know-about-LLMs.md,The impact of model scale on overfitting vulnerability.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""SummarizerModel"": ""gemini-2.5-flash"", ""commitMessage"": ""make the quastions easy""}"
2025-08-28T08:06:29Z,sections-0-1-update-SummarizerModel,,,/home/user/studies/QFS-evaluation/articles/Attention-Is-All-You-Need.md,Computational efficiency and theoretical limits of sequence modeling.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""SummarizerModel"": ""gemini-2.5-flash"", ""commitMessage"": ""make the quastions easy""}"
2025-08-28T08:06:29Z,sections-0-1-update-SummarizerModel,,,/home/user/studies/QFS-evaluation/articles/Evolutionary Computation LLM.md,The article's perspective on the theme of black-box optimization.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""SummarizerModel"": ""gemini-2.5-flash"", ""commitMessage"": ""make the quastions easy""}"
2025-08-28T08:06:29Z,sections-0-1-update-SummarizerModel,,,/home/user/studies/QFS-evaluation/articles/Attention-Is-All-You-Need.md,The role of regularization and optimization schemes in training large-scale models.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""SummarizerModel"": ""gemini-2.5-flash"", ""commitMessage"": ""make the quastions easy""}"
2025-08-28T08:06:29Z,sections-0-1-update-SummarizerModel,,,/home/user/studies/QFS-evaluation/articles/Hierarchical-Reasoning-Model.md,"An argument against the architectural and computational limitations of the Transformer/Chain-of-Thought paradigm for genuine, latent algorithmic reasoning.",10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""SummarizerModel"": ""gemini-2.5-flash"", ""commitMessage"": ""make the quastions easy""}"
2025-08-28T08:06:29Z,sections-0-1-update-SummarizerModel,,,/home/user/studies/QFS-evaluation/articles/Forget-What-You-Know-about-LLMs.md,The paper's critique of a leaderboard-centric evaluation culture in NLP.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""SummarizerModel"": ""gemini-2.5-flash"", ""commitMessage"": ""make the quastions easy""}"
2025-08-28T08:06:29Z,sections-0-1-update-SummarizerModel,,,/home/user/studies/QFS-evaluation/articles/Attention-Is-All-You-Need.md,"Enhancing representational power through parallel, subspace-specific attention.",10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""SummarizerModel"": ""gemini-2.5-flash"", ""commitMessage"": ""make the quastions easy""}"
2025-08-28T08:06:29Z,sections-0-1-update-SummarizerModel,,,/home/user/studies/QFS-evaluation/articles/Learn Beyond The Answer.md,Efficiency trade-offs in LLM data augmentation strategies.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""SummarizerModel"": ""gemini-2.5-flash"", ""commitMessage"": ""make the quastions easy""}"
2025-08-28T08:06:29Z,sections-0-1-update-SummarizerModel,,,/home/user/studies/QFS-evaluation/articles/Evolutionary Computation LLM.md,The LLM as a substitute for evolutionary operators.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""SummarizerModel"": ""gemini-2.5-flash"", ""commitMessage"": ""make the quastions easy""}"
2025-08-28T08:06:29Z,sections-0-1-update-SummarizerModel,,,/home/user/studies/QFS-evaluation/articles/Forget-What-You-Know-about-LLMs.md,The methodology of using generative models to create adversarial evaluation datasets.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""SummarizerModel"": ""gemini-2.5-flash"", ""commitMessage"": ""make the quastions easy""}"
2025-08-28T08:06:29Z,sections-0-1-update-SummarizerModel,,,/home/user/studies/QFS-evaluation/articles/Evolutionary Computation LLM.md,The dual role of the LLM-EA synergy in the context of model and code security.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""SummarizerModel"": ""gemini-2.5-flash"", ""commitMessage"": ""make the quastions easy""}"
2025-08-28T08:06:29Z,sections-0-1-update-SummarizerModel,,,/home/user/studies/QFS-evaluation/articles/Forget-What-You-Know-about-LLMs.md,Differential robustness across distinct LLM architectural families.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""SummarizerModel"": ""gemini-2.5-flash"", ""commitMessage"": ""make the quastions easy""}"
2025-08-28T08:06:29Z,sections-0-1-update-SummarizerModel,,,/home/user/studies/QFS-evaluation/articles/Learn Beyond The Answer.md,"The interplay between data quantity, quality, and complexity in fine-tuning language models for mathematical tasks.",10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""SummarizerModel"": ""gemini-2.5-flash"", ""commitMessage"": ""make the quastions easy""}"
2025-08-28T08:06:29Z,sections-0-1-update-SummarizerModel,,,/home/user/studies/QFS-evaluation/articles/Evolutionary Computation LLM.md,Neural Architecture Search (NAS) as both a target for optimization and a domain for synergy.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""SummarizerModel"": ""gemini-2.5-flash"", ""commitMessage"": ""make the quastions easy""}"
2025-08-28T08:06:29Z,sections-0-1-update-SummarizerModel,,,/home/user/studies/QFS-evaluation/articles/Learn Beyond The Answer.md,The role and limitations of proprietary expert models in synthetic data generation for LLM research.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""SummarizerModel"": ""gemini-2.5-flash"", ""commitMessage"": ""make the quastions easy""}"
2025-08-28T08:06:29Z,sections-0-1-update-SummarizerModel,,,/home/user/studies/QFS-evaluation/articles/Learn Beyond The Answer.md,Comparing instance-dimension vs. sequence-dimension data augmentation for enhancing LLM reasoning.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""SummarizerModel"": ""gemini-2.5-flash"", ""commitMessage"": ""make the quastions easy""}"
2025-08-28T08:06:29Z,sections-0-1-update-SummarizerModel,,,/home/user/studies/QFS-evaluation/articles/Forget-What-You-Know-about-LLMs.md,The paradox of high accuracy as an indicator of fragility.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""SummarizerModel"": ""gemini-2.5-flash"", ""commitMessage"": ""make the quastions easy""}"
2025-08-28T08:06:29Z,sections-0-1-update-SummarizerModel,,,/home/user/studies/QFS-evaluation/articles/MemOS.md,"The role of structured metadata in enabling the scheduling, governance, and evolution of memory.",10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""SummarizerModel"": ""gemini-2.5-flash"", ""commitMessage"": ""make the quastions easy""}"
2025-08-28T08:06:29Z,sections-0-1-update-SummarizerModel,,,/home/user/studies/QFS-evaluation/articles/Random Teachers are Good Teachers.md,The asymmetric nature of the loss landscape around a random initialization.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""SummarizerModel"": ""gemini-2.5-flash"", ""commitMessage"": ""make the quastions easy""}"
2025-08-28T08:06:29Z,sections-0-1-update-SummarizerModel,,,/home/user/studies/QFS-evaluation/articles/MemOS.md,The vision for a collaborative and decentralized memory sharing ecosystem.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""SummarizerModel"": ""gemini-2.5-flash"", ""commitMessage"": ""make the quastions easy""}"
2025-08-28T08:06:29Z,sections-0-1-update-SummarizerModel,,,/home/user/studies/QFS-evaluation/articles/Random Teachers are Good Teachers.md,How random teacher distillation pre-conditions a network for supervised training.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""SummarizerModel"": ""gemini-2.5-flash"", ""commitMessage"": ""make the quastions easy""}"
2025-08-28T08:06:29Z,sections-0-1-update-SummarizerModel,,,/home/user/studies/QFS-evaluation/articles/Hierarchical-Reasoning-Model.md,"Applying principles of hierarchical processing, temporal separation, and local credit assignment from neuroscience to engineer stable, deep reasoning in artificial neural networks.",10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""SummarizerModel"": ""gemini-2.5-flash"", ""commitMessage"": ""make the quastions easy""}"
2025-08-28T08:06:29Z,sections-0-1-update-SummarizerModel,,,/home/user/studies/QFS-evaluation/articles/MemOS.md,Mechanisms for the lifecycle and cross-type transformation of memory.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""SummarizerModel"": ""gemini-2.5-flash"", ""commitMessage"": ""make the quastions easy""}"
2025-08-28T08:06:29Z,sections-0-1-update-SummarizerModel,,,/home/user/studies/QFS-evaluation/articles/Hierarchical-Reasoning-Model.md,"Mechanisms for achieving and leveraging effective computational depth, using ""hierarchical convergence"" to overcome the vanishing gradient and premature convergence problems in recurrent systems.",10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""SummarizerModel"": ""gemini-2.5-flash"", ""commitMessage"": ""make the quastions easy""}"
2025-08-28T08:06:29Z,sections-0-1-update-SummarizerModel,,,/home/user/studies/QFS-evaluation/articles/Reinforcement Learning for Reasoning in Small LLMs.md,Causes of optimization instability and performance degradation during RL fine-tuning of small language models.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""SummarizerModel"": ""gemini-2.5-flash"", ""commitMessage"": ""make the quastions easy""}"
2025-08-28T08:06:29Z,sections-0-1-update-SummarizerModel,,,/home/user/studies/QFS-evaluation/articles/Hierarchical-Reasoning-Model.md,"Novel training strategies for recurrent architectures, combining BPTT-free approximate gradients, deep supervision, and reinforcement learning for adaptive computation.",10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""SummarizerModel"": ""gemini-2.5-flash"", ""commitMessage"": ""make the quastions easy""}"
2025-08-28T08:06:29Z,sections-0-1-update-SummarizerModel,,,/home/user/studies/QFS-evaluation/articles/Learn Beyond The Answer.md,"A framework for categorizing mathematical reasoning skills in language models, from basic forward reasoning to complex reflective capabilities.",10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""SummarizerModel"": ""gemini-2.5-flash"", ""commitMessage"": ""make the quastions easy""}"
2025-08-28T08:06:29Z,sections-0-1-update-SummarizerModel,,,/home/user/studies/QFS-evaluation/articles/Reinforcement Learning for Reasoning in Small LLMs.md,The challenge of managing multilingual language drift when fine-tuning a foundational model for a monolingual reasoning task.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""SummarizerModel"": ""gemini-2.5-flash"", ""commitMessage"": ""make the quastions easy""}"
2025-08-28T08:06:29Z,sections-0-1-update-SummarizerModel,,,/home/user/studies/QFS-evaluation/articles/Reinforcement Learning for Reasoning in Small LLMs.md,Strategies for curating compact datasets by balancing problem difficulty to enhance RL training efficiency under resource constraints.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""SummarizerModel"": ""gemini-2.5-flash"", ""commitMessage"": ""make the quastions easy""}"
2025-08-28T08:06:29Z,sections-0-1-update-SummarizerModel,,,/home/user/studies/QFS-evaluation/articles/Reinforcement Learning for Reasoning in Small LLMs.md,The impact of output length constraints on the stability and efficacy of training LLMs for complex reasoning tasks.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""SummarizerModel"": ""gemini-2.5-flash"", ""commitMessage"": ""make the quastions easy""}"
2025-08-28T08:06:29Z,sections-0-1-update-SummarizerModel,,,/home/user/studies/QFS-evaluation/articles/Random Teachers are Good Teachers.md,The impact of student initialization proximity to the teacher.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""SummarizerModel"": ""gemini-2.5-flash"", ""commitMessage"": ""make the quastions easy""}"
2025-08-28T08:06:29Z,sections-0-1-update-SummarizerModel,,,/home/user/studies/QFS-evaluation/articles/MemOS.md,The framework for accountable and governed memory in multi-agent systems.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""SummarizerModel"": ""gemini-2.5-flash"", ""commitMessage"": ""make the quastions easy""}"
2025-08-28T08:06:29Z,sections-0-1-update-SummarizerModel,,,/home/user/studies/QFS-evaluation/articles/Task Singular Vectors.md,An argument for structure-aware model manipulation over flattened parameter approaches.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""SummarizerModel"": ""gemini-2.5-flash"", ""commitMessage"": ""make the quastions easy""}"
2025-08-28T08:06:29Z,sections-0-1-update-SummarizerModel,,,/home/user/studies/QFS-evaluation/articles/Task Singular Vectors.md,The geometric interpretation of task interference through singular vector alignment.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""SummarizerModel"": ""gemini-2.5-flash"", ""commitMessage"": ""make the quastions easy""}"
2025-08-28T08:06:29Z,sections-0-1-update-SummarizerModel,,,/home/user/studies/QFS-evaluation/articles/MemOS.md,Parallels between the MemOS architecture and cognitive models of memory.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""SummarizerModel"": ""gemini-2.5-flash"", ""commitMessage"": ""make the quastions easy""}"
2025-08-28T08:06:29Z,sections-0-1-update-SummarizerModel,,,/home/user/studies/QFS-evaluation/articles/Rethinking Training Signals in RLVR.md,"An analysis of how superficial interventions, such as prompting or rewarding simple syntactic patterns, can elicit complex reasoning behaviors in certain models.",10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""SummarizerModel"": ""gemini-2.5-flash"", ""commitMessage"": ""make the quastions easy""}"
2025-08-28T08:06:29Z,sections-0-1-update-SummarizerModel,,,/home/user/studies/QFS-evaluation/articles/The-Linear-Representation-Hypothesis.md,"The paper as a solution to the geometric unidentifiability of representation spaces, resolved by imposing a causal structure.",10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""SummarizerModel"": ""gemini-2.5-flash"", ""commitMessage"": ""make the quastions easy""}"
2025-08-28T08:06:29Z,sections-0-1-update-SummarizerModel,,,/home/user/studies/QFS-evaluation/articles/Random Teachers are Good Teachers.md,The nature of implicit regularization induced by the teacher-student dynamics.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""SummarizerModel"": ""gemini-2.5-flash"", ""commitMessage"": ""make the quastions easy""}"
2025-08-28T08:06:29Z,sections-0-1-update-SummarizerModel,,,/home/user/studies/QFS-evaluation/articles/The-Linear-Representation-Hypothesis.md,How the proposed 'causal inner product' serves as a bridge to unify the conceptually separate input (embedding) and output (unembedding) representation spaces.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""SummarizerModel"": ""gemini-2.5-flash"", ""commitMessage"": ""make the quastions easy""}"
2025-08-28T08:06:29Z,sections-0-1-update-SummarizerModel,,,/home/user/studies/QFS-evaluation/articles/Random Teachers are Good Teachers.md,The role of the input data distribution in shaping the learned representations.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""SummarizerModel"": ""gemini-2.5-flash"", ""commitMessage"": ""make the quastions easy""}"
2025-08-28T08:06:29Z,sections-0-1-update-SummarizerModel,,,/home/user/studies/QFS-evaluation/articles/Rethinking Training Signals in RLVR.md,"The influence of a model's pretrained latent abilities (like code reasoning) on the outcomes of reinforcement learning, irrespective of the reward signal's quality.",10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""SummarizerModel"": ""gemini-2.5-flash"", ""commitMessage"": ""make the quastions easy""}"
2025-08-28T08:06:29Z,sections-0-1-update-SummarizerModel,,,/home/user/studies/QFS-evaluation/articles/The-Linear-Representation-Hypothesis.md,"Providing a single geometric framework that formally connects the subspace hypothesis (e.g., word2vec analogies) with the practical techniques of linear probing and activation steering.",10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""SummarizerModel"": ""gemini-2.5-flash"", ""commitMessage"": ""make the quastions easy""}"
2025-08-28T08:06:29Z,sections-0-1-update-SummarizerModel,,,/home/user/studies/QFS-evaluation/articles/Rethinking Training Signals in RLVR.md,"The divergent behaviors of different model families (Qwen, Llama, OLMo) under identical RLVR training, highlighting a fundamental lack of technique generalization.",10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""SummarizerModel"": ""gemini-2.5-flash"", ""commitMessage"": ""make the quastions easy""}"
2025-08-28T08:06:29Z,sections-0-1-update-SummarizerModel,,,/home/user/studies/QFS-evaluation/articles/The-Linear-Representation-Hypothesis.md,"The derivation of a semantically meaningful geometry from the statistical properties of the model's full vocabulary, rather than from its training data distribution.",10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""SummarizerModel"": ""gemini-2.5-flash"", ""commitMessage"": ""make the quastions easy""}"
2025-08-28T08:06:29Z,sections-0-1-update-SummarizerModel,,,/home/user/studies/QFS-evaluation/articles/Task Singular Vectors.md,The paper's empirical argument for eliminating the scaling coefficient hyperparameter.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""SummarizerModel"": ""gemini-2.5-flash"", ""commitMessage"": ""make the quastions easy""}"
2025-08-28T08:06:29Z,sections-0-1-update-SummarizerModel,,,/home/user/studies/QFS-evaluation/articles/Task Singular Vectors.md,The application of Orthogonal Procrustes analysis to decorrelate task-specific feature bases.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""SummarizerModel"": ""gemini-2.5-flash"", ""commitMessage"": ""make the quastions easy""}"
2025-08-28T08:06:29Z,sections-0-1-update-SummarizerModel,,,/home/user/studies/QFS-evaluation/articles/distillation of SOTA embedding models.md,Rationale for the staged training methodology.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""SummarizerModel"": ""gemini-2.5-flash"", ""commitMessage"": ""make the quastions easy""}"
2025-08-28T08:06:29Z,sections-0-1-update-SummarizerModel,,,/home/user/studies/QFS-evaluation/articles/The-Linear-Representation-Hypothesis.md,The article's use of counterfactuals as a formal language to ground and connect disparate intuitions about linear representations.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""SummarizerModel"": ""gemini-2.5-flash"", ""commitMessage"": ""make the quastions easy""}"
2025-08-28T08:06:29Z,sections-0-1-update-SummarizerModel,,,/home/user/studies/QFS-evaluation/articles/Rethinking Training Signals in RLVR.md,"A critique of model-specific conclusions in RL research, using the Qwen model family as a case study for how easily performance gains can be achieved with spurious signals.",10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""SummarizerModel"": ""gemini-2.5-flash"", ""commitMessage"": ""make the quastions easy""}"
2025-08-28T08:06:29Z,sections-0-1-update-SummarizerModel,,,/home/user/studies/QFS-evaluation/articles/distillation of SOTA embedding models.md,The paper's dual approach to the engineering problem of embedding dimensionality.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""SummarizerModel"": ""gemini-2.5-flash"", ""commitMessage"": ""make the quastions easy""}"
2025-08-28T08:06:29Z,sections-0-1-update-SummarizerModel,,,/home/user/studies/QFS-evaluation/articles/distillation of SOTA embedding models.md,The progressive constraint strategy in knowledge distillation.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""SummarizerModel"": ""gemini-2.5-flash"", ""commitMessage"": ""make the quastions easy""}"
2025-08-28T08:06:29Z,sections-0-1-update-SummarizerModel,,,/home/user/studies/QFS-evaluation/articles/distillation of SOTA embedding models.md,Methodology for fusing knowledge from heterogeneous teacher models.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""SummarizerModel"": ""gemini-2.5-flash"", ""commitMessage"": ""make the quastions easy""}"
2025-08-28T08:06:29Z,sections-0-1-update-SummarizerModel,,,/home/user/studies/QFS-evaluation/articles/rStar-Math.md,The viability of self-evolution as an alternative to knowledge distillation for creating state-of-the-art training datasets.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""SummarizerModel"": ""gemini-2.5-flash"", ""commitMessage"": ""make the quastions easy""}"
2025-08-28T08:06:29Z,sections-0-1-update-SummarizerModel,,,/home/user/studies/QFS-evaluation/articles/rStar-Math.md,Novel training methodologies for process reward models that bypass noisy score annotation.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""SummarizerModel"": ""gemini-2.5-flash"", ""commitMessage"": ""make the quastions easy""}"
2025-08-28T08:06:29Z,sections-0-1-update-SummarizerModel,,,/home/user/studies/QFS-evaluation/articles/Task Singular Vectors.md,The counter-intuitive necessity of low-rank approximation for effective interference reduction.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""SummarizerModel"": ""gemini-2.5-flash"", ""commitMessage"": ""make the quastions easy""}"
2025-08-28T08:06:29Z,sections-0-1-update-SummarizerModel,,,/home/user/studies/QFS-evaluation/articles/rStar-Math.md,The emergence of self-correction capabilities as an intrinsic byproduct of the MCTS-based reasoning process.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""SummarizerModel"": ""gemini-2.5-flash"", ""commitMessage"": ""make the quastions easy""}"
2025-08-28T08:06:29Z,sections-0-1-update-SummarizerModel,,,/home/user/studies/QFS-evaluation/articles/rStar-Math.md,The role of code execution as a verifier for synthetic reasoning data.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""SummarizerModel"": ""gemini-2.5-flash"", ""commitMessage"": ""make the quastions easy""}"
2025-08-28T08:06:29Z,sections-0-1-update-SummarizerModel,,,/home/user/studies/QFS-evaluation/articles/distillation of SOTA embedding models.md,Self-distillation as a mechanism for post-hoc modality alignment.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""SummarizerModel"": ""gemini-2.5-flash"", ""commitMessage"": ""make the quastions easy""}"
2025-08-28T10:19:05Z,auto-judge-0-0-fix-counter,,,/home/user/studies/QFS-evaluation/articles/Forget-What-You-Know-about-LLMs.md,The paper's critique of a leaderboard-centric evaluation culture in NLP.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""fix counter""}"
2025-08-28T10:19:05Z,auto-judge-0-0-fix-counter,,,/home/user/studies/QFS-evaluation/articles/Attention-Is-All-You-Need.md,The role of regularization and optimization schemes in training large-scale models.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""fix counter""}"
2025-08-28T10:19:05Z,auto-judge-0-0-fix-counter,,,/home/user/studies/QFS-evaluation/articles/Attention-Is-All-You-Need.md,Methods for encoding sequential order in non-recurrent architectures.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""fix counter""}"
2025-08-28T10:19:05Z,auto-judge-0-0-fix-counter,,,/home/user/studies/QFS-evaluation/articles/Attention-Is-All-You-Need.md,The practical challenges and solutions for stabilizing dot-product attention.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""fix counter""}"
2025-08-28T10:19:05Z,auto-judge-0-0-fix-counter,,,/home/user/studies/QFS-evaluation/articles/Forget-What-You-Know-about-LLMs.md,The paradox of high accuracy as an indicator of fragility.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""fix counter""}"
2025-08-28T10:19:05Z,auto-judge-0-0-fix-counter,,,/home/user/studies/QFS-evaluation/articles/Attention-Is-All-You-Need.md,"Enhancing representational power through parallel, subspace-specific attention.",10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""fix counter""}"
2025-08-28T10:19:05Z,auto-judge-0-0-fix-counter,,,/home/user/studies/QFS-evaluation/articles/Evolutionary Computation LLM.md,The application of multi-objective evolutionary principles throughout the surveyed research.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""fix counter""}"
2025-08-28T10:19:05Z,auto-judge-0-0-fix-counter,,,/home/user/studies/QFS-evaluation/articles/Evolutionary Computation LLM.md,Neural Architecture Search (NAS) as both a target for optimization and a domain for synergy.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""fix counter""}"
2025-08-28T10:19:05Z,auto-judge-0-0-fix-counter,,,/home/user/studies/QFS-evaluation/articles/Evolutionary Computation LLM.md,The article's perspective on the theme of black-box optimization.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""fix counter""}"
2025-08-28T10:19:05Z,auto-judge-0-0-fix-counter,,,/home/user/studies/QFS-evaluation/articles/Evolutionary Computation LLM.md,The LLM as a substitute for evolutionary operators.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""fix counter""}"
2025-08-28T10:19:05Z,auto-judge-0-0-fix-counter,,,/home/user/studies/QFS-evaluation/articles/Forget-What-You-Know-about-LLMs.md,The impact of model scale on overfitting vulnerability.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""fix counter""}"
2025-08-28T10:19:05Z,auto-judge-0-0-fix-counter,,,/home/user/studies/QFS-evaluation/articles/Forget-What-You-Know-about-LLMs.md,Differential robustness across distinct LLM architectural families.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""fix counter""}"
2025-08-28T10:19:05Z,auto-judge-0-0-fix-counter,,,/home/user/studies/QFS-evaluation/articles/MemOS.md,The framework for accountable and governed memory in multi-agent systems.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""fix counter""}"
2025-08-28T10:19:05Z,auto-judge-0-0-fix-counter,,,/home/user/studies/QFS-evaluation/articles/Hierarchical-Reasoning-Model.md,Emergent dimensionality hierarchy in a trained reasoning model as a parallel to the functional organization of the mouse cortex.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""fix counter""}"
2025-08-28T10:19:05Z,auto-judge-0-0-fix-counter,,,/home/user/studies/QFS-evaluation/articles/Learn Beyond The Answer.md,"A framework for categorizing mathematical reasoning skills in language models, from basic forward reasoning to complex reflective capabilities.",10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""fix counter""}"
2025-08-28T10:19:05Z,auto-judge-0-0-fix-counter,,,/home/user/studies/QFS-evaluation/articles/Learn Beyond The Answer.md,"The interplay between data quantity, quality, and complexity in fine-tuning language models for mathematical tasks.",10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""fix counter""}"
2025-08-28T10:19:05Z,auto-judge-0-0-fix-counter,,,/home/user/studies/QFS-evaluation/articles/Learn Beyond The Answer.md,Efficiency trade-offs in LLM data augmentation strategies.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""fix counter""}"
2025-08-28T10:19:05Z,auto-judge-0-0-fix-counter,,,/home/user/studies/QFS-evaluation/articles/Hierarchical-Reasoning-Model.md,"Mechanisms for achieving and leveraging effective computational depth, using ""hierarchical convergence"" to overcome the vanishing gradient and premature convergence problems in recurrent systems.",10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""fix counter""}"
2025-08-28T10:19:05Z,auto-judge-0-0-fix-counter,,,/home/user/studies/QFS-evaluation/articles/Forget-What-You-Know-about-LLMs.md,The methodology of using generative models to create adversarial evaluation datasets.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""fix counter""}"
2025-08-28T10:19:05Z,auto-judge-0-0-fix-counter,,,/home/user/studies/QFS-evaluation/articles/Rethinking Training Signals in RLVR.md,"The role of optimization algorithm artifacts, specifically the clipping mechanism in GRPO, in creating a directional training signal from pure noise.",10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""fix counter""}"
2025-08-28T10:19:05Z,auto-judge-0-0-fix-counter,,,/home/user/studies/QFS-evaluation/articles/Reinforcement Learning for Reasoning in Small LLMs.md,Causes of optimization instability and performance degradation during RL fine-tuning of small language models.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""fix counter""}"
2025-08-28T10:19:05Z,auto-judge-0-0-fix-counter,,,/home/user/studies/QFS-evaluation/articles/MemOS.md,Parallels between the MemOS architecture and cognitive models of memory.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""fix counter""}"
2025-08-28T10:19:05Z,auto-judge-0-0-fix-counter,,,/home/user/studies/QFS-evaluation/articles/Reinforcement Learning for Reasoning in Small LLMs.md,Strategies for curating compact datasets by balancing problem difficulty to enhance RL training efficiency under resource constraints.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""fix counter""}"
2025-08-28T10:19:05Z,auto-judge-0-0-fix-counter,,,/home/user/studies/QFS-evaluation/articles/Rethinking Training Signals in RLVR.md,"A critique of model-specific conclusions in RL research, using the Qwen model family as a case study for how easily performance gains can be achieved with spurious signals.",10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""fix counter""}"
2025-08-28T10:19:05Z,auto-judge-0-0-fix-counter,,,/home/user/studies/QFS-evaluation/articles/Reinforcement Learning for Reasoning in Small LLMs.md,The impact of output length constraints on the stability and efficacy of training LLMs for complex reasoning tasks.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""fix counter""}"
2025-08-28T10:19:05Z,auto-judge-0-0-fix-counter,,,/home/user/studies/QFS-evaluation/articles/Rethinking Training Signals in RLVR.md,"The influence of a model's pretrained latent abilities (like code reasoning) on the outcomes of reinforcement learning, irrespective of the reward signal's quality.",10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""fix counter""}"
2025-08-28T10:19:05Z,auto-judge-0-0-fix-counter,,,/home/user/studies/QFS-evaluation/articles/Rethinking Training Signals in RLVR.md,"An analysis of how superficial interventions, such as prompting or rewarding simple syntactic patterns, can elicit complex reasoning behaviors in certain models.",10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""fix counter""}"
2025-08-28T10:19:05Z,auto-judge-0-0-fix-counter,,,/home/user/studies/QFS-evaluation/articles/Rethinking Training Signals in RLVR.md,"The divergent behaviors of different model families (Qwen, Llama, OLMo) under identical RLVR training, highlighting a fundamental lack of technique generalization.",10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""fix counter""}"
2025-08-28T10:19:05Z,auto-judge-0-0-fix-counter,,,/home/user/studies/QFS-evaluation/articles/rStar-Math.md,The role of code execution as a verifier for synthetic reasoning data.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""fix counter""}"
2025-08-28T10:19:05Z,auto-judge-0-0-fix-counter,,,/home/user/studies/QFS-evaluation/articles/The-Linear-Representation-Hypothesis.md,How the proposed 'causal inner product' serves as a bridge to unify the conceptually separate input (embedding) and output (unembedding) representation spaces.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""fix counter""}"
2025-08-28T10:19:05Z,auto-judge-0-0-fix-counter,,,/home/user/studies/QFS-evaluation/articles/The-Linear-Representation-Hypothesis.md,The article's use of counterfactuals as a formal language to ground and connect disparate intuitions about linear representations.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""fix counter""}"
2025-08-28T10:19:05Z,auto-judge-0-0-fix-counter,,,/home/user/studies/QFS-evaluation/articles/distillation of SOTA embedding models.md,Rationale for the staged training methodology.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""fix counter""}"
2025-08-28T10:19:05Z,auto-judge-0-0-fix-counter,,,/home/user/studies/QFS-evaluation/articles/distillation of SOTA embedding models.md,The progressive constraint strategy in knowledge distillation.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""fix counter""}"
2025-08-28T10:19:05Z,auto-judge-0-0-fix-counter,,,/home/user/studies/QFS-evaluation/articles/distillation of SOTA embedding models.md,The paper's dual approach to the engineering problem of embedding dimensionality.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""fix counter""}"
2025-08-28T10:19:05Z,auto-judge-0-0-fix-counter,,,/home/user/studies/QFS-evaluation/articles/distillation of SOTA embedding models.md,Self-distillation as a mechanism for post-hoc modality alignment.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""fix counter""}"
2025-08-28T10:19:05Z,auto-judge-0-0-fix-counter,,,/home/user/studies/QFS-evaluation/articles/Task Singular Vectors.md,The application of Orthogonal Procrustes analysis to decorrelate task-specific feature bases.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""fix counter""}"
2025-08-28T10:19:05Z,auto-judge-0-0-fix-counter,,,/home/user/studies/QFS-evaluation/articles/The-Linear-Representation-Hypothesis.md,"Providing a single geometric framework that formally connects the subspace hypothesis (e.g., word2vec analogies) with the practical techniques of linear probing and activation steering.",10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""fix counter""}"
2025-08-28T10:19:05Z,auto-judge-0-0-fix-counter,,,/home/user/studies/QFS-evaluation/articles/Task Singular Vectors.md,An argument for structure-aware model manipulation over flattened parameter approaches.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""fix counter""}"
2025-08-28T10:19:05Z,auto-judge-0-0-fix-counter,,,/home/user/studies/QFS-evaluation/articles/Task Singular Vectors.md,The paper's empirical argument for eliminating the scaling coefficient hyperparameter.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""fix counter""}"
2025-08-28T10:19:05Z,auto-judge-0-0-fix-counter,,,/home/user/studies/QFS-evaluation/articles/rStar-Math.md,Novel training methodologies for process reward models that bypass noisy score annotation.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""fix counter""}"
2025-08-28T10:44:03Z,auto-judge-0-1-update-prompt,,,/home/user/studies/QFS-evaluation/articles/Attention-Is-All-You-Need.md,Methods for encoding sequential order in non-recurrent architectures.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""update prompt to easy ACU""}"
2025-08-28T10:44:03Z,auto-judge-0-1-update-prompt,,,/home/user/studies/QFS-evaluation/articles/Evolutionary Computation LLM.md,The dual role of the LLM-EA synergy in the context of model and code security.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""update prompt to easy ACU""}"
2025-08-28T10:44:03Z,auto-judge-0-1-update-prompt,,,/home/user/studies/QFS-evaluation/articles/Attention-Is-All-You-Need.md,The practical challenges and solutions for stabilizing dot-product attention.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""update prompt to easy ACU""}"
2025-08-28T10:44:03Z,auto-judge-0-1-update-prompt,,,/home/user/studies/QFS-evaluation/articles/Attention-Is-All-You-Need.md,"Enhancing representational power through parallel, subspace-specific attention.",10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""update prompt to easy ACU""}"
2025-08-28T10:44:03Z,auto-judge-0-1-update-prompt,,,/home/user/studies/QFS-evaluation/articles/Evolutionary Computation LLM.md,Neural Architecture Search (NAS) as both a target for optimization and a domain for synergy.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""update prompt to easy ACU""}"
2025-08-28T10:44:03Z,auto-judge-0-1-update-prompt,,,/home/user/studies/QFS-evaluation/articles/Evolutionary Computation LLM.md,The application of multi-objective evolutionary principles throughout the surveyed research.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""update prompt to easy ACU""}"
2025-08-28T10:44:03Z,auto-judge-0-1-update-prompt,,,/home/user/studies/QFS-evaluation/articles/Evolutionary Computation LLM.md,The LLM as a substitute for evolutionary operators.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""update prompt to easy ACU""}"
2025-08-28T10:44:03Z,auto-judge-0-1-update-prompt,,,/home/user/studies/QFS-evaluation/articles/Attention-Is-All-You-Need.md,Computational efficiency and theoretical limits of sequence modeling.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""update prompt to easy ACU""}"
2025-08-28T10:44:03Z,auto-judge-0-1-update-prompt,,,/home/user/studies/QFS-evaluation/articles/Forget-What-You-Know-about-LLMs.md,The paper's critique of a leaderboard-centric evaluation culture in NLP.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""update prompt to easy ACU""}"
2025-08-28T10:44:03Z,auto-judge-0-1-update-prompt,,,/home/user/studies/QFS-evaluation/articles/Forget-What-You-Know-about-LLMs.md,The paradox of high accuracy as an indicator of fragility.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""update prompt to easy ACU""}"
2025-08-28T10:44:03Z,auto-judge-0-1-update-prompt,,,/home/user/studies/QFS-evaluation/articles/Forget-What-You-Know-about-LLMs.md,The impact of model scale on overfitting vulnerability.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""update prompt to easy ACU""}"
2025-08-28T10:44:03Z,auto-judge-0-1-update-prompt,,,/home/user/studies/QFS-evaluation/articles/Forget-What-You-Know-about-LLMs.md,Differential robustness across distinct LLM architectural families.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""update prompt to easy ACU""}"
2025-08-28T10:44:03Z,auto-judge-0-1-update-prompt,,,/home/user/studies/QFS-evaluation/articles/Hierarchical-Reasoning-Model.md,"An argument against the architectural and computational limitations of the Transformer/Chain-of-Thought paradigm for genuine, latent algorithmic reasoning.",10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""update prompt to easy ACU""}"
2025-08-28T10:44:03Z,auto-judge-0-1-update-prompt,,,/home/user/studies/QFS-evaluation/articles/Learn Beyond The Answer.md,"A framework for categorizing mathematical reasoning skills in language models, from basic forward reasoning to complex reflective capabilities.",10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""update prompt to easy ACU""}"
2025-08-28T10:44:03Z,auto-judge-0-1-update-prompt,,,/home/user/studies/QFS-evaluation/articles/Hierarchical-Reasoning-Model.md,"Applying principles of hierarchical processing, temporal separation, and local credit assignment from neuroscience to engineer stable, deep reasoning in artificial neural networks.",10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""update prompt to easy ACU""}"
2025-08-28T10:44:03Z,auto-judge-0-1-update-prompt,,,/home/user/studies/QFS-evaluation/articles/MemOS.md,"The role of structured metadata in enabling the scheduling, governance, and evolution of memory.",10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""update prompt to easy ACU""}"
2025-08-28T10:44:03Z,auto-judge-0-1-update-prompt,,,/home/user/studies/QFS-evaluation/articles/Evolutionary Computation LLM.md,The article's perspective on the theme of black-box optimization.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""update prompt to easy ACU""}"
2025-08-28T10:44:03Z,auto-judge-0-1-update-prompt,,,/home/user/studies/QFS-evaluation/articles/MemOS.md,The framework for accountable and governed memory in multi-agent systems.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""update prompt to easy ACU""}"
2025-08-28T10:44:03Z,auto-judge-0-1-update-prompt,,,/home/user/studies/QFS-evaluation/articles/Learn Beyond The Answer.md,"The interplay between data quantity, quality, and complexity in fine-tuning language models for mathematical tasks.",10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""update prompt to easy ACU""}"
2025-08-28T10:44:03Z,auto-judge-0-1-update-prompt,,,/home/user/studies/QFS-evaluation/articles/Random Teachers are Good Teachers.md,The nature of implicit regularization induced by the teacher-student dynamics.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""update prompt to easy ACU""}"
2025-08-28T10:44:03Z,auto-judge-0-1-update-prompt,,,/home/user/studies/QFS-evaluation/articles/Reinforcement Learning for Reasoning in Small LLMs.md,The impact of output length constraints on the stability and efficacy of training LLMs for complex reasoning tasks.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""update prompt to easy ACU""}"
2025-08-28T10:44:03Z,auto-judge-0-1-update-prompt,,,/home/user/studies/QFS-evaluation/articles/Reinforcement Learning for Reasoning in Small LLMs.md,Strategies for curating compact datasets by balancing problem difficulty to enhance RL training efficiency under resource constraints.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""update prompt to easy ACU""}"
2025-08-28T10:44:03Z,auto-judge-0-1-update-prompt,,,/home/user/studies/QFS-evaluation/articles/Rethinking Training Signals in RLVR.md,"The influence of a model's pretrained latent abilities (like code reasoning) on the outcomes of reinforcement learning, irrespective of the reward signal's quality.",10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""update prompt to easy ACU""}"
2025-08-28T10:44:03Z,auto-judge-0-1-update-prompt,,,/home/user/studies/QFS-evaluation/articles/Reinforcement Learning for Reasoning in Small LLMs.md,A comparative economic analysis of achieving state-of-the-art reasoning performance in small vs. large language models.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""update prompt to easy ACU""}"
2025-08-28T10:44:03Z,auto-judge-0-1-update-prompt,,,/home/user/studies/QFS-evaluation/articles/Forget-What-You-Know-about-LLMs.md,The methodology of using generative models to create adversarial evaluation datasets.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""update prompt to easy ACU""}"
2025-08-28T10:44:03Z,auto-judge-0-1-update-prompt,,,/home/user/studies/QFS-evaluation/articles/Learn Beyond The Answer.md,Efficiency trade-offs in LLM data augmentation strategies.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""update prompt to easy ACU""}"
2025-08-28T10:44:03Z,auto-judge-0-1-update-prompt,,,/home/user/studies/QFS-evaluation/articles/Rethinking Training Signals in RLVR.md,"A critique of model-specific conclusions in RL research, using the Qwen model family as a case study for how easily performance gains can be achieved with spurious signals.",10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""update prompt to easy ACU""}"
2025-08-28T10:44:03Z,auto-judge-0-1-update-prompt,,,/home/user/studies/QFS-evaluation/articles/Rethinking Training Signals in RLVR.md,"The role of optimization algorithm artifacts, specifically the clipping mechanism in GRPO, in creating a directional training signal from pure noise.",10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""update prompt to easy ACU""}"
2025-08-28T10:44:03Z,auto-judge-0-1-update-prompt,,,/home/user/studies/QFS-evaluation/articles/Rethinking Training Signals in RLVR.md,"The divergent behaviors of different model families (Qwen, Llama, OLMo) under identical RLVR training, highlighting a fundamental lack of technique generalization.",10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""update prompt to easy ACU""}"
2025-08-28T10:44:03Z,auto-judge-0-1-update-prompt,,,/home/user/studies/QFS-evaluation/articles/Rethinking Training Signals in RLVR.md,"An analysis of how superficial interventions, such as prompting or rewarding simple syntactic patterns, can elicit complex reasoning behaviors in certain models.",10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""update prompt to easy ACU""}"
2025-08-28T10:44:03Z,auto-judge-0-1-update-prompt,,,/home/user/studies/QFS-evaluation/articles/The-Linear-Representation-Hypothesis.md,"Providing a single geometric framework that formally connects the subspace hypothesis (e.g., word2vec analogies) with the practical techniques of linear probing and activation steering.",10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""update prompt to easy ACU""}"
2025-08-28T10:44:03Z,auto-judge-0-1-update-prompt,,,/home/user/studies/QFS-evaluation/articles/The-Linear-Representation-Hypothesis.md,"The paper as a solution to the geometric unidentifiability of representation spaces, resolved by imposing a causal structure.",10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""update prompt to easy ACU""}"
2025-08-28T10:44:03Z,auto-judge-0-1-update-prompt,,,/home/user/studies/QFS-evaluation/articles/distillation of SOTA embedding models.md,The progressive constraint strategy in knowledge distillation.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""update prompt to easy ACU""}"
2025-08-28T10:44:03Z,auto-judge-0-1-update-prompt,,,/home/user/studies/QFS-evaluation/articles/The-Linear-Representation-Hypothesis.md,How the proposed 'causal inner product' serves as a bridge to unify the conceptually separate input (embedding) and output (unembedding) representation spaces.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""update prompt to easy ACU""}"
2025-08-28T10:44:03Z,auto-judge-0-1-update-prompt,,,/home/user/studies/QFS-evaluation/articles/distillation of SOTA embedding models.md,The paper's dual approach to the engineering problem of embedding dimensionality.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""update prompt to easy ACU""}"
2025-08-28T10:44:03Z,auto-judge-0-1-update-prompt,,,/home/user/studies/QFS-evaluation/articles/distillation of SOTA embedding models.md,Self-distillation as a mechanism for post-hoc modality alignment.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""update prompt to easy ACU""}"
2025-08-28T10:44:03Z,auto-judge-0-1-update-prompt,,,/home/user/studies/QFS-evaluation/articles/distillation of SOTA embedding models.md,Methodology for fusing knowledge from heterogeneous teacher models.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""update prompt to easy ACU""}"
2025-08-28T10:44:03Z,auto-judge-0-1-update-prompt,,,/home/user/studies/QFS-evaluation/articles/rStar-Math.md,The emergence of self-correction capabilities as an intrinsic byproduct of the MCTS-based reasoning process.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""update prompt to easy ACU""}"
2025-08-28T10:44:03Z,auto-judge-0-1-update-prompt,,,/home/user/studies/QFS-evaluation/articles/rStar-Math.md,Novel training methodologies for process reward models that bypass noisy score annotation.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""update prompt to easy ACU""}"
2025-08-28T10:44:03Z,auto-judge-0-1-update-prompt,,,/home/user/studies/QFS-evaluation/articles/Task Singular Vectors.md,An argument for structure-aware model manipulation over flattened parameter approaches.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""update prompt to easy ACU""}"
2025-08-28T11:09:31Z,auto-judge-0-2-fix-send-sections,,,/home/user/studies/QFS-evaluation/articles/Attention-Is-All-You-Need.md,The role of regularization and optimization schemes in training large-scale models.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""fix send sections""}"
2025-08-28T11:09:31Z,auto-judge-0-2-fix-send-sections,,,/home/user/studies/QFS-evaluation/articles/Evolutionary Computation LLM.md,The dual role of the LLM-EA synergy in the context of model and code security.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""fix send sections""}"
2025-08-28T11:09:31Z,auto-judge-0-2-fix-send-sections,,,/home/user/studies/QFS-evaluation/articles/Attention-Is-All-You-Need.md,The practical challenges and solutions for stabilizing dot-product attention.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""fix send sections""}"
2025-08-28T11:09:31Z,auto-judge-0-2-fix-send-sections,,,/home/user/studies/QFS-evaluation/articles/Attention-Is-All-You-Need.md,Computational efficiency and theoretical limits of sequence modeling.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""fix send sections""}"
2025-08-28T11:09:31Z,auto-judge-0-2-fix-send-sections,,,/home/user/studies/QFS-evaluation/articles/Attention-Is-All-You-Need.md,Methods for encoding sequential order in non-recurrent architectures.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""fix send sections""}"
2025-08-28T11:09:31Z,auto-judge-0-2-fix-send-sections,,,/home/user/studies/QFS-evaluation/articles/Evolutionary Computation LLM.md,The LLM as a substitute for evolutionary operators.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""fix send sections""}"
2025-08-28T11:09:31Z,auto-judge-0-2-fix-send-sections,,,/home/user/studies/QFS-evaluation/articles/Evolutionary Computation LLM.md,The application of multi-objective evolutionary principles throughout the surveyed research.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""fix send sections""}"
2025-08-28T11:09:31Z,auto-judge-0-2-fix-send-sections,,,/home/user/studies/QFS-evaluation/articles/Forget-What-You-Know-about-LLMs.md,The paradox of high accuracy as an indicator of fragility.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""fix send sections""}"
2025-08-28T11:09:31Z,auto-judge-0-2-fix-send-sections,,,/home/user/studies/QFS-evaluation/articles/Evolutionary Computation LLM.md,Neural Architecture Search (NAS) as both a target for optimization and a domain for synergy.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""fix send sections""}"
2025-08-28T11:09:31Z,auto-judge-0-2-fix-send-sections,,,/home/user/studies/QFS-evaluation/articles/Attention-Is-All-You-Need.md,"Enhancing representational power through parallel, subspace-specific attention.",10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""fix send sections""}"
2025-08-28T11:09:31Z,auto-judge-0-2-fix-send-sections,,,/home/user/studies/QFS-evaluation/articles/Forget-What-You-Know-about-LLMs.md,The paper's critique of a leaderboard-centric evaluation culture in NLP.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""fix send sections""}"
2025-08-28T11:09:31Z,auto-judge-0-2-fix-send-sections,,,/home/user/studies/QFS-evaluation/articles/Hierarchical-Reasoning-Model.md,"Novel training strategies for recurrent architectures, combining BPTT-free approximate gradients, deep supervision, and reinforcement learning for adaptive computation.",10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""fix send sections""}"
2025-08-28T11:09:31Z,auto-judge-0-2-fix-send-sections,,,/home/user/studies/QFS-evaluation/articles/Learn Beyond The Answer.md,"A framework for categorizing mathematical reasoning skills in language models, from basic forward reasoning to complex reflective capabilities.",10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""fix send sections""}"
2025-08-28T11:09:31Z,auto-judge-0-2-fix-send-sections,,,/home/user/studies/QFS-evaluation/articles/Forget-What-You-Know-about-LLMs.md,The impact of model scale on overfitting vulnerability.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""fix send sections""}"
2025-08-28T11:09:31Z,auto-judge-0-2-fix-send-sections,,,/home/user/studies/QFS-evaluation/articles/Learn Beyond The Answer.md,Comparing instance-dimension vs. sequence-dimension data augmentation for enhancing LLM reasoning.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""fix send sections""}"
2025-08-28T11:09:31Z,auto-judge-0-2-fix-send-sections,,,/home/user/studies/QFS-evaluation/articles/Forget-What-You-Know-about-LLMs.md,The methodology of using generative models to create adversarial evaluation datasets.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""fix send sections""}"
2025-08-28T11:09:31Z,auto-judge-0-2-fix-send-sections,,,/home/user/studies/QFS-evaluation/articles/Hierarchical-Reasoning-Model.md,"An argument against the architectural and computational limitations of the Transformer/Chain-of-Thought paradigm for genuine, latent algorithmic reasoning.",10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""fix send sections""}"
2025-08-28T11:09:31Z,auto-judge-0-2-fix-send-sections,,,/home/user/studies/QFS-evaluation/articles/Hierarchical-Reasoning-Model.md,Emergent dimensionality hierarchy in a trained reasoning model as a parallel to the functional organization of the mouse cortex.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""fix send sections""}"
2025-08-28T11:09:31Z,auto-judge-0-2-fix-send-sections,,,/home/user/studies/QFS-evaluation/articles/Hierarchical-Reasoning-Model.md,"Mechanisms for achieving and leveraging effective computational depth, using ""hierarchical convergence"" to overcome the vanishing gradient and premature convergence problems in recurrent systems.",10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""fix send sections""}"
2025-08-28T11:09:31Z,auto-judge-0-2-fix-send-sections,,,/home/user/studies/QFS-evaluation/articles/MemOS.md,"The role of structured metadata in enabling the scheduling, governance, and evolution of memory.",10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""fix send sections""}"
2025-08-28T11:09:31Z,auto-judge-0-2-fix-send-sections,,,/home/user/studies/QFS-evaluation/articles/Random Teachers are Good Teachers.md,The role of the input data distribution in shaping the learned representations.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""fix send sections""}"
2025-08-28T11:09:31Z,auto-judge-0-2-fix-send-sections,,,/home/user/studies/QFS-evaluation/articles/Evolutionary Computation LLM.md,The article's perspective on the theme of black-box optimization.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""fix send sections""}"
2025-08-28T11:09:31Z,auto-judge-0-2-fix-send-sections,,,/home/user/studies/QFS-evaluation/articles/MemOS.md,The vision for a collaborative and decentralized memory sharing ecosystem.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""fix send sections""}"
2025-08-28T11:09:31Z,auto-judge-0-2-fix-send-sections,,,/home/user/studies/QFS-evaluation/articles/Learn Beyond The Answer.md,"The interplay between data quantity, quality, and complexity in fine-tuning language models for mathematical tasks.",10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""fix send sections""}"
2025-08-28T11:09:31Z,auto-judge-0-2-fix-send-sections,,,/home/user/studies/QFS-evaluation/articles/Random Teachers are Good Teachers.md,How random teacher distillation pre-conditions a network for supervised training.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""fix send sections""}"
2025-08-28T11:09:31Z,auto-judge-0-2-fix-send-sections,,,/home/user/studies/QFS-evaluation/articles/Random Teachers are Good Teachers.md,The asymmetric nature of the loss landscape around a random initialization.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""fix send sections""}"
2025-08-28T11:09:31Z,auto-judge-0-2-fix-send-sections,,,/home/user/studies/QFS-evaluation/articles/Reinforcement Learning for Reasoning in Small LLMs.md,The impact of output length constraints on the stability and efficacy of training LLMs for complex reasoning tasks.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""fix send sections""}"
2025-08-28T11:09:31Z,auto-judge-0-2-fix-send-sections,,,/home/user/studies/QFS-evaluation/articles/Random Teachers are Good Teachers.md,The nature of implicit regularization induced by the teacher-student dynamics.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""fix send sections""}"
2025-08-28T11:09:31Z,auto-judge-0-2-fix-send-sections,,,/home/user/studies/QFS-evaluation/articles/Reinforcement Learning for Reasoning in Small LLMs.md,Strategies for curating compact datasets by balancing problem difficulty to enhance RL training efficiency under resource constraints.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""fix send sections""}"
2025-08-28T11:09:31Z,auto-judge-0-2-fix-send-sections,,,/home/user/studies/QFS-evaluation/articles/Reinforcement Learning for Reasoning in Small LLMs.md,Causes of optimization instability and performance degradation during RL fine-tuning of small language models.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""fix send sections""}"
2025-08-28T11:09:31Z,auto-judge-0-2-fix-send-sections,,,/home/user/studies/QFS-evaluation/articles/Reinforcement Learning for Reasoning in Small LLMs.md,The challenge of managing multilingual language drift when fine-tuning a foundational model for a monolingual reasoning task.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""fix send sections""}"
2025-08-28T11:09:31Z,auto-judge-0-2-fix-send-sections,,,/home/user/studies/QFS-evaluation/articles/Reinforcement Learning for Reasoning in Small LLMs.md,A comparative economic analysis of achieving state-of-the-art reasoning performance in small vs. large language models.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""fix send sections""}"
2025-08-28T11:09:31Z,auto-judge-0-2-fix-send-sections,,,/home/user/studies/QFS-evaluation/articles/Rethinking Training Signals in RLVR.md,"The influence of a model's pretrained latent abilities (like code reasoning) on the outcomes of reinforcement learning, irrespective of the reward signal's quality.",10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""fix send sections""}"
2025-08-28T11:09:31Z,auto-judge-0-2-fix-send-sections,,,/home/user/studies/QFS-evaluation/articles/Rethinking Training Signals in RLVR.md,"The role of optimization algorithm artifacts, specifically the clipping mechanism in GRPO, in creating a directional training signal from pure noise.",10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""fix send sections""}"
2025-08-28T11:09:31Z,auto-judge-0-2-fix-send-sections,,,/home/user/studies/QFS-evaluation/articles/Task Singular Vectors.md,An argument for structure-aware model manipulation over flattened parameter approaches.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""fix send sections""}"
2025-08-28T11:09:31Z,auto-judge-0-2-fix-send-sections,,,/home/user/studies/QFS-evaluation/articles/Task Singular Vectors.md,The paper's empirical argument for eliminating the scaling coefficient hyperparameter.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""fix send sections""}"
2025-08-28T11:09:31Z,auto-judge-0-2-fix-send-sections,,,/home/user/studies/QFS-evaluation/articles/Task Singular Vectors.md,The geometric interpretation of task interference through singular vector alignment.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""fix send sections""}"
2025-08-28T11:09:31Z,auto-judge-0-2-fix-send-sections,,,/home/user/studies/QFS-evaluation/articles/Task Singular Vectors.md,The counter-intuitive necessity of low-rank approximation for effective interference reduction.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""fix send sections""}"
2025-08-28T11:09:31Z,auto-judge-0-2-fix-send-sections,,,/home/user/studies/QFS-evaluation/articles/Rethinking Training Signals in RLVR.md,"The divergent behaviors of different model families (Qwen, Llama, OLMo) under identical RLVR training, highlighting a fundamental lack of technique generalization.",10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""fix send sections""}"
2025-08-28T11:09:31Z,auto-judge-0-2-fix-send-sections,,,/home/user/studies/QFS-evaluation/articles/Rethinking Training Signals in RLVR.md,"An analysis of how superficial interventions, such as prompting or rewarding simple syntactic patterns, can elicit complex reasoning behaviors in certain models.",10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""fix send sections""}"
2025-08-28T11:09:31Z,auto-judge-0-2-fix-send-sections,,,/home/user/studies/QFS-evaluation/articles/The-Linear-Representation-Hypothesis.md,"The derivation of a semantically meaningful geometry from the statistical properties of the model's full vocabulary, rather than from its training data distribution.",10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""fix send sections""}"
2025-08-28T11:09:31Z,auto-judge-0-2-fix-send-sections,,,/home/user/studies/QFS-evaluation/articles/The-Linear-Representation-Hypothesis.md,"The paper as a solution to the geometric unidentifiability of representation spaces, resolved by imposing a causal structure.",10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""fix send sections""}"
2025-08-28T11:09:31Z,auto-judge-0-2-fix-send-sections,,,/home/user/studies/QFS-evaluation/articles/distillation of SOTA embedding models.md,The paper's dual approach to the engineering problem of embedding dimensionality.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""fix send sections""}"
2025-08-28T11:09:31Z,auto-judge-0-2-fix-send-sections,,,/home/user/studies/QFS-evaluation/articles/The-Linear-Representation-Hypothesis.md,How the proposed 'causal inner product' serves as a bridge to unify the conceptually separate input (embedding) and output (unembedding) representation spaces.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""fix send sections""}"
2025-08-28T11:09:31Z,auto-judge-0-2-fix-send-sections,,,/home/user/studies/QFS-evaluation/articles/distillation of SOTA embedding models.md,Rationale for the staged training methodology.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""fix send sections""}"
2025-08-28T11:09:31Z,auto-judge-0-2-fix-send-sections,,,/home/user/studies/QFS-evaluation/articles/rStar-Math.md,The role of code execution as a verifier for synthetic reasoning data.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""fix send sections""}"
2025-08-28T11:09:31Z,auto-judge-0-2-fix-send-sections,,,/home/user/studies/QFS-evaluation/articles/distillation of SOTA embedding models.md,The progressive constraint strategy in knowledge distillation.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""fix send sections""}"
2025-08-28T11:09:31Z,auto-judge-0-2-fix-send-sections,,,/home/user/studies/QFS-evaluation/articles/rStar-Math.md,The emergence of self-correction capabilities as an intrinsic byproduct of the MCTS-based reasoning process.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""fix send sections""}"
2025-08-28T11:09:31Z,auto-judge-0-2-fix-send-sections,,,/home/user/studies/QFS-evaluation/articles/distillation of SOTA embedding models.md,Self-distillation as a mechanism for post-hoc modality alignment.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""fix send sections""}"
2025-08-28T11:09:31Z,auto-judge-0-2-fix-send-sections,,,/home/user/studies/QFS-evaluation/articles/rStar-Math.md,Novel training methodologies for process reward models that bypass noisy score annotation.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""fix send sections""}"
2025-08-28T11:09:31Z,auto-judge-0-2-fix-send-sections,,,/home/user/studies/QFS-evaluation/articles/distillation of SOTA embedding models.md,Methodology for fusing knowledge from heterogeneous teacher models.,10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""fix send sections""}"
2025-08-28T11:09:31Z,auto-judge-0-2-fix-send-sections,,,/home/user/studies/QFS-evaluation/articles/The-Linear-Representation-Hypothesis.md,"Providing a single geometric framework that formally connects the subspace hypothesis (e.g., word2vec analogies) with the practical techniques of linear probing and activation steering.",10,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""fix send sections""}"
2025-08-28T11:33:59Z,auto-judge-0-3-sections-as-set,,,/home/user/studies/QFS-evaluation/articles/Evolutionary Computation LLM.md,The dual role of the LLM-EA synergy in the context of model and code security.,15,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""sections as set""}"
2025-08-28T11:33:59Z,auto-judge-0-3-sections-as-set,,,/home/user/studies/QFS-evaluation/articles/Evolutionary Computation LLM.md,The application of multi-objective evolutionary principles throughout the surveyed research.,15,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""sections as set""}"
2025-08-28T11:33:59Z,auto-judge-0-3-sections-as-set,,,/home/user/studies/QFS-evaluation/articles/Hierarchical-Reasoning-Model.md,"Applying principles of hierarchical processing, temporal separation, and local credit assignment from neuroscience to engineer stable, deep reasoning in artificial neural networks.",15,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""sections as set""}"
2025-08-28T11:33:59Z,auto-judge-0-3-sections-as-set,,,/home/user/studies/QFS-evaluation/articles/Attention-Is-All-You-Need.md,"Enhancing representational power through parallel, subspace-specific attention.",15,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""sections as set""}"
2025-08-28T11:33:59Z,auto-judge-0-3-sections-as-set,,,/home/user/studies/QFS-evaluation/articles/Forget-What-You-Know-about-LLMs.md,Differential robustness across distinct LLM architectural families.,15,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""sections as set""}"
2025-08-28T11:33:59Z,auto-judge-0-3-sections-as-set,,,/home/user/studies/QFS-evaluation/articles/Attention-Is-All-You-Need.md,The role of regularization and optimization schemes in training large-scale models.,15,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""sections as set""}"
2025-08-28T11:33:59Z,auto-judge-0-3-sections-as-set,,,/home/user/studies/QFS-evaluation/articles/Evolutionary Computation LLM.md,The article's perspective on the theme of black-box optimization.,15,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""sections as set""}"
2025-08-28T11:33:59Z,auto-judge-0-3-sections-as-set,,,/home/user/studies/QFS-evaluation/articles/Learn Beyond The Answer.md,The role and limitations of proprietary expert models in synthetic data generation for LLM research.,15,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""sections as set""}"
2025-08-28T11:33:59Z,auto-judge-0-3-sections-as-set,,,/home/user/studies/QFS-evaluation/articles/Evolutionary Computation LLM.md,Neural Architecture Search (NAS) as both a target for optimization and a domain for synergy.,15,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""sections as set""}"
2025-08-28T11:33:59Z,auto-judge-0-3-sections-as-set,,,/home/user/studies/QFS-evaluation/articles/Evolutionary Computation LLM.md,The LLM as a substitute for evolutionary operators.,15,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""sections as set""}"
2025-08-28T11:33:59Z,auto-judge-0-3-sections-as-set,,,/home/user/studies/QFS-evaluation/articles/Forget-What-You-Know-about-LLMs.md,The paper's critique of a leaderboard-centric evaluation culture in NLP.,15,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""sections as set""}"
2025-08-28T11:33:59Z,auto-judge-0-3-sections-as-set,,,/home/user/studies/QFS-evaluation/articles/MemOS.md,The framework for accountable and governed memory in multi-agent systems.,15,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""sections as set""}"
2025-08-28T11:33:59Z,auto-judge-0-3-sections-as-set,,,/home/user/studies/QFS-evaluation/articles/Learn Beyond The Answer.md,"A framework for categorizing mathematical reasoning skills in language models, from basic forward reasoning to complex reflective capabilities.",15,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""sections as set""}"
2025-08-28T11:33:59Z,auto-judge-0-3-sections-as-set,,,/home/user/studies/QFS-evaluation/articles/Reinforcement Learning for Reasoning in Small LLMs.md,The impact of output length constraints on the stability and efficacy of training LLMs for complex reasoning tasks.,15,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""sections as set""}"
2025-08-28T11:33:59Z,auto-judge-0-3-sections-as-set,,,/home/user/studies/QFS-evaluation/articles/Learn Beyond The Answer.md,"The interplay between data quantity, quality, and complexity in fine-tuning language models for mathematical tasks.",15,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""sections as set""}"
2025-08-28T11:33:59Z,auto-judge-0-3-sections-as-set,,,/home/user/studies/QFS-evaluation/articles/Random Teachers are Good Teachers.md,The asymmetric nature of the loss landscape around a random initialization.,15,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""sections as set""}"
2025-08-28T11:33:59Z,auto-judge-0-3-sections-as-set,,,/home/user/studies/QFS-evaluation/articles/Reinforcement Learning for Reasoning in Small LLMs.md,The challenge of managing multilingual language drift when fine-tuning a foundational model for a monolingual reasoning task.,15,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""sections as set""}"
2025-08-28T11:33:59Z,auto-judge-0-3-sections-as-set,,,/home/user/studies/QFS-evaluation/articles/Forget-What-You-Know-about-LLMs.md,The methodology of using generative models to create adversarial evaluation datasets.,15,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""sections as set""}"
2025-08-28T11:33:59Z,auto-judge-0-3-sections-as-set,,,/home/user/studies/QFS-evaluation/articles/Reinforcement Learning for Reasoning in Small LLMs.md,Causes of optimization instability and performance degradation during RL fine-tuning of small language models.,15,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""sections as set""}"
2025-08-28T11:33:59Z,auto-judge-0-3-sections-as-set,,,/home/user/studies/QFS-evaluation/articles/Rethinking Training Signals in RLVR.md,"The influence of a model's pretrained latent abilities (like code reasoning) on the outcomes of reinforcement learning, irrespective of the reward signal's quality.",15,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""sections as set""}"
2025-08-28T11:33:59Z,auto-judge-0-3-sections-as-set,,,/home/user/studies/QFS-evaluation/articles/distillation of SOTA embedding models.md,Self-distillation as a mechanism for post-hoc modality alignment.,15,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""sections as set""}"
2025-08-28T11:33:59Z,auto-judge-0-3-sections-as-set,,,/home/user/studies/QFS-evaluation/articles/rStar-Math.md,The role of code execution as a verifier for synthetic reasoning data.,15,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""sections as set""}"
2025-08-28T11:33:59Z,auto-judge-0-3-sections-as-set,,,/home/user/studies/QFS-evaluation/articles/Rethinking Training Signals in RLVR.md,"A critique of model-specific conclusions in RL research, using the Qwen model family as a case study for how easily performance gains can be achieved with spurious signals.",15,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""sections as set""}"
2025-08-28T11:33:59Z,auto-judge-0-3-sections-as-set,,,/home/user/studies/QFS-evaluation/articles/The-Linear-Representation-Hypothesis.md,"The derivation of a semantically meaningful geometry from the statistical properties of the model's full vocabulary, rather than from its training data distribution.",15,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""sections as set""}"
2025-08-28T11:33:59Z,auto-judge-0-3-sections-as-set,,,/home/user/studies/QFS-evaluation/articles/Task Singular Vectors.md,The application of Orthogonal Procrustes analysis to decorrelate task-specific feature bases.,15,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""sections as set""}"
2025-08-28T11:33:59Z,auto-judge-0-3-sections-as-set,,,/home/user/studies/QFS-evaluation/articles/The-Linear-Representation-Hypothesis.md,The article's use of counterfactuals as a formal language to ground and connect disparate intuitions about linear representations.,15,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""sections as set""}"
2025-08-28T11:33:59Z,auto-judge-0-3-sections-as-set,,,/home/user/studies/QFS-evaluation/articles/Task Singular Vectors.md,An argument for structure-aware model manipulation over flattened parameter approaches.,15,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""sections as set""}"
2025-08-28T11:33:59Z,auto-judge-0-3-sections-as-set,,,/home/user/studies/QFS-evaluation/articles/rStar-Math.md,Novel training methodologies for process reward models that bypass noisy score annotation.,15,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""sections as set""}"
2025-08-28T11:33:59Z,auto-judge-0-3-sections-as-set,,,/home/user/studies/QFS-evaluation/articles/distillation of SOTA embedding models.md,Methodology for fusing knowledge from heterogeneous teacher models.,15,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""sections as set""}"
2025-08-28T11:33:59Z,auto-judge-0-3-sections-as-set,,,/home/user/studies/QFS-evaluation/articles/The-Linear-Representation-Hypothesis.md,"Providing a single geometric framework that formally connects the subspace hypothesis (e.g., word2vec analogies) with the practical techniques of linear probing and activation steering.",15,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": ""sections as set""}"
2025-08-28T13:19:39Z,STH-00,,,/home/user/studies/QFS-evaluation/articles/Forget-What-You-Know-about-LLMs.md,The methodology of using generative models to create adversarial evaluation datasets.,15,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": """"}"
2025-08-28T13:19:39Z,STH-00,,,/home/user/studies/QFS-evaluation/articles/Evolutionary Computation LLM.md,The application of multi-objective evolutionary principles throughout the surveyed research.,15,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": """"}"
2025-08-28T13:19:39Z,STH-00,,,/home/user/studies/QFS-evaluation/articles/Evolutionary Computation LLM.md,The LLM as a substitute for evolutionary operators.,15,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": """"}"
2025-08-28T13:19:39Z,STH-00,,,/home/user/studies/QFS-evaluation/articles/Evolutionary Computation LLM.md,The dual role of the LLM-EA synergy in the context of model and code security.,15,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": """"}"
2025-08-28T13:19:39Z,STH-00,,,/home/user/studies/QFS-evaluation/articles/Forget-What-You-Know-about-LLMs.md,Differential robustness across distinct LLM architectural families.,15,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": """"}"
2025-08-28T13:19:39Z,STH-00,,,/home/user/studies/QFS-evaluation/articles/Attention-Is-All-You-Need.md,The role of regularization and optimization schemes in training large-scale models.,15,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": """"}"
2025-08-28T13:19:39Z,STH-00,,,/home/user/studies/QFS-evaluation/articles/Learn Beyond The Answer.md,The role and limitations of proprietary expert models in synthetic data generation for LLM research.,15,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": """"}"
2025-08-28T13:19:39Z,STH-00,,,/home/user/studies/QFS-evaluation/articles/Hierarchical-Reasoning-Model.md,Emergent dimensionality hierarchy in a trained reasoning model as a parallel to the functional organization of the mouse cortex.,15,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": """"}"
2025-08-28T13:19:39Z,STH-00,,,/home/user/studies/QFS-evaluation/articles/Attention-Is-All-You-Need.md,"Enhancing representational power through parallel, subspace-specific attention.",15,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": """"}"
2025-08-28T13:19:39Z,STH-00,,,/home/user/studies/QFS-evaluation/articles/MemOS.md,Mechanisms for the lifecycle and cross-type transformation of memory.,15,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": """"}"
2025-08-28T13:19:39Z,STH-00,,,/home/user/studies/QFS-evaluation/articles/Learn Beyond The Answer.md,"A framework for categorizing mathematical reasoning skills in language models, from basic forward reasoning to complex reflective capabilities.",15,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": """"}"
2025-08-28T13:19:39Z,STH-00,,,/home/user/studies/QFS-evaluation/articles/Random Teachers are Good Teachers.md,The asymmetric nature of the loss landscape around a random initialization.,15,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": """"}"
2025-08-28T13:19:39Z,STH-00,,,/home/user/studies/QFS-evaluation/articles/MemOS.md,The framework for accountable and governed memory in multi-agent systems.,15,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": """"}"
2025-08-28T13:19:39Z,STH-00,,,/home/user/studies/QFS-evaluation/articles/Hierarchical-Reasoning-Model.md,"Applying principles of hierarchical processing, temporal separation, and local credit assignment from neuroscience to engineer stable, deep reasoning in artificial neural networks.",15,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": """"}"
2025-08-28T13:19:39Z,STH-00,,,/home/user/studies/QFS-evaluation/articles/Random Teachers are Good Teachers.md,How random teacher distillation pre-conditions a network for supervised training.,15,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": """"}"
2025-08-28T13:19:39Z,STH-00,,,/home/user/studies/QFS-evaluation/articles/Reinforcement Learning for Reasoning in Small LLMs.md,The impact of output length constraints on the stability and efficacy of training LLMs for complex reasoning tasks.,15,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": """"}"
2025-08-28T13:19:39Z,STH-00,,,/home/user/studies/QFS-evaluation/articles/Reinforcement Learning for Reasoning in Small LLMs.md,The challenge of managing multilingual language drift when fine-tuning a foundational model for a monolingual reasoning task.,15,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": """"}"
2025-08-28T13:19:39Z,STH-00,,,/home/user/studies/QFS-evaluation/articles/Rethinking Training Signals in RLVR.md,"The influence of a model's pretrained latent abilities (like code reasoning) on the outcomes of reinforcement learning, irrespective of the reward signal's quality.",15,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": """"}"
2025-08-28T13:19:39Z,STH-00,,,/home/user/studies/QFS-evaluation/articles/Evolutionary Computation LLM.md,The article's perspective on the theme of black-box optimization.,15,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": """"}"
2025-08-28T13:19:39Z,STH-00,,,/home/user/studies/QFS-evaluation/articles/Task Singular Vectors.md,The geometric interpretation of task interference through singular vector alignment.,15,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": """"}"
2025-08-28T13:19:39Z,STH-00,,,/home/user/studies/QFS-evaluation/articles/Rethinking Training Signals in RLVR.md,"A critique of model-specific conclusions in RL research, using the Qwen model family as a case study for how easily performance gains can be achieved with spurious signals.",15,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": """"}"
2025-08-28T13:19:39Z,STH-00,,,/home/user/studies/QFS-evaluation/articles/Evolutionary Computation LLM.md,Neural Architecture Search (NAS) as both a target for optimization and a domain for synergy.,15,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": """"}"
2025-08-28T13:19:39Z,STH-00,,,/home/user/studies/QFS-evaluation/articles/Task Singular Vectors.md,An argument for structure-aware model manipulation over flattened parameter approaches.,15,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": """"}"
2025-08-28T13:19:39Z,STH-00,,,/home/user/studies/QFS-evaluation/articles/Task Singular Vectors.md,The application of Orthogonal Procrustes analysis to decorrelate task-specific feature bases.,15,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": """"}"
2025-08-28T13:19:39Z,STH-00,,,/home/user/studies/QFS-evaluation/articles/distillation of SOTA embedding models.md,Self-distillation as a mechanism for post-hoc modality alignment.,15,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": """"}"
2025-08-28T13:19:39Z,STH-00,,,/home/user/studies/QFS-evaluation/articles/rStar-Math.md,Novel training methodologies for process reward models that bypass noisy score annotation.,15,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": """"}"
2025-08-28T13:19:39Z,STH-00,,,/home/user/studies/QFS-evaluation/articles/Learn Beyond The Answer.md,"The interplay between data quantity, quality, and complexity in fine-tuning language models for mathematical tasks.",15,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": """"}"
2025-08-28T13:19:39Z,STH-00,,,/home/user/studies/QFS-evaluation/articles/Reinforcement Learning for Reasoning in Small LLMs.md,Causes of optimization instability and performance degradation during RL fine-tuning of small language models.,15,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": """"}"
2025-08-28T13:19:39Z,STH-00,,,/home/user/studies/QFS-evaluation/articles/distillation of SOTA embedding models.md,Methodology for fusing knowledge from heterogeneous teacher models.,15,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": """"}"
2025-08-28T13:19:39Z,STH-00,,,/home/user/studies/QFS-evaluation/articles/rStar-Math.md,The role of code execution as a verifier for synthetic reasoning data.,15,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": """"}"
2025-08-28T13:19:39Z,STH-00,,,/home/user/studies/QFS-evaluation/articles/The-Linear-Representation-Hypothesis.md,"The derivation of a semantically meaningful geometry from the statistical properties of the model's full vocabulary, rather than from its training data distribution.",15,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": """"}"
2025-08-28T13:19:39Z,STH-00,,,/home/user/studies/QFS-evaluation/articles/The-Linear-Representation-Hypothesis.md,The article's use of counterfactuals as a formal language to ground and connect disparate intuitions about linear representations.,15,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": """"}"
2025-08-28T13:19:39Z,STH-00,,,/home/user/studies/QFS-evaluation/articles/The-Linear-Representation-Hypothesis.md,"Providing a single geometric framework that formally connects the subspace hypothesis (e.g., word2vec analogies) with the practical techniques of linear probing and activation steering.",15,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": """"}"
2025-08-28T13:36:51Z,STH-01-run-all,,,/home/user/studies/QFS-evaluation/articles/Attention-Is-All-You-Need.md,Methods for encoding sequential order in non-recurrent architectures.,15,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": """"}"
2025-08-28T13:36:51Z,STH-01-run-all,,,/home/user/studies/QFS-evaluation/articles/Attention-Is-All-You-Need.md,The role of regularization and optimization schemes in training large-scale models.,15,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": """"}"
2025-08-28T13:36:51Z,STH-01-run-all,,,/home/user/studies/QFS-evaluation/articles/Evolutionary Computation LLM.md,The LLM as a substitute for evolutionary operators.,15,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": """"}"
2025-08-28T13:36:51Z,STH-01-run-all,,,/home/user/studies/QFS-evaluation/articles/Evolutionary Computation LLM.md,The article's perspective on the theme of black-box optimization.,15,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": """"}"
2025-08-28T13:36:51Z,STH-01-run-all,,,/home/user/studies/QFS-evaluation/articles/Evolutionary Computation LLM.md,The application of multi-objective evolutionary principles throughout the surveyed research.,15,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": """"}"
2025-08-28T13:36:51Z,STH-01-run-all,,,/home/user/studies/QFS-evaluation/articles/Evolutionary Computation LLM.md,The dual role of the LLM-EA synergy in the context of model and code security.,15,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": """"}"
2025-08-28T13:36:51Z,STH-01-run-all,,,/home/user/studies/QFS-evaluation/articles/Evolutionary Computation LLM.md,Neural Architecture Search (NAS) as both a target for optimization and a domain for synergy.,15,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": """"}"
2025-08-28T13:36:51Z,STH-01-run-all,,,/home/user/studies/QFS-evaluation/articles/Attention-Is-All-You-Need.md,"Enhancing representational power through parallel, subspace-specific attention.",15,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": """"}"
2025-08-28T13:36:51Z,STH-01-run-all,,,/home/user/studies/QFS-evaluation/articles/Forget-What-You-Know-about-LLMs.md,The impact of model scale on overfitting vulnerability.,15,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": """"}"
2025-08-28T13:36:51Z,STH-01-run-all,,,/home/user/studies/QFS-evaluation/articles/Attention-Is-All-You-Need.md,The practical challenges and solutions for stabilizing dot-product attention.,15,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": """"}"
2025-08-28T13:36:51Z,STH-01-run-all,,,/home/user/studies/QFS-evaluation/articles/Forget-What-You-Know-about-LLMs.md,The methodology of using generative models to create adversarial evaluation datasets.,15,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": """"}"
2025-08-28T13:36:51Z,STH-01-run-all,,,/home/user/studies/QFS-evaluation/articles/Forget-What-You-Know-about-LLMs.md,The paradox of high accuracy as an indicator of fragility.,15,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": """"}"
2025-08-28T13:36:51Z,STH-01-run-all,,,/home/user/studies/QFS-evaluation/articles/Forget-What-You-Know-about-LLMs.md,The paper's critique of a leaderboard-centric evaluation culture in NLP.,15,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": """"}"
2025-08-28T13:36:51Z,STH-01-run-all,,,/home/user/studies/QFS-evaluation/articles/Hierarchical-Reasoning-Model.md,Emergent dimensionality hierarchy in a trained reasoning model as a parallel to the functional organization of the mouse cortex.,15,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": """"}"
2025-08-28T13:36:51Z,STH-01-run-all,,,/home/user/studies/QFS-evaluation/articles/Hierarchical-Reasoning-Model.md,"Novel training strategies for recurrent architectures, combining BPTT-free approximate gradients, deep supervision, and reinforcement learning for adaptive computation.",15,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": """"}"
2025-08-28T13:36:51Z,STH-01-run-all,,,/home/user/studies/QFS-evaluation/articles/Forget-What-You-Know-about-LLMs.md,Differential robustness across distinct LLM architectural families.,15,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": """"}"
2025-08-28T13:36:51Z,STH-01-run-all,,,/home/user/studies/QFS-evaluation/articles/Learn Beyond The Answer.md,The role and limitations of proprietary expert models in synthetic data generation for LLM research.,15,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": """"}"
2025-08-28T13:36:51Z,STH-01-run-all,,,/home/user/studies/QFS-evaluation/articles/Learn Beyond The Answer.md,Efficiency trade-offs in LLM data augmentation strategies.,15,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": """"}"
2025-08-28T13:36:51Z,STH-01-run-all,,,/home/user/studies/QFS-evaluation/articles/Hierarchical-Reasoning-Model.md,"An argument against the architectural and computational limitations of the Transformer/Chain-of-Thought paradigm for genuine, latent algorithmic reasoning.",15,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": """"}"
2025-08-28T13:36:51Z,STH-01-run-all,,,/home/user/studies/QFS-evaluation/articles/Learn Beyond The Answer.md,Comparing instance-dimension vs. sequence-dimension data augmentation for enhancing LLM reasoning.,15,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": """"}"
2025-08-28T13:36:51Z,STH-01-run-all,,,/home/user/studies/QFS-evaluation/articles/Hierarchical-Reasoning-Model.md,"Applying principles of hierarchical processing, temporal separation, and local credit assignment from neuroscience to engineer stable, deep reasoning in artificial neural networks.",15,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": """"}"
2025-08-28T13:36:51Z,STH-01-run-all,,,/home/user/studies/QFS-evaluation/articles/Learn Beyond The Answer.md,"The interplay between data quantity, quality, and complexity in fine-tuning language models for mathematical tasks.",15,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": """"}"
2025-08-28T13:36:51Z,STH-01-run-all,,,/home/user/studies/QFS-evaluation/articles/MemOS.md,The framework for accountable and governed memory in multi-agent systems.,15,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": """"}"
2025-08-28T13:36:51Z,STH-01-run-all,,,/home/user/studies/QFS-evaluation/articles/MemOS.md,Mechanisms for the lifecycle and cross-type transformation of memory.,15,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": """"}"
2025-08-28T13:36:51Z,STH-01-run-all,,,/home/user/studies/QFS-evaluation/articles/MemOS.md,The vision for a collaborative and decentralized memory sharing ecosystem.,15,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": """"}"
2025-08-28T13:36:51Z,STH-01-run-all,,,/home/user/studies/QFS-evaluation/articles/Hierarchical-Reasoning-Model.md,"Mechanisms for achieving and leveraging effective computational depth, using ""hierarchical convergence"" to overcome the vanishing gradient and premature convergence problems in recurrent systems.",15,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": """"}"
2025-08-28T13:36:51Z,STH-01-run-all,,,/home/user/studies/QFS-evaluation/articles/MemOS.md,"The role of structured metadata in enabling the scheduling, governance, and evolution of memory.",15,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": """"}"
2025-08-28T13:36:51Z,STH-01-run-all,,,/home/user/studies/QFS-evaluation/articles/Learn Beyond The Answer.md,"A framework for categorizing mathematical reasoning skills in language models, from basic forward reasoning to complex reflective capabilities.",15,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": """"}"
2025-08-28T13:36:51Z,STH-01-run-all,,,/home/user/studies/QFS-evaluation/articles/Random Teachers are Good Teachers.md,The asymmetric nature of the loss landscape around a random initialization.,15,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": """"}"
2025-08-28T13:36:51Z,STH-01-run-all,,,/home/user/studies/QFS-evaluation/articles/Random Teachers are Good Teachers.md,The impact of student initialization proximity to the teacher.,15,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": """"}"
2025-08-28T13:36:51Z,STH-01-run-all,,,/home/user/studies/QFS-evaluation/articles/Random Teachers are Good Teachers.md,The role of the input data distribution in shaping the learned representations.,15,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": """"}"
2025-08-28T13:36:51Z,STH-01-run-all,,,/home/user/studies/QFS-evaluation/articles/Reinforcement Learning for Reasoning in Small LLMs.md,Causes of optimization instability and performance degradation during RL fine-tuning of small language models.,15,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": """"}"
2025-08-28T13:36:51Z,STH-01-run-all,,,/home/user/studies/QFS-evaluation/articles/Reinforcement Learning for Reasoning in Small LLMs.md,The impact of output length constraints on the stability and efficacy of training LLMs for complex reasoning tasks.,15,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": """"}"
2025-08-28T13:36:51Z,STH-01-run-all,,,/home/user/studies/QFS-evaluation/articles/Reinforcement Learning for Reasoning in Small LLMs.md,The challenge of managing multilingual language drift when fine-tuning a foundational model for a monolingual reasoning task.,15,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": """"}"
2025-08-28T13:36:51Z,STH-01-run-all,,,/home/user/studies/QFS-evaluation/articles/Random Teachers are Good Teachers.md,How random teacher distillation pre-conditions a network for supervised training.,15,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": """"}"
2025-08-28T13:36:51Z,STH-01-run-all,,,/home/user/studies/QFS-evaluation/articles/MemOS.md,Parallels between the MemOS architecture and cognitive models of memory.,15,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": """"}"
2025-08-28T13:36:51Z,STH-01-run-all,,,/home/user/studies/QFS-evaluation/articles/Reinforcement Learning for Reasoning in Small LLMs.md,A comparative economic analysis of achieving state-of-the-art reasoning performance in small vs. large language models.,15,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": """"}"
2025-08-28T13:36:51Z,STH-01-run-all,,,/home/user/studies/QFS-evaluation/articles/Rethinking Training Signals in RLVR.md,"The influence of a model's pretrained latent abilities (like code reasoning) on the outcomes of reinforcement learning, irrespective of the reward signal's quality.",15,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": """"}"
2025-08-28T13:36:51Z,STH-01-run-all,,,/home/user/studies/QFS-evaluation/articles/Rethinking Training Signals in RLVR.md,"A critique of model-specific conclusions in RL research, using the Qwen model family as a case study for how easily performance gains can be achieved with spurious signals.",15,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": """"}"
2025-08-28T13:36:51Z,STH-01-run-all,,,/home/user/studies/QFS-evaluation/articles/Rethinking Training Signals in RLVR.md,"The role of optimization algorithm artifacts, specifically the clipping mechanism in GRPO, in creating a directional training signal from pure noise.",15,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": """"}"
2025-08-28T13:36:51Z,STH-01-run-all,,,/home/user/studies/QFS-evaluation/articles/Rethinking Training Signals in RLVR.md,"An analysis of how superficial interventions, such as prompting or rewarding simple syntactic patterns, can elicit complex reasoning behaviors in certain models.",15,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": """"}"
2025-08-28T13:36:51Z,STH-01-run-all,,,/home/user/studies/QFS-evaluation/articles/Task Singular Vectors.md,The geometric interpretation of task interference through singular vector alignment.,15,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": """"}"
2025-08-28T13:36:51Z,STH-01-run-all,,,/home/user/studies/QFS-evaluation/articles/Reinforcement Learning for Reasoning in Small LLMs.md,Strategies for curating compact datasets by balancing problem difficulty to enhance RL training efficiency under resource constraints.,15,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": """"}"
2025-08-28T13:36:51Z,STH-01-run-all,,,/home/user/studies/QFS-evaluation/articles/Task Singular Vectors.md,The counter-intuitive necessity of low-rank approximation for effective interference reduction.,15,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": """"}"
2025-08-28T13:36:51Z,STH-01-run-all,,,/home/user/studies/QFS-evaluation/articles/Rethinking Training Signals in RLVR.md,"The divergent behaviors of different model families (Qwen, Llama, OLMo) under identical RLVR training, highlighting a fundamental lack of technique generalization.",15,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": """"}"
2025-08-28T13:36:51Z,STH-01-run-all,,,/home/user/studies/QFS-evaluation/articles/Task Singular Vectors.md,The application of Orthogonal Procrustes analysis to decorrelate task-specific feature bases.,15,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": """"}"
2025-08-28T13:36:51Z,STH-01-run-all,,,/home/user/studies/QFS-evaluation/articles/Task Singular Vectors.md,An argument for structure-aware model manipulation over flattened parameter approaches.,15,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": """"}"
2025-08-28T13:36:51Z,STH-01-run-all,,,/home/user/studies/QFS-evaluation/articles/The-Linear-Representation-Hypothesis.md,How the proposed 'causal inner product' serves as a bridge to unify the conceptually separate input (embedding) and output (unembedding) representation spaces.,15,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": """"}"
2025-08-28T13:36:51Z,STH-01-run-all,,,/home/user/studies/QFS-evaluation/articles/Task Singular Vectors.md,The paper's empirical argument for eliminating the scaling coefficient hyperparameter.,15,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": """"}"
2025-08-28T13:36:51Z,STH-01-run-all,,,/home/user/studies/QFS-evaluation/articles/The-Linear-Representation-Hypothesis.md,The article's use of counterfactuals as a formal language to ground and connect disparate intuitions about linear representations.,15,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": """"}"
2025-08-28T13:36:51Z,STH-01-run-all,,,/home/user/studies/QFS-evaluation/articles/The-Linear-Representation-Hypothesis.md,"Providing a single geometric framework that formally connects the subspace hypothesis (e.g., word2vec analogies) with the practical techniques of linear probing and activation steering.",15,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": """"}"
2025-08-28T13:36:51Z,STH-01-run-all,,,/home/user/studies/QFS-evaluation/articles/The-Linear-Representation-Hypothesis.md,"The derivation of a semantically meaningful geometry from the statistical properties of the model's full vocabulary, rather than from its training data distribution.",15,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": """"}"
2025-08-28T13:36:51Z,STH-01-run-all,,,/home/user/studies/QFS-evaluation/articles/distillation of SOTA embedding models.md,The progressive constraint strategy in knowledge distillation.,15,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": """"}"
2025-08-28T13:36:51Z,STH-01-run-all,,,/home/user/studies/QFS-evaluation/articles/distillation of SOTA embedding models.md,The paper's dual approach to the engineering problem of embedding dimensionality.,15,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": """"}"
2025-08-28T13:36:51Z,STH-01-run-all,,,/home/user/studies/QFS-evaluation/articles/distillation of SOTA embedding models.md,Methodology for fusing knowledge from heterogeneous teacher models.,15,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": """"}"
2025-08-28T13:36:51Z,STH-01-run-all,,,/home/user/studies/QFS-evaluation/articles/distillation of SOTA embedding models.md,Self-distillation as a mechanism for post-hoc modality alignment.,15,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": """"}"
2025-08-28T13:36:51Z,STH-01-run-all,,,/home/user/studies/QFS-evaluation/articles/The-Linear-Representation-Hypothesis.md,"The paper as a solution to the geometric unidentifiability of representation spaces, resolved by imposing a causal structure.",15,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": """"}"
2025-08-28T13:36:51Z,STH-01-run-all,,,/home/user/studies/QFS-evaluation/articles/rStar-Math.md,The role of code execution as a verifier for synthetic reasoning data.,15,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": """"}"
2025-08-28T13:36:51Z,STH-01-run-all,,,/home/user/studies/QFS-evaluation/articles/rStar-Math.md,The emergence of self-correction capabilities as an intrinsic byproduct of the MCTS-based reasoning process.,15,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": """"}"
2025-08-28T13:36:51Z,STH-01-run-all,,,/home/user/studies/QFS-evaluation/articles/rStar-Math.md,Novel training methodologies for process reward models that bypass noisy score annotation.,15,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": """"}"
2025-08-28T13:36:51Z,STH-01-run-all,,,/home/user/studies/QFS-evaluation/articles/distillation of SOTA embedding models.md,Rationale for the staged training methodology.,15,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": """"}"
2025-08-28T13:36:51Z,STH-01-run-all,,,/home/user/studies/QFS-evaluation/articles/rStar-Math.md,The viability of self-evolution as an alternative to knowledge distillation for creating state-of-the-art training datasets.,15,"{""QuestionGeneratorModel"": ""gemini-2.5-pro"", ""commitMessage"": """"}"
