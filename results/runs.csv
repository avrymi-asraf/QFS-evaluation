timestamp,run_name,branch,commit,article,query,max_iterations,meta_json
2025-08-16T07:47:33Z,exp-001,main,4c9f022f108f6cb362d7ea2d3e77cbaab275573b,/home/avreymi/code/QFS-code-and-evaluation/Query-Focused-Summarization/articals/Attention-Is-All-You-Need.pdf,What are the core innovations of the Transformer architecture and how do they replace recurrence?,,{}
2025-08-16T07:47:33Z,exp-001,main,4c9f022f108f6cb362d7ea2d3e77cbaab275573b,/home/avreymi/code/QFS-code-and-evaluation/Query-Focused-Summarization/articals/The Linear Representation Hypothesis.pdf,Summarize the linear representation hypothesis and its implications for neural network interpretability.,,{}
2025-08-16T08:13:42Z,exp-002,main,44f67b3b8816d5393c6e95f8d9cdc8dfcd3fae19,/home/avreymi/code/QFS-code-and-evaluation/Query-Focused-Summarization/articals/Attention-Is-All-You-Need.pdf,What are the core innovations of the Transformer architecture and how do they replace recurrence?,,{}
2025-08-16T08:13:42Z,exp-002,main,44f67b3b8816d5393c6e95f8d9cdc8dfcd3fae19,/home/avreymi/code/QFS-code-and-evaluation/Query-Focused-Summarization/articals/The Linear Representation Hypothesis.pdf,Summarize the linear representation hypothesis and its implications for neural network interpretability.,,{}
2025-08-16T16:33:39Z,first_run,main,44f67b3b8816d5393c6e95f8d9cdc8dfcd3fae19,/home/avreymi/code/QFS-code-and-evaluation/QFS-evaluation/articles/Attention-Is-All-You-Need.md,What are the core innovations of the Transformer architecture and how do they replace recurrence?,,{}
2025-08-16T16:33:39Z,first_run,main,44f67b3b8816d5393c6e95f8d9cdc8dfcd3fae19,/home/avreymi/code/QFS-code-and-evaluation/QFS-evaluation/articles/The-Linear-Representation-Hypothesis.md,Summarize the linear representation hypothesis and its implications for neural network interpretability.,,{}
2025-08-16T16:33:39Z,first_run,main,44f67b3b8816d5393c6e95f8d9cdc8dfcd3fae19,/home/avreymi/code/QFS-code-and-evaluation/QFS-evaluation/articles/Forget-What-You-Know-about-LLMs.md,Discuss the limitations of traditional evaluation methods for LLMs and propose alternative approaches.,,{}
2025-08-16T16:33:39Z,first_run,main,44f67b3b8816d5393c6e95f8d9cdc8dfcd3fae19,/home/avreymi/code/QFS-code-and-evaluation/QFS-evaluation/articles/Hierarchical-Reasoning-Model.md,Explain the hierarchical reasoning model and its significance in understanding complex systems.,,{}
2025-08-17T07:58:59Z,secode_run,main,44f67b3b8816d5393c6e95f8d9cdc8dfcd3fae19,/home/avreymi/code/QFS-code-and-evaluation/QFS-evaluation/articles/Attention-Is-All-You-Need.md,What are the core innovations of the Transformer architecture and how do they replace recurrence?,,"{""meta"": ""{'key':'value', 'key2':'value'}""}"
2025-08-17T07:58:59Z,secode_run,main,44f67b3b8816d5393c6e95f8d9cdc8dfcd3fae19,/home/avreymi/code/QFS-code-and-evaluation/QFS-evaluation/articles/The-Linear-Representation-Hypothesis.md,Summarize the linear representation hypothesis and its implications for neural network interpretability.,,"{""meta"": ""{'key':'value', 'key2':'value'}""}"
2025-08-17T07:58:59Z,secode_run,main,44f67b3b8816d5393c6e95f8d9cdc8dfcd3fae19,/home/avreymi/code/QFS-code-and-evaluation/QFS-evaluation/articles/Forget-What-You-Know-about-LLMs.md,Discuss the limitations of traditional evaluation methods for LLMs and propose alternative approaches.,,"{""meta"": ""{'key':'value', 'key2':'value'}""}"
2025-08-17T07:58:59Z,secode_run,main,44f67b3b8816d5393c6e95f8d9cdc8dfcd3fae19,/home/avreymi/code/QFS-code-and-evaluation/QFS-evaluation/articles/Hierarchical-Reasoning-Model.md,Explain the hierarchical reasoning model and its significance in understanding complex systems.,,"{""meta"": ""{'key':'value', 'key2':'value'}""}"
2025-08-17T12:46:49Z,secode_run,main,44f67b3b8816d5393c6e95f8d9cdc8dfcd3fae19,/home/avreymi/code/QFS-code-and-evaluation/QFS-evaluation/articles/Attention-Is-All-You-Need.md,What are the core innovations of the Transformer architecture and how do they replace recurrence?,7,{}
2025-08-19T18:02:42Z,paid_key,dev-avreymi,44f67b3b8816d5393c6e95f8d9cdc8dfcd3fae19,/home/avreymi/code/QFS-code-and-evaluation/QFS-evaluation/articles/Attention-Is-All-You-Need.md,What are the core innovations of the Transformer architecture and how do they replace recurrence?,,{}
2025-08-19T18:02:42Z,paid_key,dev-avreymi,44f67b3b8816d5393c6e95f8d9cdc8dfcd3fae19,/home/avreymi/code/QFS-code-and-evaluation/QFS-evaluation/articles/The-Linear-Representation-Hypothesis.md,Summarize the linear representation hypothesis and its implications for neural network interpretability.,,{}
2025-08-19T18:02:42Z,paid_key,dev-avreymi,44f67b3b8816d5393c6e95f8d9cdc8dfcd3fae19,/home/avreymi/code/QFS-code-and-evaluation/QFS-evaluation/articles/Forget-What-You-Know-about-LLMs.md,Discuss the limitations of traditional evaluation methods for LLMs and propose alternative approaches.,,{}
2025-08-19T18:05:26Z,paid_key_it_10,dev-avreymi,44f67b3b8816d5393c6e95f8d9cdc8dfcd3fae19,/home/avreymi/code/QFS-code-and-evaluation/QFS-evaluation/articles/Attention-Is-All-You-Need.md,What are the core innovations of the Transformer architecture and how do they replace recurrence?,10,{}
2025-08-19T18:02:42Z,paid_key,dev-avreymi,44f67b3b8816d5393c6e95f8d9cdc8dfcd3fae19,/home/avreymi/code/QFS-code-and-evaluation/QFS-evaluation/articles/Hierarchical-Reasoning-Model.md,Explain the hierarchical reasoning model and its significance in understanding complex systems.,,{}
2025-08-19T18:05:26Z,paid_key_it_10,dev-avreymi,44f67b3b8816d5393c6e95f8d9cdc8dfcd3fae19,/home/avreymi/code/QFS-code-and-evaluation/QFS-evaluation/articles/The-Linear-Representation-Hypothesis.md,Summarize the linear representation hypothesis and its implications for neural network interpretability.,10,{}
2025-08-19T18:05:26Z,paid_key_it_10,dev-avreymi,44f67b3b8816d5393c6e95f8d9cdc8dfcd3fae19,/home/avreymi/code/QFS-code-and-evaluation/QFS-evaluation/articles/Forget-What-You-Know-about-LLMs.md,Discuss the limitations of traditional evaluation methods for LLMs and propose alternative approaches.,10,{}
2025-08-19T18:05:26Z,paid_key_it_10,dev-avreymi,44f67b3b8816d5393c6e95f8d9cdc8dfcd3fae19,/home/avreymi/code/QFS-code-and-evaluation/QFS-evaluation/articles/Hierarchical-Reasoning-Model.md,Explain the hierarchical reasoning model and its significance in understanding complex systems.,10,{}
2025-08-19T18:27:02Z,paid_key_10_without_limit,dev-avreymi,74ba08f05ba272dd9e26ffe0db92446668818b35,/home/avreymi/code/QFS-code-and-evaluation/QFS-evaluation/articles/Attention-Is-All-You-Need.md,What are the core innovations of the Transformer architecture and how do they replace recurrence?,10,{}
2025-08-19T18:27:02Z,paid_key_10_without_limit,dev-avreymi,74ba08f05ba272dd9e26ffe0db92446668818b35,/home/avreymi/code/QFS-code-and-evaluation/QFS-evaluation/articles/The-Linear-Representation-Hypothesis.md,Summarize the linear representation hypothesis and its implications for neural network interpretability.,10,{}
2025-08-19T18:27:02Z,paid_key_10_without_limit,dev-avreymi,74ba08f05ba272dd9e26ffe0db92446668818b35,/home/avreymi/code/QFS-code-and-evaluation/QFS-evaluation/articles/Forget-What-You-Know-about-LLMs.md,Discuss the limitations of traditional evaluation methods for LLMs and propose alternative approaches.,10,{}
2025-08-19T18:27:02Z,paid_key_10_without_limit,dev-avreymi,74ba08f05ba272dd9e26ffe0db92446668818b35,/home/avreymi/code/QFS-code-and-evaluation/QFS-evaluation/articles/Hierarchical-Reasoning-Model.md,Explain the hierarchical reasoning model and its significance in understanding complex systems.,10,{}
2025-08-19T19:04:06Z,concurrency-test,main,44f67b3b8816d5393c6e95f8d9cdc8dfcd3fae19,/home/avreymi/code/QFS-code-and-evaluation/QFS-evaluation/articles/Attention-Is-All-You-Need.md,In what ways does the multi-head attention mechanism allow the model to jointly attend to information from different representational subspaces at various positions?,1,{}
2025-08-19T19:04:06Z,concurrency-test,main,44f67b3b8816d5393c6e95f8d9cdc8dfcd3fae19,/home/avreymi/code/QFS-code-and-evaluation/QFS-evaluation/articles/Attention-Is-All-You-Need.md,training methodologies and hardware configurations,1,{}
2025-08-19T19:04:06Z,concurrency-test,main,44f67b3b8816d5393c6e95f8d9cdc8dfcd3fae19,/home/avreymi/code/QFS-code-and-evaluation/QFS-evaluation/articles/Forget-What-You-Know-about-LLMs.md,Model scale as an indicator of overfitting sensitivity.,1,{}
2025-08-19T19:04:06Z,concurrency-test,main,44f67b3b8816d5393c6e95f8d9cdc8dfcd3fae19,/home/avreymi/code/QFS-code-and-evaluation/QFS-evaluation/articles/The-Linear-Representation-Hypothesis.md,Summarize the linear representation hypothesis and its implications for neural network interpretability.,1,{}
2025-08-19T19:04:06Z,concurrency-test,main,44f67b3b8816d5393c6e95f8d9cdc8dfcd3fae19,/home/avreymi/code/QFS-code-and-evaluation/QFS-evaluation/articles/Forget-What-You-Know-about-LLMs.md,"Summarize the article's findings on the comparative resilience of different LLM families, such as Llama versus others, to textual perturbations.",1,{}
2025-08-19T19:04:06Z,concurrency-test,main,44f67b3b8816d5393c6e95f8d9cdc8dfcd3fae19,/home/avreymi/code/QFS-code-and-evaluation/QFS-evaluation/articles/Hierarchical-Reasoning-Model.md,Explain the hierarchical reasoning model and its significance in understanding complex systems.,1,{}
2025-08-19T19:07:54Z,concurrency-requstes-02,main,44f67b3b8816d5393c6e95f8d9cdc8dfcd3fae19,/home/avreymi/code/QFS-code-and-evaluation/QFS-evaluation/articles/Attention-Is-All-You-Need.md,In what ways does the multi-head attention mechanism allow the model to jointly attend to information from different representational subspaces at various positions?,5,{}
2025-08-19T19:07:54Z,concurrency-requstes-02,main,44f67b3b8816d5393c6e95f8d9cdc8dfcd3fae19,/home/avreymi/code/QFS-code-and-evaluation/QFS-evaluation/articles/Forget-What-You-Know-about-LLMs.md,"Summarize the article's findings on the comparative resilience of different LLM families, such as Llama versus others, to textual perturbations.",5,{}
2025-08-19T19:07:54Z,concurrency-requstes-02,main,44f67b3b8816d5393c6e95f8d9cdc8dfcd3fae19,/home/avreymi/code/QFS-code-and-evaluation/QFS-evaluation/articles/Attention-Is-All-You-Need.md,training methodologies and hardware configurations,5,{}
2025-08-19T19:07:54Z,concurrency-requstes-02,main,44f67b3b8816d5393c6e95f8d9cdc8dfcd3fae19,/home/avreymi/code/QFS-code-and-evaluation/QFS-evaluation/articles/Forget-What-You-Know-about-LLMs.md,Model scale as an indicator of overfitting sensitivity.,5,{}
2025-08-19T19:07:54Z,concurrency-requstes-02,main,44f67b3b8816d5393c6e95f8d9cdc8dfcd3fae19,/home/avreymi/code/QFS-code-and-evaluation/QFS-evaluation/articles/The-Linear-Representation-Hypothesis.md,Summarize the linear representation hypothesis and its implications for neural network interpretability.,5,{}
2025-08-19T19:07:54Z,concurrency-requstes-02,main,44f67b3b8816d5393c6e95f8d9cdc8dfcd3fae19,/home/avreymi/code/QFS-code-and-evaluation/QFS-evaluation/articles/Hierarchical-Reasoning-Model.md,Explain the hierarchical reasoning model and its significance in understanding complex systems.,5,{}
2025-08-20T19:02:50Z,new_output,dev-avreymi,3b3d05aaa5656b0af0aa88fd436d382ea8e7c670,/home/avreymi/code/QFS-code-and-evaluation/QFS-evaluation/articles/Attention-Is-All-You-Need.md,In what ways does the multi-head attention mechanism allow the model to jointly attend to information from different representational subspaces at various positions?,,{}
2025-08-20T19:02:50Z,new_output,dev-avreymi,3b3d05aaa5656b0af0aa88fd436d382ea8e7c670,/home/avreymi/code/QFS-code-and-evaluation/QFS-evaluation/articles/Attention-Is-All-You-Need.md,training methodologies and hardware configurations,,{}
2025-08-20T19:02:50Z,new_output,dev-avreymi,3b3d05aaa5656b0af0aa88fd436d382ea8e7c670,/home/avreymi/code/QFS-code-and-evaluation/QFS-evaluation/articles/Forget-What-You-Know-about-LLMs.md,Model scale as an indicator of overfitting sensitivity.,,{}
2025-08-20T19:02:50Z,new_output,dev-avreymi,3b3d05aaa5656b0af0aa88fd436d382ea8e7c670,/home/avreymi/code/QFS-code-and-evaluation/QFS-evaluation/articles/Forget-What-You-Know-about-LLMs.md,"Summarize the article's findings on the comparative resilience of different LLM families, such as Llama versus others, to textual perturbations.",,{}
2025-08-20T19:02:50Z,new_output,dev-avreymi,3b3d05aaa5656b0af0aa88fd436d382ea8e7c670,/home/avreymi/code/QFS-code-and-evaluation/QFS-evaluation/articles/Hierarchical-Reasoning-Model.md,Explain the hierarchical reasoning model and its significance in understanding complex systems.,,{}
2025-08-20T19:02:50Z,new_output,dev-avreymi,3b3d05aaa5656b0af0aa88fd436d382ea8e7c670,/home/avreymi/code/QFS-code-and-evaluation/QFS-evaluation/articles/The-Linear-Representation-Hypothesis.md,Summarize the linear representation hypothesis and its implications for neural network interpretability.,,{}
2025-08-27T05:20:33Z,new_format,ACU,fbab11fe8b1e01f39d817ed38d2a71960775b545,/home/user/studies/QFS-evaluation/articles/Attention-Is-All-You-Need.md,training methodologies and hardware configurations,,{}
2025-08-27T05:20:33Z,new_format,ACU,fbab11fe8b1e01f39d817ed38d2a71960775b545,/home/user/studies/QFS-evaluation/articles/Forget-What-You-Know-about-LLMs.md,"Summarize the article's findings on the comparative resilience of different LLM families, such as Llama versus others, to textual perturbations.",,{}
2025-08-27T05:20:33Z,new_format,ACU,fbab11fe8b1e01f39d817ed38d2a71960775b545,/home/user/studies/QFS-evaluation/articles/Forget-What-You-Know-about-LLMs.md,Model scale as an indicator of overfitting sensitivity.,,{}
2025-08-27T05:20:33Z,new_format,ACU,fbab11fe8b1e01f39d817ed38d2a71960775b545,/home/user/studies/QFS-evaluation/articles/Hierarchical-Reasoning-Model.md,Explain the hierarchical reasoning model and its significance in understanding complex systems.,,{}
2025-08-27T05:20:33Z,new_format,ACU,fbab11fe8b1e01f39d817ed38d2a71960775b545,/home/user/studies/QFS-evaluation/articles/Attention-Is-All-You-Need.md,In what ways does the multi-head attention mechanism allow the model to jointly attend to information from different representational subspaces at various positions?,,{}
2025-08-27T05:20:33Z,new_format,ACU,fbab11fe8b1e01f39d817ed38d2a71960775b545,/home/user/studies/QFS-evaluation/articles/The-Linear-Representation-Hypothesis.md,Summarize the linear representation hypothesis and its implications for neural network interpretability.,,{}
2025-08-27T08:07:37Z,all_section,sections,5f7e5593c4131cbe67d15207eaa6e54344a79e7e,/home/user/studies/QFS-evaluation/articles/Forget-What-You-Know-about-LLMs.md,Model scale as an indicator of overfitting sensitivity.,,{}
2025-08-27T08:07:37Z,all_section,sections,5f7e5593c4131cbe67d15207eaa6e54344a79e7e,/home/user/studies/QFS-evaluation/articles/Forget-What-You-Know-about-LLMs.md,"Summarize the article's findings on the comparative resilience of different LLM families, such as Llama versus others, to textual perturbations.",,{}
2025-08-27T08:07:37Z,all_section,sections,5f7e5593c4131cbe67d15207eaa6e54344a79e7e,/home/user/studies/QFS-evaluation/articles/Attention-Is-All-You-Need.md,training methodologies and hardware configurations,,{}
2025-08-27T08:07:37Z,all_section,sections,5f7e5593c4131cbe67d15207eaa6e54344a79e7e,/home/user/studies/QFS-evaluation/articles/Hierarchical-Reasoning-Model.md,Explain the hierarchical reasoning model and its significance in understanding complex systems.,,{}
2025-08-27T08:07:37Z,all_section,sections,5f7e5593c4131cbe67d15207eaa6e54344a79e7e,/home/user/studies/QFS-evaluation/articles/The-Linear-Representation-Hypothesis.md,Summarize the linear representation hypothesis and its implications for neural network interpretability.,,{}
2025-08-27T08:07:37Z,all_section,sections,5f7e5593c4131cbe67d15207eaa6e54344a79e7e,/home/user/studies/QFS-evaluation/articles/Attention-Is-All-You-Need.md,In what ways does the multi-head attention mechanism allow the model to jointly attend to information from different representational subspaces at various positions?,,{}
2025-08-27T08:10:43Z,all_section,sections,5f7e5593c4131cbe67d15207eaa6e54344a79e7e,/home/user/studies/QFS-evaluation/articles/Attention-Is-All-You-Need.md,In what ways does the multi-head attention mechanism allow the model to jointly attend to information from different representational subspaces at various positions?,10,{}
2025-08-27T08:10:43Z,all_section,sections,5f7e5593c4131cbe67d15207eaa6e54344a79e7e,/home/user/studies/QFS-evaluation/articles/The-Linear-Representation-Hypothesis.md,Summarize the linear representation hypothesis and its implications for neural network interpretability.,10,{}
2025-08-27T08:10:43Z,all_section,sections,5f7e5593c4131cbe67d15207eaa6e54344a79e7e,/home/user/studies/QFS-evaluation/articles/Hierarchical-Reasoning-Model.md,Explain the hierarchical reasoning model and its significance in understanding complex systems.,10,{}
2025-08-27T08:10:43Z,all_section,sections,5f7e5593c4131cbe67d15207eaa6e54344a79e7e,/home/user/studies/QFS-evaluation/articles/Forget-What-You-Know-about-LLMs.md,Model scale as an indicator of overfitting sensitivity.,10,{}
2025-08-27T08:10:43Z,all_section,sections,5f7e5593c4131cbe67d15207eaa6e54344a79e7e,/home/user/studies/QFS-evaluation/articles/Attention-Is-All-You-Need.md,training methodologies and hardware configurations,10,{}
2025-08-27T08:10:43Z,all_section,sections,5f7e5593c4131cbe67d15207eaa6e54344a79e7e,/home/user/studies/QFS-evaluation/articles/Forget-What-You-Know-about-LLMs.md,"Summarize the article's findings on the comparative resilience of different LLM families, such as Llama versus others, to textual perturbations.",10,{}
