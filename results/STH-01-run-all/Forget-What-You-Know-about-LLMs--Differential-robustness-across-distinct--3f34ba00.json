{
  "query": "Differential robustness across distinct LLM architectural families.",
  "max_iterations": 15,
  "iterations": [
    {
      "iteration_number": 1,
      "summary": "1. SUMMARY:\nThis research introduces the Chameleon Benchmark Overfit Detector (C-BOD) to evaluate Large Language Model (LLM) robustness by systematically distorting benchmark prompts while preserving semantic content. The study reveals that many LLMs over-rely on surface-level cues from popular benchmarks like MMLU, leading to significant performance degradation when prompts are rephrased. Notably, the Llama family of models and those with lower baseline accuracy demonstrated greater robustness, exhibiting less performance decline under these perturbations. In contrast, larger LLMs and those with higher initial accuracy showed a tendency to be more sensitive to rephrasing, suggesting a potential over-reliance on fixed prompt patterns. This indicates that high scores on standard benchmarks may not always translate to true language understanding. The findings challenge the LLM community to prioritize resilience and generalization in evaluation beyond simple leaderboard scores.\n\n2. KEY HIGHLIGHTS:\n* The Llama family of LLMs exhibits greater robustness to prompt rephrasing compared to other architectural families, indicating reduced dependency on superficial cues.\n* Larger LLMs and those with higher baseline accuracy tend to be more sensitive to prompt perturbations, suggesting a potential over-reliance on specific phrasing.\n* The C-BOD framework, which systematically distorts prompts, found that 20 out of 26 evaluated LLMs showed statistically significant performance degradation, highlighting widespread overfitting to benchmark surface features.\n* Models with lower baseline accuracy, such as some in the Llama family, showed less significant performance degradation, implying a more robust form of language understanding.",
      "qa_pairs": [
        {
          "question": "How did the robustness of the Llama model family compare to other architectural families like Gemma and Qwen when subjected to the C-BOD framework's textual perturbations?",
          "answer": "The Llama family of LLMs exhibits greater robustness to prompt rephrasing compared to other architectural families, indicating reduced dependency on superficial cues."
        },
        {
          "question": "The article states that larger models tend to exhibit greater performance degradation. How does this finding apply to the differential robustness observed across specific architectural families like Gemma and Qwen, as shown in the results?",
          "answer": "Larger LLMs and those with higher baseline accuracy tend to be more sensitive to prompt perturbations, suggesting a potential over-reliance on specific phrasing. The summary does not provide specific details on how this applies to Gemma and Qwen."
        },
        {
          "question": "According to the findings, what is the relationship between a model family's baseline accuracy on the original MMLU benchmark and its robustness against prompt rephrasing?",
          "answer": "Models with lower baseline accuracy, such as some in the Llama family, showed less significant performance degradation, implying a more robust form of language understanding. Conversely, models with higher baseline accuracy tend to be more sensitive to prompt perturbations."
        },
        {
          "question": "Besides the Llama family, which other models from different architectural families were found to have statistically insignificant performance differences, and what does this suggest about their robustness?",
          "answer": "Not enough information in summary"
        },
        {
          "question": "What specific reasons does the article suggest for why models from the Qwen family failed on rephrased questions, and how does this illustrate a lack of robustness compared to more stable models?",
          "answer": "Not enough information in summary"
        },
        {
          "question": "ACU. Which LLM family is noted for showing insignificant performance degradation, suggesting greater robustness?",
          "answer": "The Llama family of LLMs is noted for showing insignificant performance degradation, suggesting greater robustness."
        },
        {
          "question": "ACU. What is the name of the benchmark dataset used to compare the performance of the different LLM families?",
          "answer": "The article mentions the MMLU benchmark, but it is used as an example of a popular benchmark that LLMs are evaluated on, not as the dataset used to compare the performance of different LLM families in this specific study."
        },
        {
          "question": "ACU. Out of the 26 LLMs evaluated, how many showed statistically significant performance differences when tested on rephrased prompts?",
          "answer": "20 out of 26 evaluated LLMs showed statistically significant performance degradation when tested on rephrased prompts."
        }
      ],
      "judge": {
        "evaluations": [
          {
            "qa": {
              "question": "How did the robustness of the Llama model family compare to other architectural families like Gemma and Qwen when subjected to the C-BOD framework's textual perturbations?",
              "answer": "The Llama family of LLMs exhibits greater robustness to prompt rephrasing compared to other architectural families, indicating reduced dependency on superficial cues."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "The article states that larger models tend to exhibit greater performance degradation. How does this finding apply to the differential robustness observed across specific architectural families like Gemma and Qwen, as shown in the results?",
              "answer": "Larger LLMs and those with higher baseline accuracy tend to be more sensitive to prompt perturbations, suggesting a potential over-reliance on specific phrasing. The summary does not provide specific details on how this applies to Gemma and Qwen."
            },
            "result": false,
            "issue": "The summary does not provide specific details on how the finding of larger models exhibiting greater performance degradation applies to the differential robustness of Gemma and Qwen."
          },
          {
            "qa": {
              "question": "According to the findings, what is the relationship between a model family's baseline accuracy on the original MMLU benchmark and its robustness against prompt rephrasing?",
              "answer": "Models with lower baseline accuracy, such as some in the Llama family, showed less significant performance degradation, implying a more robust form of language understanding. Conversely, models with higher baseline accuracy tend to be more sensitive to prompt perturbations."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "Besides the Llama family, which other models from different architectural families were found to have statistically insignificant performance differences, and what does this suggest about their robustness?",
              "answer": "Not enough information in summary"
            },
            "result": false,
            "issue": "The summary does not list other model families with statistically insignificant performance differences."
          },
          {
            "qa": {
              "question": "What specific reasons does the article suggest for why models from the Qwen family failed on rephrased questions, and how does this illustrate a lack of robustness compared to more stable models?",
              "answer": "Not enough information in summary"
            },
            "result": false,
            "issue": "The summary does not provide specific reasons for Qwen family model failures on rephrased questions."
          },
          {
            "qa": {
              "question": "ACU. Which LLM family is noted for showing insignificant performance degradation, suggesting greater robustness?",
              "answer": "The Llama family of LLMs is noted for showing insignificant performance degradation, suggesting greater robustness."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "ACU. What is the name of the benchmark dataset used to compare the performance of the different LLM families?",
              "answer": "The article mentions the MMLU benchmark, but it is used as an example of a popular benchmark that LLMs are evaluated on, not as the dataset used to compare the performance of different LLM families in this specific study."
            },
            "result": false,
            "issue": "The article clearly states that the MMLU benchmark was used to evaluate the LLMs and compare their performance. The summary's answer is contradictory and inaccurate."
          },
          {
            "qa": {
              "question": "ACU. Out of the 26 LLMs evaluated, how many showed statistically significant performance differences when tested on rephrased prompts?",
              "answer": "20 out of 26 evaluated LLMs showed statistically significant performance degradation when tested on rephrased prompts."
            },
            "result": true,
            "issue": ""
          }
        ],
        "judgment": false,
        "sections_to_highlight": [
          "Introduction — Problem of LLM evaluation",
          "Methodology — C-BOD framework",
          "Results — Model robustness comparison",
          "Results — Model size vs. degradation",
          "Results — Accuracy vs. degradation",
          "Limitations — Scope of C-BOD",
          "Discussion — Implications for LLM evaluation"
        ]
      },
      "correct_count_all": 4,
      "correct_count_acu": 2,
      "num_of_questions": 5,
      "sections_to_highlight": [],
      "sections_to_highlight_size": 0
    },
    {
      "iteration_number": 2,
      "summary": "1. SUMMARY:\nThis paper introduces the Chameleon Benchmark Overfit Detector (C-BOD), a framework designed to expose LLMs' overreliance on surface-level cues in benchmark evaluations. By systematically rephrasing benchmark prompts while preserving semantic meaning, C-BOD measures performance degradation. Experiments on the MMLU benchmark with 26 LLMs revealed an average accuracy drop of 2.15% under modest perturbations, with 20 models showing statistically significant declines. A key finding is the differential robustness across LLM architectural families: larger models and those with higher baseline accuracy tend to be more sensitive to rephrasing, suggesting overfitting to prompt patterns. Conversely, the Llama family and models with lower baseline accuracy exhibited less degradation, indicating reduced dependency on superficial cues. The study challenges current evaluation practices, emphasizing the need to prioritize resilience and generalization over leaderboard scores. While effective for surface-level overfitting, C-BOD's limitations include potential oversight of deeper overfitting and high computational costs for iterative mitigation.\n\n2. KEY HIGHLIGHTS:\n*   LLMs often overfit to specific prompt phrasing, leading to significant performance drops when prompts are rephrased, even while preserving meaning.\n*   Larger LLMs and those with higher baseline accuracy are more prone to performance degradation under prompt perturbations, indicating a reliance on fixed patterns.\n*   The Llama family and models with lower initial accuracy demonstrate greater robustness, suggesting a weaker dependency on superficial cues.\n*   Current evaluation metrics can be misleading, as high benchmark scores may mask an LLM's vulnerability to prompt variations.\n*   C-BOD offers a dataset-agnostic framework to detect surface-level overfitting, but may not capture deeper forms of model vulnerability and can be computationally intensive for mitigation.",
      "qa_pairs": [
        {
          "question": "How did the robustness of the Llama model family compare to other architectural families like Gemma and Qwen when subjected to the C-BOD framework's textual perturbations?",
          "answer": "The Llama family exhibited less degradation, indicating reduced dependency on superficial cues."
        },
        {
          "question": "The article states that larger models tend to exhibit greater performance degradation. How does this finding apply to the differential robustness observed across specific architectural families like Gemma and Qwen, as shown in the results?",
          "answer": "Larger models and those with higher baseline accuracy tend to be more sensitive to rephrasing, suggesting overfitting to prompt patterns. The Llama family and models with lower baseline accuracy exhibited less degradation, indicating reduced dependency on superficial cues."
        },
        {
          "question": "According to the findings, what is the relationship between a model family's baseline accuracy on the original MMLU benchmark and its robustness against prompt rephrasing?",
          "answer": "Models with higher baseline accuracy tend to be more sensitive to rephrasing, suggesting overfitting to prompt patterns. Conversely, models with lower baseline accuracy exhibited less degradation, indicating reduced dependency on superficial cues."
        },
        {
          "question": "Besides the Llama family, which other models from different architectural families were found to have statistically insignificant performance differences, and what does this suggest about their robustness?",
          "answer": "Not enough information in summary"
        },
        {
          "question": "What specific reasons does the article suggest for why models from the Qwen family failed on rephrased questions, and how does this illustrate a lack of robustness compared to more stable models?",
          "answer": "Not enough information in summary"
        },
        {
          "question": "ACU. Which LLM family is noted for showing insignificant performance degradation, suggesting greater robustness?",
          "answer": "The Llama family is noted for showing insignificant performance degradation, suggesting greater robustness."
        },
        {
          "question": "ACU. What is the name of the benchmark dataset used to compare the performance of the different LLM families?",
          "answer": "The MMLU benchmark dataset was used to compare the performance of the different LLM families."
        },
        {
          "question": "ACU. Out of the 26 LLMs evaluated, how many showed statistically significant performance differences when tested on rephrased prompts?",
          "answer": "20 models showed statistically significant declines."
        }
      ],
      "judge": {
        "evaluations": [
          {
            "qa": {
              "question": "How did the robustness of the Llama model family compare to other architectural families like Gemma and Qwen when subjected to the C-BOD framework's textual perturbations?",
              "answer": "The Llama family exhibited less degradation, indicating reduced dependency on superficial cues."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "The article states that larger models tend to exhibit greater performance degradation. How does this finding apply to the differential robustness observed across specific architectural families like Gemma and Qwen, as shown in the results?",
              "answer": "Larger models and those with higher baseline accuracy tend to be more sensitive to rephrasing, suggesting overfitting to prompt patterns. The Llama family and models with lower baseline accuracy exhibited less degradation, indicating reduced dependency on superficial cues."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "According to the findings, what is the relationship between a model family's baseline accuracy on the original MMLU benchmark and its robustness against prompt rephrasing?",
              "answer": "Models with higher baseline accuracy tend to be more sensitive to rephrasing, suggesting overfitting to prompt patterns. Conversely, models with lower baseline accuracy exhibited less degradation, indicating reduced dependency on superficial cues."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "Besides the Llama family, which other models from different architectural families were found to have statistically insignificant performance differences, and what does this suggest about their robustness?",
              "answer": "Not enough information in summary"
            },
            "result": false,
            "issue": "The summary does not provide specific examples of other models with statistically insignificant performance differences besides the Llama family. The article mentions Falcon 7B, DeepSeek 7B, Qwen 2.5 3B, and Jetmoe 8B in this context."
          },
          {
            "qa": {
              "question": "What specific reasons does the article suggest for why models from the Qwen family failed on rephrased questions, and how does this illustrate a lack of robustness compared to more stable models?",
              "answer": "Not enough information in summary"
            },
            "result": false,
            "issue": "The summary does not provide specific reasons for Qwen family failures. The article's Table 3 provides examples for Qwen2.5 (32B parameters) related to legal terminology, word choice ('problems' vs. 'issues'), and question structure."
          },
          {
            "qa": {
              "question": "ACU. Which LLM family is noted for showing insignificant performance degradation, suggesting greater robustness?",
              "answer": "The Llama family is noted for showing insignificant performance degradation, suggesting greater robustness."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "ACU. What is the name of the benchmark dataset used to compare the performance of the different LLM families?",
              "answer": "The MMLU benchmark dataset was used to compare the performance of the different LLM families."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "ACU. Out of the 26 LLMs evaluated, how many showed statistically significant performance differences when tested on rephrased prompts?",
              "answer": "20 models showed statistically significant declines."
            },
            "result": true,
            "issue": ""
          }
        ],
        "judgment": false,
        "sections_to_highlight": [
          "Results — Model robustness comparison",
          "Results — Model size vs. degradation",
          "Results — Accuracy vs. degradation",
          "Methodology — C-BOD framework",
          "Discussion — LLM overfitting reasons",
          "Limitations — C-BOD scope and cost"
        ]
      },
      "correct_count_all": 6,
      "correct_count_acu": 3,
      "num_of_questions": 5,
      "sections_to_highlight": [
        "Limitations — Scope of C-BOD",
        "Results — Model robustness comparison",
        "Results — Model size vs. degradation",
        "Methodology — C-BOD framework",
        "Discussion — Implications for LLM evaluation",
        "Introduction — Problem of LLM evaluation",
        "Results — Accuracy vs. degradation"
      ],
      "sections_to_highlight_size": 7
    },
    {
      "iteration_number": 3,
      "summary": "1. SUMMARY:\nThis research introduces the Chameleon Benchmark Overfit Detector (C-BOD), a meta-evaluation framework designed to identify LLMs that over-rely on surface-level cues in benchmarks rather than demonstrating true language understanding. By systematically rephrasing benchmark prompts while preserving their semantic content, C-BOD exposes overfitting. Evaluation on the MMLU benchmark with 26 LLMs revealed that most models experience a performance drop (average 2.15%) under these perturbations, with 20 showing statistically significant degradation. Notably, larger models and those with higher baseline accuracy tend to be more sensitive to these rephrasings, indicating a greater reliance on fixed prompt patterns. Conversely, the Llama family and models with lower baseline accuracy showed less degradation, suggesting reduced dependency on superficial cues. The C-BOD framework is dataset- and model-agnostic, offering a means to promote more robust language understanding and challenging the community to prioritize resilience over leaderboard scores.\n\n2. KEY HIGHLIGHTS:\n*   LLMs often overfit to specific prompt patterns in benchmarks, leading to performance drops when prompts are rephrased while preserving meaning.\n*   Larger LLMs and those with higher baseline accuracy are more susceptible to performance degradation due to prompt perturbations, suggesting a reliance on surface cues.\n*   The Llama family and models with lower baseline accuracy demonstrated greater robustness and less sensitivity to rephrased prompts.\n*   C-BOD is a dataset-agnostic framework that quantifies overfitting by comparing performance on original versus semantically equivalent perturbed prompts, using statistical significance tests.",
      "qa_pairs": [
        {
          "question": "How did the robustness of the Llama model family compare to other architectural families like Gemma and Qwen when subjected to the C-BOD framework's textual perturbations?",
          "answer": "Not enough information in summary"
        },
        {
          "question": "The article states that larger models tend to exhibit greater performance degradation. How does this finding apply to the differential robustness observed across specific architectural families like Gemma and Qwen, as shown in the results?",
          "answer": "Not enough information in summary"
        },
        {
          "question": "According to the findings, what is the relationship between a model family's baseline accuracy on the original MMLU benchmark and its robustness against prompt rephrasing?",
          "answer": "Models with lower baseline accuracy showed less degradation, suggesting reduced dependency on superficial cues."
        },
        {
          "question": "Besides the Llama family, which other models from different architectural families were found to have statistically insignificant performance differences, and what does this suggest about their robustness?",
          "answer": "Not enough information in summary"
        },
        {
          "question": "What specific reasons does the article suggest for why models from the Qwen family failed on rephrased questions, and how does this illustrate a lack of robustness compared to more stable models?",
          "answer": "Not enough information in summary"
        },
        {
          "question": "ACU. Which LLM family is noted for showing insignificant performance degradation, suggesting greater robustness?",
          "answer": "The Llama family is noted for showing insignificant performance degradation, suggesting greater robustness."
        },
        {
          "question": "ACU. What is the name of the benchmark dataset used to compare the performance of the different LLM families?",
          "answer": "The benchmark dataset used to compare the performance of the different LLM families is MMLU."
        },
        {
          "question": "ACU. Out of the 26 LLMs evaluated, how many showed statistically significant performance differences when tested on rephrased prompts?",
          "answer": "20 LLMs showed statistically significant performance differences when tested on rephrased prompts."
        }
      ],
      "judge": {
        "evaluations": [
          {
            "qa": {
              "question": "How did the robustness of the Llama model family compare to other architectural families like Gemma and Qwen when subjected to the C-BOD framework's textual perturbations?",
              "answer": "Not enough information in summary"
            },
            "result": false,
            "issue": "The summary does not contain specific comparative details about the robustness of Llama, Gemma, and Qwen families under C-BOD perturbations, only stating that Llama showed less degradation."
          },
          {
            "qa": {
              "question": "The article states that larger models tend to exhibit greater performance degradation. How does this finding apply to the differential robustness observed across specific architectural families like Gemma and Qwen, as shown in the results?",
              "answer": "Not enough information in summary"
            },
            "result": false,
            "issue": "The summary mentions that larger models and those with higher baseline accuracy are more sensitive, but it does not connect this to specific differential robustness within Gemma and Qwen families as observed in the results."
          },
          {
            "qa": {
              "question": "According to the findings, what is the relationship between a model family's baseline accuracy on the original MMLU benchmark and its robustness against prompt rephrasing?",
              "answer": "Models with lower baseline accuracy showed less degradation, suggesting reduced dependency on superficial cues."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "Besides the Llama family, which other models from different architectural families were found to have statistically insignificant performance differences, and what does this suggest about their robustness?",
              "answer": "Not enough information in summary"
            },
            "result": false,
            "issue": "The summary does not list other specific models or families that showed statistically insignificant performance differences, only mentioning Llama and generally models with lower baseline accuracy."
          },
          {
            "qa": {
              "question": "What specific reasons does the article suggest for why models from the Qwen family failed on rephrased questions, and how does this illustrate a lack of robustness compared to more stable models?",
              "answer": "Not enough information in summary"
            },
            "result": false,
            "issue": "The summary does not provide specific reasons for Qwen family failures on rephrased questions; it only mentions that models with higher baseline accuracy and larger models are more sensitive."
          },
          {
            "qa": {
              "question": "ACU. Which LLM family is noted for showing insignificant performance degradation, suggesting greater robustness?",
              "answer": "The Llama family is noted for showing insignificant performance degradation, suggesting greater robustness."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "ACU. What is the name of the benchmark dataset used to compare the performance of the different LLM families?",
              "answer": "The benchmark dataset used to compare the performance of the different LLM families is MMLU."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "ACU. Out of the 26 LLMs evaluated, how many showed statistically significant performance differences when tested on rephrased prompts?",
              "answer": "20 LLMs showed statistically significant performance differences when tested on rephrased prompts."
            },
            "result": true,
            "issue": ""
          }
        ],
        "judgment": false,
        "sections_to_highlight": [
          "Results — Model robustness comparison (Llama vs. Gemma/Qwen)",
          "Results — Model size vs. degradation (specific families)",
          "Results — Other models with insignificant performance differences",
          "Discussion — Reasons for Qwen family failures",
          "Introduction — Problem of LLM evaluation",
          "Methodology — C-BOD framework",
          "Results — Accuracy vs. degradation"
        ]
      },
      "correct_count_all": 4,
      "correct_count_acu": 3,
      "num_of_questions": 5,
      "sections_to_highlight": [
        "Limitations — Scope of C-BOD",
        "Discussion — LLM overfitting reasons",
        "Results — Model robustness comparison",
        "Results — Model size vs. degradation",
        "Limitations — C-BOD scope and cost",
        "Methodology — C-BOD framework",
        "Discussion — Implications for LLM evaluation",
        "Introduction — Problem of LLM evaluation",
        "Results — Accuracy vs. degradation"
      ],
      "sections_to_highlight_size": 9
    },
    {
      "iteration_number": 4,
      "summary": "1. SUMMARY:\nThis research introduces the Chameleon Benchmark Overfit Detector (C-BOD) to assess LLM robustness against prompt rephrasing, revealing overreliance on surface cues. Across 26 LLMs evaluated on MMLU, C-BOD demonstrated an average 2.15% performance drop under modest perturbations, with 20 models showing statistically significant degradation. Notably, larger models and those with higher baseline accuracy exhibited greater performance declines, suggesting they are more susceptible to overfitting fixed prompt patterns. In contrast, the Llama family and models with lower baseline accuracy showed significantly less degradation, indicating reduced dependency on superficial cues. The study highlights a paradox where high benchmark scores might mask vulnerabilities to prompt variations. While C-BOD effectively detects surface-level overfitting, its limitations include potential oversight of deeper overfitting forms and the computational cost of integrating it into training pipelines. The findings emphasize the need to move beyond leaderboard scores and prioritize resilience in LLM evaluation.\n\n2. KEY HIGHLIGHTS:\n*   LLMs exhibit differential robustness across architectural families, with Llama models showing greater stability against prompt perturbations compared to others like Gemma and Qwen.\n*   Larger LLMs and those with higher initial accuracy tend to be more sensitive to prompt rephrasing, indicating a greater reliance on surface-level cues and potential overfitting.\n*   Models with lower baseline accuracy, such as certain DeepSeek and Qwen variants, displayed statistically insignificant performance degradation, suggesting a reduced dependency on superficial patterns.\n*   The C-BOD framework effectively uncovers surface-level overfitting but has limitations in detecting deeper forms of overfitting and can be computationally expensive to integrate into training.\n*   The research challenges current LLM evaluation practices, suggesting that high benchmark scores may not always reflect true language understanding due to overfitting.",
      "qa_pairs": [
        {
          "question": "How did the robustness of the Llama model family compare to other architectural families like Gemma and Qwen when subjected to the C-BOD framework's textual perturbations?",
          "answer": "LLMs exhibit differential robustness across architectural families, with Llama models showing greater stability against prompt perturbations compared to others like Gemma and Qwen."
        },
        {
          "question": "The article states that larger models tend to exhibit greater performance degradation. How does this finding apply to the differential robustness observed across specific architectural families like Gemma and Qwen, as shown in the results?",
          "answer": "Larger LLMs and those with higher initial accuracy tend to be more sensitive to prompt rephrasing, indicating a greater reliance on surface-level cues and potential overfitting. This suggests that families like Gemma and Qwen, if they are larger or have higher baseline accuracy, may exhibit more degradation compared to more robust families."
        },
        {
          "question": "According to the findings, what is the relationship between a model family's baseline accuracy on the original MMLU benchmark and its robustness against prompt rephrasing?",
          "answer": "Models with higher baseline accuracy exhibited greater performance declines, suggesting they are more susceptible to overfitting fixed prompt patterns. Conversely, models with lower baseline accuracy showed significantly less degradation, indicating reduced dependency on superficial cues."
        },
        {
          "question": "Besides the Llama family, which other models from different architectural families were found to have statistically insignificant performance differences, and what does this suggest about their robustness?",
          "answer": "Models with lower baseline accuracy, such as certain DeepSeek and Qwen variants, displayed statistically insignificant performance degradation, suggesting a reduced dependency on superficial patterns and thus greater robustness."
        },
        {
          "question": "What specific reasons does the article suggest for why models from the Qwen family failed on rephrased questions, and how does this illustrate a lack of robustness compared to more stable models?",
          "answer": "The article suggests that models from the Qwen family, particularly variants with lower baseline accuracy, displayed statistically insignificant performance degradation, indicating reduced dependency on superficial patterns. This contrasts with more stable models and implies a greater robustness against prompt rephrasing."
        },
        {
          "question": "ACU. Which LLM family is noted for showing insignificant performance degradation, suggesting greater robustness?",
          "answer": "The Llama family is noted for showing insignificant performance degradation, suggesting greater robustness."
        },
        {
          "question": "ACU. What is the name of the benchmark dataset used to compare the performance of the different LLM families?",
          "answer": "The benchmark dataset used to compare the performance of the different LLM families is MMLU."
        },
        {
          "question": "ACU. Out of the 26 LLMs evaluated, how many showed statistically significant performance differences when tested on rephrased prompts?",
          "answer": "Out of the 26 LLMs evaluated, 20 models showed statistically significant degradation when subjected to modest perturbations."
        }
      ],
      "judge": {
        "evaluations": [
          {
            "qa": {
              "question": "How did the robustness of the Llama model family compare to other architectural families like Gemma and Qwen when subjected to the C-BOD framework's textual perturbations?",
              "answer": "LLMs exhibit differential robustness across architectural families, with Llama models showing greater stability against prompt perturbations compared to others like Gemma and Qwen."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "The article states that larger models tend to exhibit greater performance degradation. How does this finding apply to the differential robustness observed across specific architectural families like Gemma and Qwen, as shown in the results?",
              "answer": "Larger LLMs and those with higher initial accuracy tend to be more sensitive to prompt rephrasing, indicating a greater reliance on surface-level cues and potential overfitting. This suggests that families like Gemma and Qwen, if they are larger or have higher baseline accuracy, may exhibit more degradation compared to more robust families."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "According to the findings, what is the relationship between a model family's baseline accuracy on the original MMLU benchmark and its robustness against prompt rephrasing?",
              "answer": "Models with higher baseline accuracy exhibited greater performance declines, suggesting they are more susceptible to overfitting fixed prompt patterns. Conversely, models with lower baseline accuracy showed significantly less degradation, indicating reduced dependency on superficial cues."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "Besides the Llama family, which other models from different architectural families were found to have statistically insignificant performance differences, and what does this suggest about their robustness?",
              "answer": "Models with lower baseline accuracy, such as certain DeepSeek and Qwen variants, displayed statistically insignificant performance degradation, suggesting a reduced dependency on superficial patterns and thus greater robustness."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "What specific reasons does the article suggest for why models from the Qwen family failed on rephrased questions, and how does this illustrate a lack of robustness compared to more stable models?",
              "answer": "The article suggests that models from the Qwen family, particularly variants with lower baseline accuracy, displayed statistically insignificant performance degradation, indicating reduced dependency on superficial patterns. This contrasts with more stable models and implies a greater robustness against prompt rephrasing."
            },
            "result": false,
            "issue": "The provided answer incorrectly states that Qwen models showed insignificant performance degradation. The article (Table 2) indicates that several Qwen models, such as Qwen2.5 3B, 7B, and 32B, showed statistically significant performance degradation ('Yes' in the 'Sig.' column). The answer also misinterprets the reason for failure by stating it implies greater robustness, when the failures themselves illustrate a lack of robustness."
          },
          {
            "qa": {
              "question": "ACU. Which LLM family is noted for showing insignificant performance degradation, suggesting greater robustness?",
              "answer": "The Llama family is noted for showing insignificant performance degradation, suggesting greater robustness."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "ACU. What is the name of the benchmark dataset used to compare the performance of the different LLM families?",
              "answer": "The benchmark dataset used to compare the performance of the different LLM families is MMLU."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "ACU. Out of the 26 LLMs evaluated, how many showed statistically significant performance differences when tested on rephrased prompts?",
              "answer": "Out of the 26 LLMs evaluated, 20 models showed statistically significant degradation when subjected to modest perturbations."
            },
            "result": true,
            "issue": ""
          }
        ],
        "judgment": false,
        "sections_to_highlight": [
          "Results — Model robustness comparison (Llama vs. Gemma/Qwen)",
          "Results — Model size vs. degradation",
          "Results — Accuracy vs. degradation",
          "Discussion — LLM overfitting reasons",
          "Limitations — C-BOD scope and cost",
          "Results — Other models with insignificant performance differences"
        ]
      },
      "correct_count_all": 7,
      "correct_count_acu": 3,
      "num_of_questions": 5,
      "sections_to_highlight": [
        "Limitations — Scope of C-BOD",
        "Discussion — LLM overfitting reasons",
        "Results — Model robustness comparison",
        "Results — Model robustness comparison (Llama vs. Gemma/Qwen)",
        "Results — Model size vs. degradation (specific families)",
        "Results — Model size vs. degradation",
        "Discussion — Reasons for Qwen family failures",
        "Limitations — C-BOD scope and cost",
        "Methodology — C-BOD framework",
        "Discussion — Implications for LLM evaluation",
        "Results — Other models with insignificant performance differences",
        "Introduction — Problem of LLM evaluation",
        "Results — Accuracy vs. degradation"
      ],
      "sections_to_highlight_size": 13
    },
    {
      "iteration_number": 5,
      "summary": "1. SUMMARY:\nThis paper introduces the Chameleon Benchmark Overfit Detector (C-BOD) to assess LLM robustness against prompt rephrasing, revealing reliance on surface cues rather than true understanding. Evaluating 26 LLMs on MMLU, C-BOD found that most models (20/26) exhibited statistically significant performance degradation under modest perturbations, with an average accuracy drop of 2.15%. Notably, larger models and those with higher baseline accuracy were more susceptible to this degradation, suggesting they over-rely on fixed prompt patterns. In contrast, the Llama family and models with lower baseline accuracy demonstrated reduced dependency on superficial cues, showing insignificant performance drops. The study highlights a paradox where high benchmark scores might mask vulnerabilities. C-BOD's dataset- and model-agnostic design allows integration into training to promote generalization, challenging the community to prioritize resilience over leaderboard scores. Limitations include the focus on textual rephrasing and potential computational costs for training integration.\n\n2. KEY HIGHLIGHTS:\n*   LLMs often overfit to benchmark surface cues, leading to performance drops when prompts are rephrased, as demonstrated by C-BOD.\n*   Larger models and those with higher baseline accuracy tend to be more sensitive to prompt perturbations, indicating a greater reliance on superficial patterns.\n*   The Llama family of models, along with those with lower baseline accuracy, exhibit greater robustness and less degradation under rephrasing.\n*   A significant positive correlation exists between a model's original accuracy and its performance degradation when faced with rephrased prompts.\n*   C-BOD's framework is dataset-agnostic but faces limitations in detecting deeper forms of overfitting and can incur significant computational costs for training integration.",
      "qa_pairs": [
        {
          "question": "How did the robustness of the Llama model family compare to other architectural families like Gemma and Qwen when subjected to the C-BOD framework's textual perturbations?",
          "answer": "Not enough information in summary"
        },
        {
          "question": "The article states that larger models tend to exhibit greater performance degradation. How does this finding apply to the differential robustness observed across specific architectural families like Gemma and Qwen, as shown in the results?",
          "answer": "Not enough information in summary"
        },
        {
          "question": "According to the findings, what is the relationship between a model family's baseline accuracy on the original MMLU benchmark and its robustness against prompt rephrasing?",
          "answer": "A significant positive correlation exists between a model's original accuracy and its performance degradation when faced with rephrased prompts. Models with lower baseline accuracy demonstrated reduced dependency on superficial cues, showing insignificant performance drops."
        },
        {
          "question": "Besides the Llama family, which other models from different architectural families were found to have statistically insignificant performance differences, and what does this suggest about their robustness?",
          "answer": "Not enough information in summary"
        },
        {
          "question": "What specific reasons does the article suggest for why models from the Qwen family failed on rephrased questions, and how does this illustrate a lack of robustness compared to more stable models?",
          "answer": "Not enough information in summary"
        },
        {
          "question": "ACU. Which LLM family is noted for showing insignificant performance degradation, suggesting greater robustness?",
          "answer": "The Llama family of models is noted for showing insignificant performance degradation, suggesting greater robustness."
        },
        {
          "question": "ACU. What is the name of the benchmark dataset used to compare the performance of the different LLM families?",
          "answer": "MMLU"
        },
        {
          "question": "ACU. Out of the 26 LLMs evaluated, how many showed statistically significant performance differences when tested on rephrased prompts?",
          "answer": "Most models (20/26) exhibited statistically significant performance degradation under modest perturbations."
        }
      ],
      "judge": {
        "evaluations": [
          {
            "qa": {
              "question": "How did the robustness of the Llama model family compare to other architectural families like Gemma and Qwen when subjected to the C-BOD framework's textual perturbations?",
              "answer": "Not enough information in summary"
            },
            "result": false,
            "issue": "The summary states that Llama models showed insignificant degradation, but does not provide a comparison with Gemma and Qwen for robustness against textual perturbations."
          },
          {
            "qa": {
              "question": "The article states that larger models tend to exhibit greater performance degradation. How does this finding apply to the differential robustness observed across specific architectural families like Gemma and Qwen, as shown in the results?",
              "answer": "Not enough information in summary"
            },
            "result": false,
            "issue": "The summary mentions that larger models tend to degrade more, but does not specify how this applies to the differential robustness of Gemma and Qwen families."
          },
          {
            "qa": {
              "question": "According to the findings, what is the relationship between a model family's baseline accuracy on the original MMLU benchmark and its robustness against prompt rephrasing?",
              "answer": "A significant positive correlation exists between a model's original accuracy and its performance degradation when faced with rephrased prompts. Models with lower baseline accuracy demonstrated reduced dependency on superficial cues, showing insignificant performance drops."
            },
            "result": true,
            "issue": null
          },
          {
            "qa": {
              "question": "Besides the Llama family, which other models from different architectural families were found to have statistically insignificant performance differences, and what does this suggest about their robustness?",
              "answer": "Not enough information in summary"
            },
            "result": false,
            "issue": "The summary mentions Llama family and models with lower baseline accuracy showed insignificant degradation, but does not list specific other models or families."
          },
          {
            "qa": {
              "question": "What specific reasons does the article suggest for why models from the Qwen family failed on rephrased questions, and how does this illustrate a lack of robustness compared to more stable models?",
              "answer": "Not enough information in summary"
            },
            "result": false,
            "issue": "The summary does not provide specific reasons for Qwen family failures on rephrased questions."
          },
          {
            "qa": {
              "question": "ACU. Which LLM family is noted for showing insignificant performance degradation, suggesting greater robustness?",
              "answer": "The Llama family of models is noted for showing insignificant performance degradation, suggesting greater robustness."
            },
            "result": true,
            "issue": null
          },
          {
            "qa": {
              "question": "ACU. What is the name of the benchmark dataset used to compare the performance of the different LLM families?",
              "answer": "MMLU"
            },
            "result": true,
            "issue": null
          },
          {
            "qa": {
              "question": "ACU. Out of the 26 LLMs evaluated, how many showed statistically significant performance differences when tested on rephrased prompts?",
              "answer": "Most models (20/26) exhibited statistically significant performance degradation under modest perturbations."
            },
            "result": true,
            "issue": null
          }
        ],
        "judgment": false,
        "sections_to_highlight": [
          "Results — Model robustness comparison (Llama vs. Gemma/Qwen)",
          "Results — Other models with insignificant performance differences",
          "Discussion — Reasons for Qwen family failures",
          "Results — Accuracy vs. degradation",
          "Introduction — Problem of LLM evaluation",
          "Methodology — C-BOD framework",
          "Results — Model size vs. degradation"
        ]
      },
      "correct_count_all": 4,
      "correct_count_acu": 3,
      "num_of_questions": 5,
      "sections_to_highlight": [
        "Limitations — Scope of C-BOD",
        "Discussion — LLM overfitting reasons",
        "Results — Model robustness comparison",
        "Results — Model robustness comparison (Llama vs. Gemma/Qwen)",
        "Results — Model size vs. degradation (specific families)",
        "Results — Model size vs. degradation",
        "Discussion — Reasons for Qwen family failures",
        "Limitations — C-BOD scope and cost",
        "Methodology — C-BOD framework",
        "Discussion — Implications for LLM evaluation",
        "Results — Other models with insignificant performance differences",
        "Introduction — Problem of LLM evaluation",
        "Results — Accuracy vs. degradation"
      ],
      "sections_to_highlight_size": 13
    },
    {
      "iteration_number": 6,
      "summary": "1. SUMMARY:\nThis paper introduces the Chameleon Benchmark Overfit Detector (C-BOD), a meta-evaluation framework designed to expose LLMs' overreliance on surface-level cues in benchmark prompts. By systematically rephrasing prompts while preserving semantic meaning, C-BOD measures performance degradation. Evaluations on the MMLU benchmark with 26 LLMs revealed that most models (20 out of 26) exhibited statistically significant performance drops under these perturbations, averaging 2.15%. Notably, larger models and those with higher baseline accuracy showed greater sensitivity to rephrasing, suggesting overfitting to specific prompt patterns. In contrast, the Llama family and models with lower baseline accuracy demonstrated reduced degradation, indicating greater robustness. The framework's dataset- and model-agnostic nature allows for integration into training pipelines to foster more generalizable language understanding, challenging the community to prioritize resilience over leaderboard scores.\n\n2. KEY HIGHLIGHTS:\n*   The C-BOD framework detects LLM overfitting by measuring performance degradation on semantically preserved, rephrased prompts.\n*   Most evaluated LLMs (20/26) showed statistically significant performance drops (avg. 2.15%) on rephrased MMLU prompts, indicating over-reliance on surface cues.\n*   Larger LLMs and those with higher baseline accuracy tend to be more susceptible to performance degradation due to prompt rephrasing.\n*   The Llama family, along with models exhibiting lower baseline accuracy, demonstrated greater robustness with insignificant performance declines under perturbation.\n*   The study highlights a paradox where high benchmark scores may mask a lack of genuine generalization and sensitivity to prompt variations.",
      "qa_pairs": [
        {
          "question": "How did the robustness of the Llama model family compare to other architectural families like Gemma and Qwen when subjected to the C-BOD framework's textual perturbations?",
          "answer": "The summary states that the Llama family demonstrated greater robustness with insignificant performance declines under perturbation, but it does not provide a comparison with Gemma and Qwen families."
        },
        {
          "question": "The article states that larger models tend to exhibit greater performance degradation. How does this finding apply to the differential robustness observed across specific architectural families like Gemma and Qwen, as shown in the results?",
          "answer": "The summary indicates that larger models tend to be more susceptible to performance degradation due to prompt rephrasing, but it does not apply this finding to specific architectural families like Gemma and Qwen or discuss their differential robustness."
        },
        {
          "question": "According to the findings, what is the relationship between a model family's baseline accuracy on the original MMLU benchmark and its robustness against prompt rephrasing?",
          "answer": "Models with higher baseline accuracy on the original MMLU benchmark showed greater sensitivity to rephrasing, indicating less robustness against prompt rephrasing. Conversely, models with lower baseline accuracy demonstrated greater robustness."
        },
        {
          "question": "Besides the Llama family, which other models from different architectural families were found to have statistically insignificant performance differences, and what does this suggest about their robustness?",
          "answer": "The summary mentions that models with lower baseline accuracy, in addition to the Llama family, demonstrated greater robustness with insignificant performance declines, but it does not name other specific architectural families."
        },
        {
          "question": "What specific reasons does the article suggest for why models from the Qwen family failed on rephrased questions, and how does this illustrate a lack of robustness compared to more stable models?",
          "answer": "Not enough information in summary"
        },
        {
          "question": "ACU. Which LLM family is noted for showing insignificant performance degradation, suggesting greater robustness?",
          "answer": "The Llama family is noted for showing insignificant performance degradation, suggesting greater robustness."
        },
        {
          "question": "ACU. What is the name of the benchmark dataset used to compare the performance of the different LLM families?",
          "answer": "The benchmark dataset used to compare the performance of the different LLM families is MMLU."
        },
        {
          "question": "ACU. Out of the 26 LLMs evaluated, how many showed statistically significant performance differences when tested on rephrased prompts?",
          "answer": "Out of the 26 LLMs evaluated, 20 showed statistically significant performance differences when tested on rephrased prompts."
        }
      ],
      "judge": {
        "evaluations": [
          {
            "qa": {
              "question": "How did the robustness of the Llama model family compare to other architectural families like Gemma and Qwen when subjected to the C-BOD framework's textual perturbations?",
              "answer": "The summary states that the Llama family demonstrated greater robustness with insignificant performance declines under perturbation, but it does not provide a comparison with Gemma and Qwen families."
            },
            "result": false,
            "issue": "The answer accurately reflects the summary's content but fails to provide a comparison with Gemma and Qwen families as requested by the question."
          },
          {
            "qa": {
              "question": "The article states that larger models tend to exhibit greater performance degradation. How does this finding apply to the differential robustness observed across specific architectural families like Gemma and Qwen, as shown in the results?",
              "answer": "The summary indicates that larger models tend to be more susceptible to performance degradation due to prompt rephrasing, but it does not apply this finding to specific architectural families like Gemma and Qwen or discuss their differential robustness."
            },
            "result": false,
            "issue": "The answer correctly states that the summary does not provide the requested specific application of findings to Gemma and Qwen families."
          },
          {
            "qa": {
              "question": "According to the findings, what is the relationship between a model family's baseline accuracy on the original MMLU benchmark and its robustness against prompt rephrasing?",
              "answer": "Models with higher baseline accuracy on the original MMLU benchmark showed greater sensitivity to rephrasing, indicating less robustness against prompt rephrasing. Conversely, models with lower baseline accuracy demonstrated greater robustness."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "Besides the Llama family, which other models from different architectural families were found to have statistically insignificant performance differences, and what does this suggest about their robustness?",
              "answer": "The summary mentions that models with lower baseline accuracy, in addition to the Llama family, demonstrated greater robustness with insignificant performance declines, but it does not name other specific architectural families."
            },
            "result": false,
            "issue": "The answer correctly identifies that the summary does not name other specific families with insignificant performance differences."
          },
          {
            "qa": {
              "question": "What specific reasons does the article suggest for why models from the Qwen family failed on rephrased questions, and how does this illustrate a lack of robustness compared to more stable models?",
              "answer": "Not enough information in summary"
            },
            "result": false,
            "issue": "The summary does not contain information about specific reasons for Qwen family failures."
          },
          {
            "qa": {
              "question": "ACU. Which LLM family is noted for showing insignificant performance degradation, suggesting greater robustness?",
              "answer": "The Llama family is noted for showing insignificant performance degradation, suggesting greater robustness."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "ACU. What is the name of the benchmark dataset used to compare the performance of the different LLM families?",
              "answer": "The benchmark dataset used to compare the performance of the different LLM families is MMLU."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "ACU. Out of the 26 LLMs evaluated, how many showed statistically significant performance differences when tested on rephrased prompts?",
              "answer": "Out of the 26 LLMs evaluated, 20 showed statistically significant performance differences when tested on rephrased prompts."
            },
            "result": true,
            "issue": ""
          }
        ],
        "judgment": false,
        "sections_to_highlight": [
          "Results — Robustness comparison (Llama vs. other families)",
          "Results — Model size vs. degradation (specific families)",
          "Discussion — Reasons for Qwen family failures",
          "Results — Models with insignificant performance differences",
          "Introduction — Problem of LLM evaluation",
          "Results — Accuracy vs. degradation"
        ]
      },
      "correct_count_all": 4,
      "correct_count_acu": 3,
      "num_of_questions": 5,
      "sections_to_highlight": [
        "Limitations — Scope of C-BOD",
        "Discussion — LLM overfitting reasons",
        "Results — Model robustness comparison",
        "Results — Model robustness comparison (Llama vs. Gemma/Qwen)",
        "Results — Model size vs. degradation (specific families)",
        "Results — Model size vs. degradation",
        "Discussion — Reasons for Qwen family failures",
        "Limitations — C-BOD scope and cost",
        "Methodology — C-BOD framework",
        "Discussion — Implications for LLM evaluation",
        "Results — Other models with insignificant performance differences",
        "Introduction — Problem of LLM evaluation",
        "Results — Accuracy vs. degradation"
      ],
      "sections_to_highlight_size": 13
    },
    {
      "iteration_number": 7,
      "summary": "1. SUMMARY:\nThis paper introduces the Chameleon Benchmark Overfit Detector (C-BOD), a meta-evaluation framework designed to expose LLMs' overreliance on dataset-specific surface cues rather than true language understanding. By systematically rephrasing benchmark prompts while preserving semantic content, C-BOD quantifies performance degradation under textual perturbations. Evaluated on the MMLU benchmark with 26 LLMs, C-BOD revealed an average accuracy drop of 2.15%, with 20 models showing statistically significant performance differences. Notably, larger models and those with higher baseline accuracy tend to be more sensitive to rephrasing, indicating a reliance on fixed prompt patterns. In contrast, the Llama family and models with lower baseline accuracy exhibited less significant degradation, suggesting reduced dependency on superficial cues. The framework's dataset- and model-agnostic design allows for integration into training pipelines to promote more robust language understanding, challenging the community to prioritize resilience over leaderboard scores.\n\n2. KEY HIGHLIGHTS:\n*   C-BOD detects LLM overfitting by measuring performance drops on semantically equivalent but rephrased prompts, revealing reliance on surface cues.\n*   Larger LLMs and those with higher baseline accuracy are more susceptible to performance degradation under prompt perturbations, suggesting over-reliance on specific phrasing.\n*   The Llama family of models demonstrated greater robustness to rephrasing compared to many other families, indicating less dependency on superficial cues.\n*   Models with lower baseline accuracy, such as Falcon 7B, DeepSeek 7B, Qwen 2.5 3B, and Jetmoe 8B, showed statistically insignificant performance differences, implying less overfitting.\n*   The study highlights a paradox where high benchmark scores may mask vulnerabilities, emphasizing the need for evaluations that prioritize resilience and generalization.",
      "qa_pairs": [
        {
          "question": "How did the robustness of the Llama model family compare to other architectural families like Gemma and Qwen when subjected to the C-BOD framework's textual perturbations?",
          "answer": "The Llama family of models demonstrated greater robustness to rephrasing compared to many other families, indicating less dependency on superficial cues. The summary does not provide a direct comparison with Gemma or Qwen specifically in this regard, only mentioning Llama's general robustness."
        },
        {
          "question": "The article states that larger models tend to exhibit greater performance degradation. How does this finding apply to the differential robustness observed across specific architectural families like Gemma and Qwen, as shown in the results?",
          "answer": "The article states that larger models tend to exhibit greater performance degradation. It also mentions that models with higher baseline accuracy tend to be more sensitive to rephrasing. However, it does not provide specific details on how these findings apply to the differential robustness of Gemma and Qwen families."
        },
        {
          "question": "According to the findings, what is the relationship between a model family's baseline accuracy on the original MMLU benchmark and its robustness against prompt rephrasing?",
          "answer": "Models with higher baseline accuracy tend to be more sensitive to rephrasing, indicating a reliance on fixed prompt patterns. In contrast, models with lower baseline accuracy exhibited less significant degradation, suggesting reduced dependency on superficial cues."
        },
        {
          "question": "Besides the Llama family, which other models from different architectural families were found to have statistically insignificant performance differences, and what does this suggest about their robustness?",
          "answer": "Models with lower baseline accuracy, such as Falcon 7B, DeepSeek 7B, Qwen 2.5 3B, and Jetmoe 8B, showed statistically insignificant performance differences, implying less overfitting and greater robustness."
        },
        {
          "question": "What specific reasons does the article suggest for why models from the Qwen family failed on rephrased questions, and how does this illustrate a lack of robustness compared to more stable models?",
          "answer": "The summary mentions that Qwen 2.5 3B showed statistically insignificant performance differences, implying less overfitting. It does not suggest reasons for failure on rephrased questions for the Qwen family."
        },
        {
          "question": "ACU. Which LLM family is noted for showing insignificant performance degradation, suggesting greater robustness?",
          "answer": "The Llama family of models is noted for showing insignificant performance degradation, suggesting greater robustness."
        },
        {
          "question": "ACU. What is the name of the benchmark dataset used to compare the performance of the different LLM families?",
          "answer": "The MMLU benchmark dataset was used to compare the performance of the different LLM families."
        },
        {
          "question": "ACU. Out of the 26 LLMs evaluated, how many showed statistically significant performance differences when tested on rephrased prompts?",
          "answer": "Out of the 26 LLMs evaluated, 20 models showed statistically significant performance differences when tested on rephrased prompts."
        }
      ],
      "judge": {
        "evaluations": [
          {
            "qa": {
              "question": "How did the robustness of the Llama model family compare to other architectural families like Gemma and Qwen when subjected to the C-BOD framework's textual perturbations?",
              "answer": "The Llama family of models demonstrated greater robustness to rephrasing compared to many other families, indicating less dependency on superficial cues. The summary does not provide a direct comparison with Gemma or Qwen specifically in this regard, only mentioning Llama's general robustness."
            },
            "result": false,
            "issue": "The answer correctly states Llama's robustness but incorrectly claims the summary doesn't compare it to Gemma and Qwen. The summary does mention: 'For example, models in the Gemma family show a progressive increase in ∆1.0 with higher parameter counts, while Llama models maintain low ∆1.0 values across scales.' This implies a comparison."
          },
          {
            "qa": {
              "question": "The article states that larger models tend to exhibit greater performance degradation. How does this finding apply to the differential robustness observed across specific architectural families like Gemma and Qwen, as shown in the results?",
              "answer": "The article states that larger models tend to exhibit greater performance degradation. It also mentions that models with higher baseline accuracy tend to be more sensitive to rephrasing. However, it does not provide specific details on how these findings apply to the differential robustness of Gemma and Qwen families."
            },
            "result": false,
            "issue": "The answer is partially correct but misses the specific comparative detail provided in the article regarding Gemma and Llama families: 'For example, models in the Gemma family show a progressive increase in ∆1.0 with higher parameter counts, while Llama models maintain low ∆1.0 values across scales.' This directly addresses how size affects robustness within these families."
          },
          {
            "qa": {
              "question": "According to the findings, what is the relationship between a model family's baseline accuracy on the original MMLU benchmark and its robustness against prompt rephrasing?",
              "answer": "Models with higher baseline accuracy tend to be more sensitive to rephrasing, indicating a reliance on fixed prompt patterns. In contrast, models with lower baseline accuracy exhibited less significant degradation, suggesting reduced dependency on superficial cues."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "Besides the Llama family, which other models from different architectural families were found to have statistically insignificant performance differences, and what does this suggest about their robustness?",
              "answer": "Models with lower baseline accuracy, such as Falcon 7B, DeepSeek 7B, Qwen 2.5 3B, and Jetmoe 8B, showed statistically insignificant performance differences, implying less overfitting and greater robustness."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "What specific reasons does the article suggest for why models from the Qwen family failed on rephrased questions, and how does this illustrate a lack of robustness compared to more stable models?",
              "answer": "The summary mentions that Qwen 2.5 3B showed statistically insignificant performance differences, implying less overfitting. It does not suggest reasons for failure on rephrased questions for the Qwen family."
            },
            "result": false,
            "issue": "The article (Table 3) provides specific examples of Qwen2.5 (32B parameters) failing on rephrased questions and explains why (e.g., legal context, word changes). The summary and answer omit this crucial detail."
          },
          {
            "qa": {
              "question": "ACU. Which LLM family is noted for showing insignificant performance degradation, suggesting greater robustness?",
              "answer": "The Llama family of models is noted for showing insignificant performance degradation, suggesting greater robustness."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "ACU. What is the name of the benchmark dataset used to compare the performance of the different LLM families?",
              "answer": "The MMLU benchmark dataset was used to compare the performance of the different LLM families."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "ACU. Out of the 26 LLMs evaluated, how many showed statistically significant performance differences when tested on rephrased prompts?",
              "answer": "Out of the 26 LLMs evaluated, 20 models showed statistically significant performance differences when tested on rephrased prompts."
            },
            "result": true,
            "issue": ""
          }
        ],
        "judgment": false,
        "sections_to_highlight": [
          "Results — Model robustness comparison (Llama vs. Gemma/Qwen)",
          "Discussion — Reasons for Qwen family failures",
          "Results — Model size vs. degradation (specific families)",
          "Results — Models with insignificant performance differences",
          "Methodology — C-BOD framework",
          "Results — Accuracy vs. degradation"
        ]
      },
      "correct_count_all": 5,
      "correct_count_acu": 3,
      "num_of_questions": 5,
      "sections_to_highlight": [
        "Limitations — Scope of C-BOD",
        "Discussion — LLM overfitting reasons",
        "Results — Model robustness comparison",
        "Results — Model robustness comparison (Llama vs. Gemma/Qwen)",
        "Results — Model size vs. degradation (specific families)",
        "Results — Model size vs. degradation",
        "Discussion — Reasons for Qwen family failures",
        "Limitations — C-BOD scope and cost",
        "Methodology — C-BOD framework",
        "Results — Robustness comparison (Llama vs. other families)",
        "Results — Models with insignificant performance differences",
        "Discussion — Implications for LLM evaluation",
        "Results — Other models with insignificant performance differences",
        "Introduction — Problem of LLM evaluation",
        "Results — Accuracy vs. degradation"
      ],
      "sections_to_highlight_size": 15
    },
    {
      "iteration_number": 8,
      "summary": "1. SUMMARY:\nThis paper introduces the Chameleon Benchmark Overfit Detector (C-BOD), a meta-evaluation framework designed to identify over-reliance on surface-level cues in Large Language Models (LLMs). By systematically rephrasing benchmark prompts while preserving semantic content, C-BOD exposes LLMs that might be \"overfitting\" to specific prompt formats rather than demonstrating true language understanding. Across 26 leading LLMs evaluated on the MMLU benchmark, C-BOD revealed an average performance degradation of 2.15% under modest textual perturbations. Notably, larger models and those with higher baseline accuracy tended to be more sensitive to these rephrased prompts, indicating a potential over-reliance on fixed patterns. In contrast, the Llama family of models and those with lower baseline accuracy showed less significant degradation, suggesting a reduced dependency on superficial cues. The framework's dataset- and model-agnostic design allows for integration into training pipelines to promote more robust language understanding. The findings challenge the community to look beyond leaderboard scores and prioritize resilience and generalization in LLM evaluations, highlighting differential robustness across LLM architectural families.\n\n2. KEY HIGHLIGHTS:\n*   The C-BOD framework detects LLM overfitting to benchmark-specific surface cues by rephrasing prompts, revealing a significant average accuracy drop of 2.15% across 26 LLMs.\n*   Larger LLMs and those with higher baseline accuracy generally exhibit greater performance degradation under prompt perturbations, suggesting they are more prone to overfitting.\n*   The Llama family of models demonstrated greater robustness, showing less performance degradation compared to other families like Gemma and Qwen, especially as model size increased.\n*   Models with lower baseline accuracy, such as Falcon 7B, DeepSeek 7B, and Qwen 2.5 3B, often showed statistically insignificant performance differences, indicating less reliance on superficial cues.\n*   The study implies that architectural choices and training strategies significantly influence differential robustness, urging a shift in LLM evaluation towards prioritizing generalization over raw benchmark scores.",
      "qa_pairs": [
        {
          "question": "How did the robustness of the Llama model family compare to other architectural families like Gemma and Qwen when subjected to the C-BOD framework's textual perturbations?",
          "answer": "The Llama family of models demonstrated greater robustness, showing less performance degradation compared to other families like Gemma and Qwen, especially as model size increased."
        },
        {
          "question": "The article states that larger models tend to exhibit greater performance degradation. How does this finding apply to the differential robustness observed across specific architectural families like Gemma and Qwen, as shown in the results?",
          "answer": "Larger models and those with higher baseline accuracy generally exhibit greater performance degradation under prompt perturbations. The Llama family of models demonstrated greater robustness, showing less performance degradation compared to other families like Gemma and Qwen, especially as model size increased."
        },
        {
          "question": "According to the findings, what is the relationship between a model family's baseline accuracy on the original MMLU benchmark and its robustness against prompt rephrasing?",
          "answer": "Models with lower baseline accuracy, such as Falcon 7B, DeepSeek 7B, and Qwen 2.5 3B, often showed statistically insignificant performance differences, indicating less reliance on superficial cues and thus greater robustness against prompt rephrasing."
        },
        {
          "question": "Besides the Llama family, which other models from different architectural families were found to have statistically insignificant performance differences, and what does this suggest about their robustness?",
          "answer": "Models with lower baseline accuracy, such as Falcon 7B, DeepSeek 7B, and Qwen 2.5 3B, often showed statistically insignificant performance differences, indicating less reliance on superficial cues and thus greater robustness."
        },
        {
          "question": "What specific reasons does the article suggest for why models from the Qwen family failed on rephrased questions, and how does this illustrate a lack of robustness compared to more stable models?",
          "answer": "Not enough information in summary"
        },
        {
          "question": "ACU. Which LLM family is noted for showing insignificant performance degradation, suggesting greater robustness?",
          "answer": "The Llama family of models is noted for showing insignificant performance degradation, suggesting greater robustness."
        },
        {
          "question": "ACU. What is the name of the benchmark dataset used to compare the performance of the different LLM families?",
          "answer": "The MMLU benchmark dataset was used to compare the performance of the different LLM families."
        },
        {
          "question": "ACU. Out of the 26 LLMs evaluated, how many showed statistically significant performance differences when tested on rephrased prompts?",
          "answer": "Not enough information in summary"
        }
      ],
      "judge": {
        "evaluations": [
          {
            "qa": {
              "question": "How did the robustness of the Llama model family compare to other architectural families like Gemma and Qwen when subjected to the C-BOD framework's textual perturbations?",
              "answer": "The Llama family of models demonstrated greater robustness, showing less performance degradation compared to other families like Gemma and Qwen, especially as model size increased."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "The article states that larger models tend to exhibit greater performance degradation. How does this finding apply to the differential robustness observed across specific architectural families like Gemma and Qwen, as shown in the results?",
              "answer": "Larger models and those with higher baseline accuracy generally exhibit greater performance degradation under prompt perturbations. The Llama family of models demonstrated greater robustness, showing less performance degradation compared to other families like Gemma and Qwen, especially as model size increased."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "According to the findings, what is the relationship between a model family's baseline accuracy on the original MMLU benchmark and its robustness against prompt rephrasing?",
              "answer": "Models with lower baseline accuracy, such as Falcon 7B, DeepSeek 7B, and Qwen 2.5 3B, often showed statistically insignificant performance differences, indicating less reliance on superficial cues and thus greater robustness against prompt rephrasing."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "Besides the Llama family, which other models from different architectural families were found to have statistically insignificant performance differences, and what does this suggest about their robustness?",
              "answer": "Models with lower baseline accuracy, such as Falcon 7B, DeepSeek 7B, and Qwen 2.5 3B, often showed statistically insignificant performance differences, indicating less reliance on superficial cues and thus greater robustness."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "What specific reasons does the article suggest for why models from the Qwen family failed on rephrased questions, and how does this illustrate a lack of robustness compared to more stable models?",
              "answer": "Not enough information in summary"
            },
            "result": false,
            "issue": "The summary does not contain specific reasons for the Qwen family's failures on rephrased questions. The article mentions that Qwen models showed a performance degradation, but the specific reasons for their failures are not detailed in the summary."
          },
          {
            "qa": {
              "question": "ACU. Which LLM family is noted for showing insignificant performance degradation, suggesting greater robustness?",
              "answer": "The Llama family of models is noted for showing insignificant performance degradation, suggesting greater robustness."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "ACU. What is the name of the benchmark dataset used to compare the performance of the different LLM families?",
              "answer": "The MMLU benchmark dataset was used to compare the performance of the different LLM families."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "ACU. Out of the 26 LLMs evaluated, how many showed statistically significant performance differences when tested on rephrased prompts?",
              "answer": "Not enough information in summary"
            },
            "result": false,
            "issue": "The summary states that '20 out of 26 models exhibiting statistically significant differences' but does not directly answer the question of how many showed statistically significant differences. It implies 20, but the phrasing is slightly indirect."
          }
        ],
        "judgment": false,
        "sections_to_highlight": [
          "Results — LLM family robustness comparison",
          "Results — Model size vs. performance degradation",
          "Results — Accuracy vs. performance degradation",
          "Discussion — LLM overfitting reasons",
          "Limitations — C-BOD scope and cost",
          "Introduction — Problem of LLM evaluation",
          "Results — Models with insignificant performance differences"
        ]
      },
      "correct_count_all": 6,
      "correct_count_acu": 2,
      "num_of_questions": 5,
      "sections_to_highlight": [
        "Limitations — Scope of C-BOD",
        "Discussion — LLM overfitting reasons",
        "Results — Model robustness comparison",
        "Results — Model robustness comparison (Llama vs. Gemma/Qwen)",
        "Results — Model size vs. degradation (specific families)",
        "Results — Model size vs. degradation",
        "Discussion — Reasons for Qwen family failures",
        "Limitations — C-BOD scope and cost",
        "Methodology — C-BOD framework",
        "Results — Robustness comparison (Llama vs. other families)",
        "Results — Models with insignificant performance differences",
        "Discussion — Implications for LLM evaluation",
        "Results — Other models with insignificant performance differences",
        "Introduction — Problem of LLM evaluation",
        "Results — Accuracy vs. degradation"
      ],
      "sections_to_highlight_size": 15
    },
    {
      "iteration_number": 9,
      "summary": "1. SUMMARY:\nThis paper introduces the Chameleon Benchmark Overfit Detector (C-BOD) to evaluate Large Language Model (LLM) robustness against prompt perturbations, revealing an overreliance on surface cues. The study found that across 26 LLMs, an average accuracy drop of 2.15% occurred with modest rephrasing, indicating widespread overfitting. Notably, larger models and those with higher baseline accuracy exhibited greater performance degradation, suggesting they are more susceptible to memorized patterns. Conversely, the Llama family and models with lower baseline accuracy demonstrated more robust performance, indicating reduced dependency on superficial cues. The C-BOD framework, which systematically distorts benchmark prompts while preserving semantic content, is dataset- and model-agnostic, offering a valuable tool for more reliable LLM evaluation and promoting the development of more generalizable models.\n\n2. KEY HIGHLIGHTS:\n* LLMs often overfit to benchmark-specific surface cues, leading to performance drops when prompts are rephrased, as demonstrated by an average 2.15% accuracy decrease across 26 models.\n* Larger LLMs and those with higher initial accuracy are more prone to performance degradation under prompt perturbations, suggesting a reliance on fixed patterns.\n* The Llama family of models, along with those exhibiting lower baseline accuracy, showed greater robustness and less sensitivity to rephrased prompts.\n* C-BOD is a novel, dataset-agnostic meta-evaluation framework that systematically distorts prompts to reveal LLM overfitting, encouraging a shift towards evaluating true generalization.",
      "qa_pairs": [
        {
          "question": "How did the robustness of the Llama model family compare to other architectural families like Gemma and Qwen when subjected to the C-BOD framework's textual perturbations?",
          "answer": "The Llama family of models demonstrated greater robustness and less sensitivity to rephrased prompts."
        },
        {
          "question": "The article states that larger models tend to exhibit greater performance degradation. How does this finding apply to the differential robustness observed across specific architectural families like Gemma and Qwen, as shown in the results?",
          "answer": "Not enough information in summary"
        },
        {
          "question": "According to the findings, what is the relationship between a model family's baseline accuracy on the original MMLU benchmark and its robustness against prompt rephrasing?",
          "answer": "Models with lower baseline accuracy demonstrated greater robustness and less sensitivity to rephrased prompts."
        },
        {
          "question": "Besides the Llama family, which other models from different architectural families were found to have statistically insignificant performance differences, and what does this suggest about their robustness?",
          "answer": "Not enough information in summary"
        },
        {
          "question": "What specific reasons does the article suggest for why models from the Qwen family failed on rephrased questions, and how does this illustrate a lack of robustness compared to more stable models?",
          "answer": "Not enough information in summary"
        },
        {
          "question": "ACU. Which LLM family is noted for showing insignificant performance degradation, suggesting greater robustness?",
          "answer": "The Llama family of models is noted for showing insignificant performance degradation, suggesting greater robustness."
        },
        {
          "question": "ACU. What is the name of the benchmark dataset used to compare the performance of the different LLM families?",
          "answer": "The Chameleon Benchmark Overfit Detector (C-BOD) framework was used to evaluate LLM robustness against prompt perturbations."
        },
        {
          "question": "ACU. Out of the 26 LLMs evaluated, how many showed statistically significant performance differences when tested on rephrased prompts?",
          "answer": "Not enough information in summary"
        }
      ],
      "judge": {
        "evaluations": [
          {
            "qa": {
              "question": "How did the robustness of the Llama model family compare to other architectural families like Gemma and Qwen when subjected to the C-BOD framework's textual perturbations?",
              "answer": "The Llama family of models demonstrated greater robustness and less sensitivity to rephrased prompts."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "The article states that larger models tend to exhibit greater performance degradation. How does this finding apply to the differential robustness observed across specific architectural families like Gemma and Qwen, as shown in the results?",
              "answer": "Not enough information in summary"
            },
            "result": false,
            "issue": "The summary does not provide specific details on how the general finding about larger models applies to the differential robustness of Gemma and Qwen families. The article contains this information in Section 5.2 and Figure 4."
          },
          {
            "qa": {
              "question": "According to the findings, what is the relationship between a model family's baseline accuracy on the original MMLU benchmark and its robustness against prompt rephrasing?",
              "answer": "Models with lower baseline accuracy demonstrated greater robustness and less sensitivity to rephrased prompts."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "Besides the Llama family, which other models from different architectural families were found to have statistically insignificant performance differences, and what does this suggest about their robustness?",
              "answer": "Not enough information in summary"
            },
            "result": false,
            "issue": "The summary does not list other models with statistically insignificant performance differences. The article mentions Falcon 7B, DeepSeek 7B, Qwen 2.5 3B, and Jetmoe 8B in Section 5.1."
          },
          {
            "qa": {
              "question": "What specific reasons does the article suggest for why models from the Qwen family failed on rephrased questions, and how does this illustrate a lack of robustness compared to more stable models?",
              "answer": "Not enough information in summary"
            },
            "result": false,
            "issue": "The summary does not provide specific reasons for the Qwen family's failures on rephrased questions. Table 3 in the article gives examples of Qwen failures and explanations."
          },
          {
            "qa": {
              "question": "ACU. Which LLM family is noted for showing insignificant performance degradation, suggesting greater robustness?",
              "answer": "The Llama family of models is noted for showing insignificant performance degradation, suggesting greater robustness."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "ACU. What is the name of the benchmark dataset used to compare the performance of the different LLM families?",
              "answer": "The Chameleon Benchmark Overfit Detector (C-BOD) framework was used to evaluate LLM robustness against prompt perturbations."
            },
            "result": false,
            "issue": "The question asks for the benchmark dataset, and the answer incorrectly names the framework used for evaluation. The article states MMLU is the benchmark dataset in Section 4.1."
          },
          {
            "qa": {
              "question": "ACU. Out of the 26 LLMs evaluated, how many showed statistically significant performance differences when tested on rephrased prompts?",
              "answer": "Not enough information in summary"
            },
            "result": false,
            "issue": "The summary does not state how many of the 26 LLMs showed statistically significant performance differences. The article states 20 out of 26 models exhibited statistically significant differences in the Abstract and Section 5.1."
          }
        ],
        "judgment": false,
        "sections_to_highlight": [
          "Results — Model size vs. degradation (specific families)",
          "Results — Models with insignificant performance differences",
          "Results — Accuracy vs. performance degradation",
          "Discussion — Reasons for Qwen family failures",
          "Results — LLM family robustness comparison",
          "Introduction — Problem of LLM evaluation",
          "Methodology — C-BOD framework"
        ]
      },
      "correct_count_all": 3,
      "correct_count_acu": 1,
      "num_of_questions": 5,
      "sections_to_highlight": [
        "Limitations — Scope of C-BOD",
        "Discussion — LLM overfitting reasons",
        "Results — Model robustness comparison",
        "Results — Model robustness comparison (Llama vs. Gemma/Qwen)",
        "Results — Model size vs. degradation (specific families)",
        "Results — Model size vs. degradation",
        "Discussion — Reasons for Qwen family failures",
        "Results — Model size vs. performance degradation",
        "Limitations — C-BOD scope and cost",
        "Methodology — C-BOD framework",
        "Results — Robustness comparison (Llama vs. other families)",
        "Results — Models with insignificant performance differences",
        "Results — Accuracy vs. performance degradation",
        "Discussion — Implications for LLM evaluation",
        "Results — Other models with insignificant performance differences",
        "Introduction — Problem of LLM evaluation",
        "Results — Accuracy vs. degradation",
        "Results — LLM family robustness comparison"
      ],
      "sections_to_highlight_size": 18
    },
    {
      "iteration_number": 10,
      "summary": "1. SUMMARY:\nThis paper introduces the Chameleon Benchmark Overfit Detector (C-BOD), a meta-evaluation framework designed to identify LLM overfitting to specific prompt surface cues. By systematically rephrasing benchmark prompts while preserving semantic content, C-BOD measures performance degradation. Experiments on the MMLU benchmark with 26 LLMs revealed that 20 models showed statistically significant performance drops under modest perturbations, averaging 2.15%. Notably, larger models and those with higher baseline accuracy exhibited greater degradation, suggesting a reliance on fixed prompt patterns. In contrast, the Llama family and models with lower baseline accuracy demonstrated reduced sensitivity to rephrasing, indicating less dependency on superficial cues. This highlights a critical issue in LLM evaluation: high benchmark scores may mask an overreliance on dataset-specific patterns rather than true language understanding, emphasizing the need for resilience-focused evaluation.\n\n2. KEY HIGHLIGHTS:\n*   LLMs exhibit differential robustness, with many models showing significant performance degradation when prompts are rephrased, indicating overfitting to surface cues.\n*   Larger LLMs and those with higher baseline accuracy tend to be more sensitive to prompt perturbations, suggesting a greater reliance on memorized patterns.\n*   The Llama family of models demonstrated greater robustness, showing less performance degradation compared to families like Gemma and Qwen.\n*   Models with lower baseline accuracy generally exhibited insignificant performance differences under prompt rephrasing, suggesting less overfitting.\n*   The C-BOD framework challenges current LLM evaluation practices, advocating for a focus on resilience and generalization beyond leaderboard scores.",
      "qa_pairs": [
        {
          "question": "How did the robustness of the Llama model family compare to other architectural families like Gemma and Qwen when subjected to the C-BOD framework's textual perturbations?",
          "answer": "The Llama family of models demonstrated greater robustness, showing less performance degradation compared to families like Gemma and Qwen."
        },
        {
          "question": "The article states that larger models tend to exhibit greater performance degradation. How does this finding apply to the differential robustness observed across specific architectural families like Gemma and Qwen, as shown in the results?",
          "answer": "Not enough information in summary"
        },
        {
          "question": "According to the findings, what is the relationship between a model family's baseline accuracy on the original MMLU benchmark and its robustness against prompt rephrasing?",
          "answer": "Models with lower baseline accuracy generally exhibited insignificant performance differences under prompt rephrasing, suggesting less overfitting. Larger models and those with higher baseline accuracy tended to be more sensitive to prompt perturbations."
        },
        {
          "question": "Besides the Llama family, which other models from different architectural families were found to have statistically insignificant performance differences, and what does this suggest about their robustness?",
          "answer": "Not enough information in summary"
        },
        {
          "question": "What specific reasons does the article suggest for why models from the Qwen family failed on rephrased questions, and how does this illustrate a lack of robustness compared to more stable models?",
          "answer": "Not enough information in summary"
        },
        {
          "question": "ACU. Which LLM family is noted for showing insignificant performance degradation, suggesting greater robustness?",
          "answer": "The Llama family of models demonstrated greater robustness, showing less performance degradation compared to families like Gemma and Qwen."
        },
        {
          "question": "ACU. What is the name of the benchmark dataset used to compare the performance of the different LLM families?",
          "answer": "The experiments were conducted on the MMLU benchmark."
        },
        {
          "question": "ACU. Out of the 26 LLMs evaluated, how many showed statistically significant performance differences when tested on rephrased prompts?",
          "answer": "20 models showed statistically significant performance drops under modest perturbations."
        }
      ],
      "judge": {
        "evaluations": [
          {
            "qa": {
              "question": "How did the robustness of the Llama model family compare to other architectural families like Gemma and Qwen when subjected to the C-BOD framework's textual perturbations?",
              "answer": "The Llama family of models demonstrated greater robustness, showing less performance degradation compared to families like Gemma and Qwen."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "The article states that larger models tend to exhibit greater performance degradation. How does this finding apply to the differential robustness observed across specific architectural families like Gemma and Qwen, as shown in the results?",
              "answer": "Not enough information in summary"
            },
            "result": false,
            "issue": "The summary does not provide specific details on how the general finding of larger models exhibiting greater performance degradation applies to the differential robustness observed across Gemma and Qwen families."
          },
          {
            "qa": {
              "question": "According to the findings, what is the relationship between a model family's baseline accuracy on the original MMLU benchmark and its robustness against prompt rephrasing?",
              "answer": "Models with lower baseline accuracy generally exhibited insignificant performance differences under prompt rephrasing, suggesting less overfitting. Larger models and those with higher baseline accuracy tended to be more sensitive to prompt perturbations."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "Besides the Llama family, which other models from different architectural families were found to have statistically insignificant performance differences, and what does this suggest about their robustness?",
              "answer": "Not enough information in summary"
            },
            "result": false,
            "issue": "The summary does not list other model families that showed statistically insignificant performance differences, only mentioning the Llama family and models with lower baseline accuracy in general."
          },
          {
            "qa": {
              "question": "What specific reasons does the article suggest for why models from the Qwen family failed on rephrased questions, and how does this illustrate a lack of robustness compared to more stable models?",
              "answer": "Not enough information in summary"
            },
            "result": false,
            "issue": "The summary does not provide specific reasons for the Qwen family's failures on rephrased questions. It only generally states that models with higher baseline accuracy and larger models tend to be more sensitive to rephrasing."
          },
          {
            "qa": {
              "question": "ACU. Which LLM family is noted for showing insignificant performance degradation, suggesting greater robustness?",
              "answer": "The Llama family of models demonstrated greater robustness, showing less performance degradation compared to families like Gemma and Qwen."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "ACU. What is the name of the benchmark dataset used to compare the performance of the different LLM families?",
              "answer": "The experiments were conducted on the MMLU benchmark."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "ACU. Out of the 26 LLMs evaluated, how many showed statistically significant performance differences when tested on rephrased prompts?",
              "answer": "20 models showed statistically significant performance drops under modest perturbations."
            },
            "result": true,
            "issue": ""
          }
        ],
        "judgment": false,
        "sections_to_highlight": [
          "Results — Model size vs. performance degradation (specific families)",
          "Results — Models with insignificant performance differences (beyond Llama)",
          "Discussion — Reasons for Qwen family failures",
          "Results — Accuracy vs. performance degradation",
          "Methodology — C-BOD framework",
          "Introduction — Problem of LLM evaluation",
          "Results — LLM family robustness comparison (Gemma/Qwen vs. Llama)"
        ]
      },
      "correct_count_all": 5,
      "correct_count_acu": 3,
      "num_of_questions": 5,
      "sections_to_highlight": [
        "Limitations — Scope of C-BOD",
        "Discussion — LLM overfitting reasons",
        "Results — Model robustness comparison",
        "Results — Model robustness comparison (Llama vs. Gemma/Qwen)",
        "Results — Model size vs. degradation (specific families)",
        "Results — Model size vs. degradation",
        "Discussion — Reasons for Qwen family failures",
        "Results — Model size vs. performance degradation",
        "Limitations — C-BOD scope and cost",
        "Methodology — C-BOD framework",
        "Results — Robustness comparison (Llama vs. other families)",
        "Results — Models with insignificant performance differences",
        "Results — Accuracy vs. performance degradation",
        "Discussion — Implications for LLM evaluation",
        "Results — Other models with insignificant performance differences",
        "Introduction — Problem of LLM evaluation",
        "Results — Accuracy vs. degradation",
        "Results — LLM family robustness comparison"
      ],
      "sections_to_highlight_size": 18
    },
    {
      "iteration_number": 11,
      "summary": "1. SUMMARY:\nThe research introduces the Chameleon Benchmark Overfit Detector (C-BOD) to identify LLMs that over-rely on surface cues in benchmark prompts, rather than demonstrating true language understanding. By systematically rephrasing benchmark questions while preserving semantic content, C-BOD measures performance degradation. Evaluations on the MMLU benchmark with 26 LLMs revealed that most models exhibit significant performance drops under these perturbations, averaging 2.15%. Notably, larger LLMs and those with higher baseline accuracy showed greater degradation, suggesting a tendency to overfit. In contrast, the Llama family and models with lower baseline accuracy demonstrated more robustness, indicating reduced dependency on superficial patterns. The study highlights that high benchmark scores may mask underlying vulnerabilities, urging a shift in evaluation focus towards resilience and generalization. The C-BOD framework is dataset- and model-agnostic, offering a method to promote more robust language understanding.\n\n2. KEY HIGHLIGHTS:\n*   LLMs often overfit to benchmark-specific surface cues, leading to performance degradation when prompts are rephrased.\n*   Larger LLMs and those with higher baseline accuracy tend to be more sensitive to prompt perturbations, indicating a greater reliance on fixed patterns.\n*   The Llama family of models, along with those with lower baseline accuracy, showed greater robustness and less degradation, suggesting reduced dependency on superficial cues.\n*   The study challenges current LLM evaluation practices, emphasizing the need to prioritize resilience and generalization over leaderboard scores.",
      "qa_pairs": [
        {
          "question": "How did the robustness of the Llama model family compare to other architectural families like Gemma and Qwen when subjected to the C-BOD framework's textual perturbations?",
          "answer": "Not enough information in summary"
        },
        {
          "question": "The article states that larger models tend to exhibit greater performance degradation. How does this finding apply to the differential robustness observed across specific architectural families like Gemma and Qwen, as shown in the results?",
          "answer": "Not enough information in summary"
        },
        {
          "question": "According to the findings, what is the relationship between a model family's baseline accuracy on the original MMLU benchmark and its robustness against prompt rephrasing?",
          "answer": "Models with higher baseline accuracy showed greater degradation, suggesting a tendency to overfit. In contrast, the Llama family and models with lower baseline accuracy demonstrated more robustness, indicating reduced dependency on superficial patterns."
        },
        {
          "question": "Besides the Llama family, which other models from different architectural families were found to have statistically insignificant performance differences, and what does this suggest about their robustness?",
          "answer": "Not enough information in summary"
        },
        {
          "question": "What specific reasons does the article suggest for why models from the Qwen family failed on rephrased questions, and how does this illustrate a lack of robustness compared to more stable models?",
          "answer": "Not enough information in summary"
        },
        {
          "question": "ACU. Which LLM family is noted for showing insignificant performance degradation, suggesting greater robustness?",
          "answer": "The Llama family of models is noted for showing insignificant performance degradation, suggesting greater robustness."
        },
        {
          "question": "ACU. What is the name of the benchmark dataset used to compare the performance of the different LLM families?",
          "answer": "The benchmark dataset used to compare the performance of the different LLM families is MMLU."
        },
        {
          "question": "ACU. Out of the 26 LLMs evaluated, how many showed statistically significant performance differences when tested on rephrased prompts?",
          "answer": "Most models exhibit significant performance drops under these perturbations, averaging 2.15%."
        }
      ],
      "judge": {
        "evaluations": [
          {
            "qa": {
              "question": "How did the robustness of the Llama model family compare to other architectural families like Gemma and Qwen when subjected to the C-BOD framework's textual perturbations?",
              "answer": "Not enough information in summary"
            },
            "result": false,
            "issue": "The summary does not contain specific comparative data between Llama, Gemma, and Qwen families regarding robustness. It only mentions Llama's robustness in contrast to models with lower baseline accuracy."
          },
          {
            "qa": {
              "question": "The article states that larger models tend to exhibit greater performance degradation. How does this finding apply to the differential robustness observed across specific architectural families like Gemma and Qwen, as shown in the results?",
              "answer": "Not enough information in summary"
            },
            "result": false,
            "issue": "The summary mentions that larger models and those with higher baseline accuracy showed greater degradation, but it does not provide specific details on how this applies to the differential robustness of Gemma and Qwen families."
          },
          {
            "qa": {
              "question": "According to the findings, what is the relationship between a model family's baseline accuracy on the original MMLU benchmark and its robustness against prompt rephrasing?",
              "answer": "Models with higher baseline accuracy showed greater degradation, suggesting a tendency to overfit. In contrast, the Llama family and models with lower baseline accuracy demonstrated more robustness, indicating reduced dependency on superficial patterns."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "Besides the Llama family, which other models from different architectural families were found to have statistically insignificant performance differences, and what does this suggest about their robustness?",
              "answer": "Not enough information in summary"
            },
            "result": false,
            "issue": "The summary mentions that models with lower baseline accuracy showed insignificant degradation, but it does not list specific models from other families besides Llama that exhibited this characteristic."
          },
          {
            "qa": {
              "question": "What specific reasons does the article suggest for why models from the Qwen family failed on rephrased questions, and how does this illustrate a lack of robustness compared to more stable models?",
              "answer": "Not enough information in summary"
            },
            "result": false,
            "issue": "The summary does not provide specific reasons for Qwen family failures on rephrased questions. It only mentions that models with lower baseline accuracy, including some Qwen models, showed insignificant degradation."
          },
          {
            "qa": {
              "question": "ACU. Which LLM family is noted for showing insignificant performance degradation, suggesting greater robustness?",
              "answer": "The Llama family of models is noted for showing insignificant performance degradation, suggesting greater robustness."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "ACU. What is the name of the benchmark dataset used to compare the performance of the different LLM families?",
              "answer": "The benchmark dataset used to compare the performance of the different LLM families is MMLU."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "ACU. Out of the 26 LLMs evaluated, how many showed statistically significant performance differences when tested on rephrased prompts?",
              "answer": "Most models exhibit significant performance drops under these perturbations, averaging 2.15%."
            },
            "result": false,
            "issue": "The answer is incomplete. The summary states that 20 out of 26 models exhibited statistically significant differences, not just that 'most models' did."
          }
        ],
        "judgment": false,
        "sections_to_highlight": [
          "Results — Model robustness comparison (Llama vs. other families)",
          "Results — Accuracy vs. performance degradation",
          "Results — Models with insignificant performance differences",
          "Introduction — Problem of LLM evaluation",
          "Discussion — LLM overfitting reasons",
          "Results — Model size vs. degradation"
        ]
      },
      "correct_count_all": 3,
      "correct_count_acu": 2,
      "num_of_questions": 5,
      "sections_to_highlight": [
        "Results — Model size vs. degradation (specific families)",
        "Discussion — Implications for LLM evaluation",
        "Results — Model size vs. degradation",
        "Limitations — Scope of C-BOD",
        "Results — Model robustness comparison",
        "Results — Models with insignificant performance differences",
        "Discussion — LLM overfitting reasons",
        "Results — Other models with insignificant performance differences",
        "Discussion — Reasons for Qwen family failures",
        "Limitations — C-BOD scope and cost",
        "Results — Models with insignificant performance differences (beyond Llama)",
        "Results — Model robustness comparison (Llama vs. Gemma/Qwen)",
        "Methodology — C-BOD framework",
        "Results — Model size vs. performance degradation (specific families)",
        "Results — Model size vs. performance degradation",
        "Results — Robustness comparison (Llama vs. other families)",
        "Results — LLM family robustness comparison (Gemma/Qwen vs. Llama)",
        "Results — Accuracy vs. performance degradation",
        "Introduction — Problem of LLM evaluation",
        "Results — Accuracy vs. degradation",
        "Results — LLM family robustness comparison"
      ],
      "sections_to_highlight_size": 21
    },
    {
      "iteration_number": 12,
      "summary": "1. SUMMARY:\nThis paper introduces the Chameleon Benchmark Overfit Detector (C-BOD) to assess Large Language Model (LLM) robustness by systematically rephrasing benchmark prompts while preserving semantic content. Evaluating 26 LLMs on MMLU, C-BOD revealed that most models exhibit significant performance degradation (average 2.15%) under these perturbations, indicating an overreliance on surface-level cues rather than true language understanding. Notably, larger LLMs and those with higher baseline accuracy showed greater sensitivity to rephrasing, suggesting they are more prone to overfitting. In contrast, the Llama family of models, along with some models with lower baseline accuracy, demonstrated significantly less performance degradation, indicating a reduced dependency on superficial patterns. This research challenges current LLM evaluation practices, emphasizing the need to prioritize resilience and generalization over leaderboard scores. The findings highlight that architectural choices and training strategies, as seen in the Llama family, can lead to more robust models.\n\n2. KEY HIGHLIGHTS:\n*   LLMs often overfit to benchmark-specific surface cues, leading to performance drops when prompts are rephrased, even while preserving semantic meaning.\n*   Larger LLMs and those with higher baseline accuracy tend to be more sensitive to prompt rephrasing, indicating a greater reliance on memorized patterns.\n*   The Llama family of models exhibits significantly lower performance degradation under prompt perturbations, suggesting greater robustness compared to other architectural families.\n*   Models with lower baseline accuracy generally show less performance degradation, indicating a weaker dependency on superficial cues.\n*   The study highlights a paradox where high benchmark scores may mask an LLM's vulnerability to prompt variations, urging a shift in evaluation focus towards resilience and generalization.",
      "qa_pairs": [
        {
          "question": "How did the robustness of the Llama model family compare to other architectural families like Gemma and Qwen when subjected to the C-BOD framework's textual perturbations?",
          "answer": "The Llama family of models exhibits significantly lower performance degradation under prompt perturbations, suggesting greater robustness compared to other architectural families. The summary does not provide a comparison with Gemma and Qwen specifically."
        },
        {
          "question": "The article states that larger models tend to exhibit greater performance degradation. How does this finding apply to the differential robustness observed across specific architectural families like Gemma and Qwen, as shown in the results?",
          "answer": "Larger models tend to exhibit greater performance degradation. The summary does not provide specific details on how this applies to Gemma and Qwen."
        },
        {
          "question": "According to the findings, what is the relationship between a model family's baseline accuracy on the original MMLU benchmark and its robustness against prompt rephrasing?",
          "answer": "Models with higher baseline accuracy showed greater sensitivity to rephrasing, indicating they are more prone to overfitting. Models with lower baseline accuracy generally show less performance degradation, indicating a weaker dependency on superficial cues."
        },
        {
          "question": "Besides the Llama family, which other models from different architectural families were found to have statistically insignificant performance differences, and what does this suggest about their robustness?",
          "answer": "Not enough information in summary"
        },
        {
          "question": "What specific reasons does the article suggest for why models from the Qwen family failed on rephrased questions, and how does this illustrate a lack of robustness compared to more stable models?",
          "answer": "Not enough information in summary"
        },
        {
          "question": "ACU. Which LLM family is noted for showing insignificant performance degradation, suggesting greater robustness?",
          "answer": "The Llama family of models is noted for showing insignificant performance degradation, suggesting greater robustness."
        },
        {
          "question": "ACU. What is the name of the benchmark dataset used to compare the performance of the different LLM families?",
          "answer": "MMLU"
        },
        {
          "question": "ACU. Out of the 26 LLMs evaluated, how many showed statistically significant performance differences when tested on rephrased prompts?",
          "answer": "Most models exhibit significant performance degradation (average 2.15%) under these perturbations, indicating an overreliance on surface-level cues rather than true language understanding. The summary does not specify the exact number out of 26 LLMs that showed statistically significant performance differences."
        }
      ],
      "judge": {
        "evaluations": [
          {
            "qa": {
              "question": "How did the robustness of the Llama model family compare to other architectural families like Gemma and Qwen when subjected to the C-BOD framework's textual perturbations?",
              "answer": "The Llama family of models exhibits significantly lower performance degradation under prompt perturbations, suggesting greater robustness compared to other architectural families. The summary does not provide a comparison with Gemma and Qwen specifically."
            },
            "result": false,
            "issue": "The answer correctly states that Llama is more robust but incorrectly claims the summary does not provide a comparison with Gemma and Qwen. The summary explicitly states: 'In contrast, the Llama family of models, along with some models with lower baseline accuracy, demonstrated significantly less performance degradation, indicating a reduced dependency on superficial patterns.' This implies a comparison by stating Llama is *more* robust than others. However, it does not detail the performance of Gemma and Qwen relative to Llama."
          },
          {
            "qa": {
              "question": "The article states that larger models tend to exhibit greater performance degradation. How does this finding apply to the differential robustness observed across specific architectural families like Gemma and Qwen, as shown in the results?",
              "answer": "Larger models tend to exhibit greater performance degradation. The summary does not provide specific details on how this applies to Gemma and Qwen."
            },
            "result": false,
            "issue": "The summary does mention that 'larger LLMs tend to be more sensitive to rephrasings' and that 'models in the Gemma family show a progressive increase in ∆1.0 with higher parameter counts, while Llama models maintain low ∆1.0 values across scales.' This directly answers how the finding applies to Gemma and Llama, implying a differential robustness based on size within families. The summary does not mention Qwen in this context, so that part of the question is not addressed."
          },
          {
            "qa": {
              "question": "According to the findings, what is the relationship between a model family's baseline accuracy on the original MMLU benchmark and its robustness against prompt rephrasing?",
              "answer": "Models with higher baseline accuracy showed greater sensitivity to rephrasing, indicating they are more prone to overfitting. Models with lower baseline accuracy generally show less performance degradation, indicating a weaker dependency on superficial cues."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "Besides the Llama family, which other models from different architectural families were found to have statistically insignificant performance differences, and what does this suggest about their robustness?",
              "answer": "Not enough information in summary"
            },
            "result": false,
            "issue": "The summary states 'We also observed that Falcon 7B, DeepSeek 7B, Qwen 2.5 3B and Jetmoe 8B show statistically insignificant differences, likely due to their lower baseline accuracy.' This directly answers the question."
          },
          {
            "qa": {
              "question": "What specific reasons does the article suggest for why models from the Qwen family failed on rephrased questions, and how does this illustrate a lack of robustness compared to more stable models?",
              "answer": "Not enough information in summary"
            },
            "result": false,
            "issue": "The article provides specific examples of Qwen failures in Table 3, explaining the reasons for the performance drop due to rephrasing. The summary does not include these details."
          },
          {
            "qa": {
              "question": "ACU. Which LLM family is noted for showing insignificant performance degradation, suggesting greater robustness?",
              "answer": "The Llama family of models is noted for showing insignificant performance degradation, suggesting greater robustness."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "ACU. What is the name of the benchmark dataset used to compare the performance of the different LLM families?",
              "answer": "MMLU"
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "ACU. Out of the 26 LLMs evaluated, how many showed statistically significant performance differences when tested on rephrased prompts?",
              "answer": "Most models exhibit significant performance degradation (average 2.15%) under these perturbations, indicating an overreliance on surface-level cues rather than true language understanding. The summary does not specify the exact number out of 26 LLMs that showed statistically significant performance differences."
            },
            "result": false,
            "issue": "The summary states 'with 20 out of 26 models exhibiting statistically significant differences.' This directly answers the question."
          }
        ],
        "judgment": false,
        "sections_to_highlight": [
          "Results — Llama family robustness vs. others",
          "Results — Model size vs. degradation by family",
          "Discussion — LLM evaluation challenges",
          "Results — Statistically insignificant models",
          "Discussion — Reasons for model failures",
          "Results — Accuracy vs. robustness correlation",
          "Methodology — C-BOD framework details"
        ]
      },
      "correct_count_all": 3,
      "correct_count_acu": 2,
      "num_of_questions": 5,
      "sections_to_highlight": [
        "Results — Model robustness comparison (Llama vs. other families)",
        "Results — Model size vs. degradation (specific families)",
        "Discussion — Implications for LLM evaluation",
        "Results — Model size vs. degradation",
        "Limitations — Scope of C-BOD",
        "Results — Model robustness comparison",
        "Results — Models with insignificant performance differences",
        "Discussion — LLM overfitting reasons",
        "Results — Other models with insignificant performance differences",
        "Discussion — Reasons for Qwen family failures",
        "Limitations — C-BOD scope and cost",
        "Results — Models with insignificant performance differences (beyond Llama)",
        "Results — Model robustness comparison (Llama vs. Gemma/Qwen)",
        "Methodology — C-BOD framework",
        "Results — Model size vs. performance degradation (specific families)",
        "Results — Model size vs. performance degradation",
        "Results — Robustness comparison (Llama vs. other families)",
        "Results — LLM family robustness comparison (Gemma/Qwen vs. Llama)",
        "Results — Accuracy vs. performance degradation",
        "Introduction — Problem of LLM evaluation",
        "Results — Accuracy vs. degradation",
        "Results — LLM family robustness comparison"
      ],
      "sections_to_highlight_size": 22
    },
    {
      "iteration_number": 13,
      "summary": "1. SUMMARY:\nThis paper introduces the Chameleon Benchmark Overfit Detector (C-BOD), a meta-evaluation framework designed to identify LLM overreliance on surface-level cues in benchmark prompts. By systematically rephrasing benchmark questions while preserving semantic meaning, C-BOD exposes models that perform well due to memorized patterns rather than true understanding. The study evaluated 26 LLMs on the MMLU benchmark, revealing that most models experience a performance degradation of around 2.15% under modest rephrasing, with 20 out of 26 showing statistically significant drops. Notably, larger models and those with higher baseline accuracy tended to be more susceptible to these perturbations, indicating a potential over-reliance on fixed prompt structures. In contrast, the Llama family and models with lower baseline accuracy demonstrated greater robustness, suggesting a reduced dependency on superficial cues. These findings challenge current LLM evaluation practices, emphasizing the need to prioritize resilience and generalization over leaderboard scores.\n\n2. KEY HIGHLIGHTS:\n*   The Llama family of LLMs demonstrated significantly higher robustness against prompt rephrasing compared to other architectural families like Gemma and Qwen, showing minimal performance degradation.\n*   A strong positive correlation was observed between a model's baseline accuracy on original prompts and its performance degradation when subjected to rephrased inputs, suggesting high scores may mask over-reliance on specific cues.\n*   Larger LLMs generally exhibited greater performance degradation under textual perturbations, indicating that increased model size can correlate with increased sensitivity to prompt variations, although this trend varies by family.\n*   The study highlights significant over-reliance on surface-level features in most evaluated LLMs, with an average accuracy drop of 2.15% on rephrased prompts, underscoring challenges in current LLM evaluation methods that prioritize benchmark scores over genuine generalization.",
      "qa_pairs": [
        {
          "question": "How did the robustness of the Llama model family compare to other architectural families like Gemma and Qwen when subjected to the C-BOD framework's textual perturbations?",
          "answer": "The Llama family of LLMs demonstrated significantly higher robustness against prompt rephrasing compared to other architectural families like Gemma and Qwen, showing minimal performance degradation."
        },
        {
          "question": "The article states that larger models tend to exhibit greater performance degradation. How does this finding apply to the differential robustness observed across specific architectural families like Gemma and Qwen, as shown in the results?",
          "answer": "Larger LLMs generally exhibited greater performance degradation under textual perturbations, indicating that increased model size can correlate with increased sensitivity to prompt variations, although this trend varies by family."
        },
        {
          "question": "According to the findings, what is the relationship between a model family's baseline accuracy on the original MMLU benchmark and its robustness against prompt rephrasing?",
          "answer": "A strong positive correlation was observed between a model's baseline accuracy on original prompts and its performance degradation when subjected to rephrased inputs, suggesting high scores may mask over-reliance on specific cues."
        },
        {
          "question": "Besides the Llama family, which other models from different architectural families were found to have statistically insignificant performance differences, and what does this suggest about their robustness?",
          "answer": "Not enough information in summary"
        },
        {
          "question": "What specific reasons does the article suggest for why models from the Qwen family failed on rephrased questions, and how does this illustrate a lack of robustness compared to more stable models?",
          "answer": "Not enough information in summary"
        },
        {
          "question": "ACU. Which LLM family is noted for showing insignificant performance degradation, suggesting greater robustness?",
          "answer": "The Llama family of LLMs demonstrated significantly higher robustness against prompt rephrasing compared to other architectural families like Gemma and Qwen, showing minimal performance degradation."
        },
        {
          "question": "ACU. What is the name of the benchmark dataset used to compare the performance of the different LLM families?",
          "answer": "The MMLU benchmark dataset was used to compare the performance of the different LLM families."
        },
        {
          "question": "ACU. Out of the 26 LLMs evaluated, how many showed statistically significant performance differences when tested on rephrased prompts?",
          "answer": "20 out of 26 LLMs showed statistically significant performance drops when subjected to modest rephrasing."
        }
      ],
      "judge": {
        "evaluations": [
          {
            "qa": {
              "question": "How did the robustness of the Llama model family compare to other architectural families like Gemma and Qwen when subjected to the C-BOD framework's textual perturbations?",
              "answer": "The Llama family of LLMs demonstrated significantly higher robustness against prompt rephrasing compared to other architectural families like Gemma and Qwen, showing minimal performance degradation."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "The article states that larger models tend to exhibit greater performance degradation. How does this finding apply to the differential robustness observed across specific architectural families like Gemma and Qwen, as shown in the results?",
              "answer": "Larger LLMs generally exhibited greater performance degradation under textual perturbations, indicating that increased model size can correlate with increased sensitivity to prompt variations, although this trend varies by family."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "According to the findings, what is the relationship between a model family's baseline accuracy on the original MMLU benchmark and its robustness against prompt rephrasing?",
              "answer": "A strong positive correlation was observed between a model's baseline accuracy on original prompts and its performance degradation when subjected to rephrased inputs, suggesting high scores may mask over-reliance on specific cues."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "Besides the Llama family, which other models from different architectural families were found to have statistically insignificant performance differences, and what does this suggest about their robustness?",
              "answer": "Not enough information in summary"
            },
            "result": false,
            "issue": "The summary does not list other models with statistically insignificant performance differences. The article mentions Falcon 7B, DeepSeek 7B, Qwen 2.5 3B, and Jetmoe 8B in this context."
          },
          {
            "qa": {
              "question": "What specific reasons does the article suggest for why models from the Qwen family failed on rephrased questions, and how does this illustrate a lack of robustness compared to more stable models?",
              "answer": "Not enough information in summary"
            },
            "result": false,
            "issue": "The summary does not provide specific reasons for Qwen family failures. Table 3 in the article offers examples of Qwen2.5 (32B) failures and their explanations."
          },
          {
            "qa": {
              "question": "ACU. Which LLM family is noted for showing insignificant performance degradation, suggesting greater robustness?",
              "answer": "The Llama family of LLMs demonstrated significantly higher robustness against prompt rephrasing compared to other architectural families like Gemma and Qwen, showing minimal performance degradation."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "ACU. What is the name of the benchmark dataset used to compare the performance of the different LLM families?",
              "answer": "The MMLU benchmark dataset was used to compare the performance of the different LLM families."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "ACU. Out of the 26 LLMs evaluated, how many showed statistically significant performance differences when tested on rephrased prompts?",
              "answer": "20 out of 26 LLMs showed statistically significant performance drops when subjected to modest rephrasing."
            },
            "result": true,
            "issue": ""
          }
        ],
        "judgment": false,
        "sections_to_highlight": [
          "Results — Comparison of Llama family robustness",
          "Results — Models with insignificant performance differences",
          "Discussion — Reasons for model failures (Qwen family)",
          "Results — Model size vs. performance degradation by family",
          "Results — Accuracy vs. performance degradation correlation",
          "Methodology — C-BOD framework details",
          "Discussion — LLM evaluation challenges"
        ]
      },
      "correct_count_all": 6,
      "correct_count_acu": 3,
      "num_of_questions": 5,
      "sections_to_highlight": [
        "Results — Model robustness comparison (Llama vs. other families)",
        "Results — Accuracy vs. robustness correlation",
        "Results — Model size vs. degradation (specific families)",
        "Discussion — Implications for LLM evaluation",
        "Results — Model size vs. degradation",
        "Limitations — Scope of C-BOD",
        "Results — Model robustness comparison",
        "Methodology — C-BOD framework details",
        "Results — Models with insignificant performance differences",
        "Discussion — LLM overfitting reasons",
        "Results — Other models with insignificant performance differences",
        "Discussion — Reasons for Qwen family failures",
        "Limitations — C-BOD scope and cost",
        "Results — Models with insignificant performance differences (beyond Llama)",
        "Results — Model robustness comparison (Llama vs. Gemma/Qwen)",
        "Methodology — C-BOD framework",
        "Results — Statistically insignificant models",
        "Results — Model size vs. performance degradation (specific families)",
        "Results — Model size vs. performance degradation",
        "Results — Model size vs. degradation by family",
        "Discussion — LLM evaluation challenges",
        "Discussion — Reasons for model failures",
        "Results — Robustness comparison (Llama vs. other families)",
        "Results — LLM family robustness comparison (Gemma/Qwen vs. Llama)",
        "Results — Llama family robustness vs. others",
        "Results — Accuracy vs. performance degradation",
        "Introduction — Problem of LLM evaluation",
        "Results — Accuracy vs. degradation",
        "Results — LLM family robustness comparison"
      ],
      "sections_to_highlight_size": 29
    },
    {
      "iteration_number": 14,
      "summary": "1. SUMMARY:\nThis paper introduces the Chameleon Benchmark Overfit Detector (C-BOD), a meta-evaluation framework designed to uncover LLM overreliance on surface-level cues in benchmarks. By systematically rephrasing prompts while preserving semantic content, C-BOD quantifies performance degradation under these perturbations. The study evaluated 26 LLMs on the MMLU benchmark, revealing that most models exhibit statistically significant performance drops, averaging 2.15%, indicating widespread overfitting. Notably, larger LLMs and those with higher baseline accuracy tend to be more susceptible to these rephrasings, suggesting they capitalize on fixed prompt patterns. In contrast, the Llama family of models and those with lower baseline accuracy demonstrated significantly less degradation, implying a reduced dependency on superficial cues. The findings challenge the reliance on leaderboard scores, advocating for evaluations that prioritize robustness and generalization, and highlight that architectural choices and training strategies, like those employed by the Llama family, can lead to more resilient language understanding.\n\n2. KEY HIGHLIGHTS:\n*   LLMs often overfit to benchmark-specific surface cues, leading to performance degradation when prompts are rephrased.\n*   The Llama family of models showed significantly less performance degradation under prompt perturbations compared to other families, indicating greater robustness.\n*   Models with higher baseline accuracy and larger parameter counts generally exhibited greater performance drops, suggesting a reliance on memorized patterns.\n*   The C-BOD framework provides a dataset-agnostic method to assess LLM resilience, highlighting the need to look beyond simple benchmark scores.\n*   The study indicates that architectural choices and training strategies can lead to more robust LLMs, as exemplified by the Llama family's performance.",
      "qa_pairs": [
        {
          "question": "How did the robustness of the Llama model family compare to other architectural families like Gemma and Qwen when subjected to the C-BOD framework's textual perturbations?",
          "answer": "The Llama family of models showed significantly less performance degradation under prompt perturbations compared to other families, indicating greater robustness. The summary does not contain specific comparisons to Gemma and Qwen."
        },
        {
          "question": "The article states that larger models tend to exhibit greater performance degradation. How does this finding apply to the differential robustness observed across specific architectural families like Gemma and Qwen, as shown in the results?",
          "answer": "The summary states that larger models tend to exhibit greater performance degradation. It also mentions that the Llama family and models with lower baseline accuracy demonstrated significantly less degradation. However, it does not provide specific results or comparisons for Gemma and Qwen in relation to their size and robustness."
        },
        {
          "question": "According to the findings, what is the relationship between a model family's baseline accuracy on the original MMLU benchmark and its robustness against prompt rephrasing?",
          "answer": "Models with higher baseline accuracy on the original MMLU benchmark tend to be more susceptible to rephrasings, suggesting they capitalize on fixed prompt patterns. In contrast, models with lower baseline accuracy demonstrated significantly less degradation, implying a reduced dependency on superficial cues."
        },
        {
          "question": "Besides the Llama family, which other models from different architectural families were found to have statistically insignificant performance differences, and what does this suggest about their robustness?",
          "answer": "Not enough information in summary"
        },
        {
          "question": "What specific reasons does the article suggest for why models from the Qwen family failed on rephrased questions, and how does this illustrate a lack of robustness compared to more stable models?",
          "answer": "Not enough information in summary"
        },
        {
          "question": "ACU. Which LLM family is noted for showing insignificant performance degradation, suggesting greater robustness?",
          "answer": "The Llama family of models is noted for showing insignificant performance degradation, suggesting greater robustness."
        },
        {
          "question": "ACU. What is the name of the benchmark dataset used to compare the performance of the different LLM families?",
          "answer": "The MMLU benchmark dataset was used to compare the performance of the different LLM families."
        },
        {
          "question": "ACU. Out of the 26 LLMs evaluated, how many showed statistically significant performance differences when tested on rephrased prompts?",
          "answer": "Most of the 26 LLMs evaluated exhibited statistically significant performance drops when tested on rephrased prompts."
        }
      ],
      "judge": {
        "evaluations": [
          {
            "qa": {
              "question": "How did the robustness of the Llama model family compare to other architectural families like Gemma and Qwen when subjected to the C-BOD framework's textual perturbations?",
              "answer": "The Llama family of models showed significantly less performance degradation under prompt perturbations compared to other families, indicating greater robustness. The summary does not contain specific comparisons to Gemma and Qwen."
            },
            "result": false,
            "issue": "The provided answer states that the summary does not contain specific comparisons to Gemma and Qwen, but the article's 'Results — Model robustness comparison (Llama vs. other families)' and 'Results — Model robustness comparison (Llama vs. Gemma/Qwen)' sections (and similar ones) do contain such comparisons. The answer should reflect this."
          },
          {
            "qa": {
              "question": "The article states that larger models tend to exhibit greater performance degradation. How does this finding apply to the differential robustness observed across specific architectural families like Gemma and Qwen, as shown in the results?",
              "answer": "The summary states that larger models tend to exhibit greater performance degradation. It also mentions that the Llama family and models with lower baseline accuracy demonstrated significantly less degradation. However, it does not provide specific results or comparisons for Gemma and Qwen in relation to their size and robustness."
            },
            "result": false,
            "issue": "The answer claims the summary does not provide specific results for Gemma and Qwen regarding size and robustness, but the article's 'Results — Model size vs. degradation (specific families)' and 'Results — Model size vs. performance degradation by family' sections do contain this information. The answer should be more specific."
          },
          {
            "qa": {
              "question": "According to the findings, what is the relationship between a model family's baseline accuracy on the original MMLU benchmark and its robustness against prompt rephrasing?",
              "answer": "Models with higher baseline accuracy on the original MMLU benchmark tend to be more susceptible to rephrasings, suggesting they capitalize on fixed prompt patterns. In contrast, models with lower baseline accuracy demonstrated significantly less degradation, implying a reduced dependency on superficial cues."
            },
            "result": true,
            "issue": null
          },
          {
            "qa": {
              "question": "Besides the Llama family, which other models from different architectural families were found to have statistically insignificant performance differences, and what does this suggest about their robustness?",
              "answer": "Not enough information in summary"
            },
            "result": false,
            "issue": "The summary does mention other models with statistically insignificant performance differences: 'Falcon 7B, DeepSeek 7B, Qwen 2.5 3B and Jetmoe 8B show statistically insignificant differences'. The answer should include these examples."
          },
          {
            "qa": {
              "question": "What specific reasons does the article suggest for why models from the Qwen family failed on rephrased questions, and how does this illustrate a lack of robustness compared to more stable models?",
              "answer": "Not enough information in summary"
            },
            "result": false,
            "issue": "The article's 'Table 3: Examples of how rephrasing affects LLM performance...' section provides specific reasons for failures, including examples from Qwen2.5 (32B parameters). The answer should reference these specific examples."
          },
          {
            "qa": {
              "question": "ACU. Which LLM family is noted for showing insignificant performance degradation, suggesting greater robustness?",
              "answer": "The Llama family of models is noted for showing insignificant performance degradation, suggesting greater robustness."
            },
            "result": true,
            "issue": null
          },
          {
            "qa": {
              "question": "ACU. What is the name of the benchmark dataset used to compare the performance of the different LLM families?",
              "answer": "The MMLU benchmark dataset was used to compare the performance of the different LLM families."
            },
            "result": true,
            "issue": null
          },
          {
            "qa": {
              "question": "ACU. Out of the 26 LLMs evaluated, how many showed statistically significant performance differences when tested on rephrased prompts?",
              "answer": "Most of the 26 LLMs evaluated exhibited statistically significant performance drops when tested on rephrased prompts."
            },
            "result": true,
            "issue": null
          }
        ],
        "judgment": false,
        "sections_to_highlight": [
          "Results — Llama family robustness vs. Gemma/Qwen",
          "Results — Model size vs. performance degradation by family",
          "Results — Models with insignificant performance differences (other than Llama)",
          "Discussion — Reasons for model failures (Qwen family examples)",
          "Results — Accuracy vs. robustness correlation",
          "Discussion — Implications for LLM evaluation"
        ]
      },
      "correct_count_all": 4,
      "correct_count_acu": 3,
      "num_of_questions": 5,
      "sections_to_highlight": [
        "Results — Model robustness comparison (Llama vs. other families)",
        "Results — Comparison of Llama family robustness",
        "Results — Accuracy vs. robustness correlation",
        "Results — Model size vs. degradation (specific families)",
        "Discussion — Implications for LLM evaluation",
        "Results — Model size vs. degradation",
        "Limitations — Scope of C-BOD",
        "Results — Accuracy vs. performance degradation correlation",
        "Results — Model robustness comparison",
        "Results — Model size vs. performance degradation by family",
        "Methodology — C-BOD framework details",
        "Results — Models with insignificant performance differences",
        "Discussion — LLM overfitting reasons",
        "Results — Other models with insignificant performance differences",
        "Discussion — Reasons for Qwen family failures",
        "Limitations — C-BOD scope and cost",
        "Results — Models with insignificant performance differences (beyond Llama)",
        "Results — Model robustness comparison (Llama vs. Gemma/Qwen)",
        "Methodology — C-BOD framework",
        "Results — Statistically insignificant models",
        "Results — Model size vs. performance degradation (specific families)",
        "Discussion — Reasons for model failures (Qwen family)",
        "Results — Model size vs. performance degradation",
        "Results — Model size vs. degradation by family",
        "Discussion — LLM evaluation challenges",
        "Discussion — Reasons for model failures",
        "Results — Robustness comparison (Llama vs. other families)",
        "Results — LLM family robustness comparison (Gemma/Qwen vs. Llama)",
        "Results — Llama family robustness vs. others",
        "Results — Accuracy vs. performance degradation",
        "Introduction — Problem of LLM evaluation",
        "Results — Accuracy vs. degradation",
        "Results — LLM family robustness comparison"
      ],
      "sections_to_highlight_size": 33
    },
    {
      "iteration_number": 15,
      "summary": "1. SUMMARY:\nThe study introduces the Chameleon Benchmark Overfit Detector (C-BOD) to assess LLM robustness against prompt rephrasing, revealing that many LLMs over-rely on surface cues rather than true understanding. Evaluating 26 LLMs on the MMLU benchmark, C-BOD found an average 2.15% performance drop under modest rephrasing, with 20 models showing statistically significant degradation. Notably, larger models and those with higher baseline accuracy exhibited greater performance declines, suggesting a reliance on fixed prompt patterns. In contrast, the Llama family and models with lower baseline accuracy demonstrated significantly less degradation, indicating a reduced dependency on superficial cues. This highlights a critical gap in current LLM evaluation, where high benchmark scores may mask underlying vulnerabilities to prompt variations. The findings advocate for prioritizing resilience and generalization over leaderboard performance.\n\n2. KEY HIGHLIGHTS:\n* The Llama family of LLMs showed significantly less performance degradation under prompt rephrasing compared to other families like Gemma and Qwen, indicating greater robustness.\n* Models with higher baseline accuracy on original prompts were found to experience larger performance drops when prompts were rephrased, suggesting that high scores may be driven by superficial cues.\n* Larger LLMs generally exhibited greater performance degradation when prompts were modified, implying that increased model size can lead to increased sensitivity to prompt variations.\n* The study identified specific LLM families and models (e.g., Llama, and models with lower baseline accuracy) that were more robust, showing statistically insignificant performance differences under rephrasing.\n* The findings challenge current LLM evaluation practices, emphasizing that high benchmark scores might not reflect true language understanding or generalization capabilities.",
      "qa_pairs": [
        {
          "question": "How did the robustness of the Llama model family compare to other architectural families like Gemma and Qwen when subjected to the C-BOD framework's textual perturbations?",
          "answer": "The Llama family of LLMs showed significantly less performance degradation under prompt rephrasing compared to other families like Gemma and Qwen, indicating greater robustness."
        },
        {
          "question": "The article states that larger models tend to exhibit greater performance degradation. How does this finding apply to the differential robustness observed across specific architectural families like Gemma and Qwen, as shown in the results?",
          "answer": "Larger LLMs generally exhibited greater performance degradation when prompts were modified, implying that increased model size can lead to increased sensitivity to prompt variations. The summary does not provide specific details on how this applies to the differential robustness across Gemma and Qwen families."
        },
        {
          "question": "According to the findings, what is the relationship between a model family's baseline accuracy on the original MMLU benchmark and its robustness against prompt rephrasing?",
          "answer": "Models with higher baseline accuracy on original prompts were found to experience larger performance drops when prompts were rephrased, suggesting that high scores may be driven by superficial cues. Conversely, models with lower baseline accuracy demonstrated significantly less degradation, indicating a reduced dependency on superficial cues."
        },
        {
          "question": "Besides the Llama family, which other models from different architectural families were found to have statistically insignificant performance differences, and what does this suggest about their robustness?",
          "answer": "The summary mentions that models with lower baseline accuracy were more robust, showing statistically insignificant performance differences under rephrasing, but does not name specific families other than Llama."
        },
        {
          "question": "What specific reasons does the article suggest for why models from the Qwen family failed on rephrased questions, and how does this illustrate a lack of robustness compared to more stable models?",
          "answer": "Not enough information in summary"
        },
        {
          "question": "ACU. Which LLM family is noted for showing insignificant performance degradation, suggesting greater robustness?",
          "answer": "The Llama family of LLMs is noted for showing insignificant performance degradation, suggesting greater robustness."
        },
        {
          "question": "ACU. What is the name of the benchmark dataset used to compare the performance of the different LLM families?",
          "answer": "The Chameleon Benchmark Overfit Detector (C-BOD) framework was used to assess LLM robustness against prompt rephrasing, and the MMLU benchmark was used to evaluate 26 LLMs."
        },
        {
          "question": "ACU. Out of the 26 LLMs evaluated, how many showed statistically significant performance differences when tested on rephrased prompts?",
          "answer": "20 models showed statistically significant degradation."
        }
      ],
      "judge": {
        "evaluations": [
          {
            "qa": {
              "question": "How did the robustness of the Llama model family compare to other architectural families like Gemma and Qwen when subjected to the C-BOD framework's textual perturbations?",
              "answer": "The Llama family of LLMs showed significantly less performance degradation under prompt rephrasing compared to other families like Gemma and Qwen, indicating greater robustness."
            },
            "result": true,
            "issue": null
          },
          {
            "qa": {
              "question": "The article states that larger models tend to exhibit greater performance degradation. How does this finding apply to the differential robustness observed across specific architectural families like Gemma and Qwen, as shown in the results?",
              "answer": "Larger LLMs generally exhibited greater performance degradation when prompts were modified, implying that increased model size can lead to increased sensitivity to prompt variations. The summary does not provide specific details on how this applies to the differential robustness across Gemma and Qwen families."
            },
            "result": false,
            "issue": "The summary correctly states that larger models show greater degradation, but it incorrectly claims the summary does not provide details on how this applies to Gemma and Qwen. Figure 4 in the article directly addresses this by showing performance difference vs. model parameters for Gemma and Llama families, illustrating that Gemma models show progressive increase in degradation with higher parameter counts, while Llama models maintain low degradation across scales."
          },
          {
            "qa": {
              "question": "According to the findings, what is the relationship between a model family's baseline accuracy on the original MMLU benchmark and its robustness against prompt rephrasing?",
              "answer": "Models with higher baseline accuracy on original prompts were found to experience larger performance drops when prompts were rephrased, suggesting that high scores may be driven by superficial cues. Conversely, models with lower baseline accuracy demonstrated significantly less degradation, indicating a reduced dependency on superficial cues."
            },
            "result": true,
            "issue": null
          },
          {
            "qa": {
              "question": "Besides the Llama family, which other models from different architectural families were found to have statistically insignificant performance differences, and what does this suggest about their robustness?",
              "answer": "The summary mentions that models with lower baseline accuracy were more robust, showing statistically insignificant performance differences under rephrasing, but does not name specific families other than Llama."
            },
            "result": true,
            "issue": null
          },
          {
            "qa": {
              "question": "What specific reasons does the article suggest for why models from the Qwen family failed on rephrased questions, and how does this illustrate a lack of robustness compared to more stable models?",
              "answer": "Not enough information in summary"
            },
            "result": false,
            "issue": "The article provides specific reasons for Qwen family failures in Table 3. For example, in Legal context, the Qwen model failed because the rephrased question was more open-ended, leading it to discuss plea bargaining instead of the legal verdict, unlike more stable models that might focus on the legal standard."
          },
          {
            "qa": {
              "question": "ACU. Which LLM family is noted for showing insignificant performance degradation, suggesting greater robustness?",
              "answer": "The Llama family of LLMs is noted for showing insignificant performance degradation, suggesting greater robustness."
            },
            "result": true,
            "issue": null
          },
          {
            "qa": {
              "question": "ACU. What is the name of the benchmark dataset used to compare the performance of the different LLM families?",
              "answer": "The Chameleon Benchmark Overfit Detector (C-BOD) framework was used to assess LLM robustness against prompt rephrasing, and the MMLU benchmark was used to evaluate 26 LLMs."
            },
            "result": true,
            "issue": null
          },
          {
            "qa": {
              "question": "ACU. Out of the 26 LLMs evaluated, how many showed statistically significant performance differences when tested on rephrased prompts?",
              "answer": "20 models showed statistically significant degradation."
            },
            "result": true,
            "issue": null
          }
        ],
        "judgment": false,
        "sections_to_highlight": [
          "Results — Model size vs. degradation (specific families)",
          "Discussion — Reasons for Qwen family failures",
          "Results — Accuracy vs. performance degradation correlation",
          "Results — Model robustness comparison (Llama vs. Gemma/Qwen)",
          "Discussion — LLM overfitting reasons",
          "Results — Models with insignificant performance differences (other than Llama)"
        ]
      },
      "correct_count_all": 6,
      "correct_count_acu": 3,
      "num_of_questions": 5,
      "sections_to_highlight": [
        "Results — Model robustness comparison (Llama vs. other families)",
        "Results — Comparison of Llama family robustness",
        "Results — Accuracy vs. robustness correlation",
        "Results — Model size vs. degradation (specific families)",
        "Discussion — Implications for LLM evaluation",
        "Results — Model size vs. degradation",
        "Limitations — Scope of C-BOD",
        "Results — Accuracy vs. performance degradation correlation",
        "Results — Model robustness comparison",
        "Results — Model size vs. performance degradation by family",
        "Results — Llama family robustness vs. Gemma/Qwen",
        "Methodology — C-BOD framework details",
        "Results — Models with insignificant performance differences (other than Llama)",
        "Results — Models with insignificant performance differences",
        "Discussion — LLM overfitting reasons",
        "Results — Other models with insignificant performance differences",
        "Discussion — Reasons for Qwen family failures",
        "Limitations — C-BOD scope and cost",
        "Results — Models with insignificant performance differences (beyond Llama)",
        "Results — Model robustness comparison (Llama vs. Gemma/Qwen)",
        "Discussion — Reasons for model failures (Qwen family examples)",
        "Methodology — C-BOD framework",
        "Results — Statistically insignificant models",
        "Results — Model size vs. performance degradation (specific families)",
        "Discussion — Reasons for model failures (Qwen family)",
        "Results — Model size vs. performance degradation",
        "Results — Model size vs. degradation by family",
        "Discussion — LLM evaluation challenges",
        "Discussion — Reasons for model failures",
        "Results — Robustness comparison (Llama vs. other families)",
        "Results — LLM family robustness comparison (Gemma/Qwen vs. Llama)",
        "Results — Llama family robustness vs. others",
        "Results — Accuracy vs. performance degradation",
        "Introduction — Problem of LLM evaluation",
        "Results — Accuracy vs. degradation",
        "Results — LLM family robustness comparison"
      ],
      "sections_to_highlight_size": 36
    }
  ],
  "final_summary": "1. SUMMARY:\nThe study introduces the Chameleon Benchmark Overfit Detector (C-BOD) to assess LLM robustness against prompt rephrasing, revealing that many LLMs over-rely on surface cues rather than true understanding. Evaluating 26 LLMs on the MMLU benchmark, C-BOD found an average 2.15% performance drop under modest rephrasing, with 20 models showing statistically significant degradation. Notably, larger models and those with higher baseline accuracy exhibited greater performance declines, suggesting a reliance on fixed prompt patterns. In contrast, the Llama family and models with lower baseline accuracy demonstrated significantly less degradation, indicating a reduced dependency on superficial cues. This highlights a critical gap in current LLM evaluation, where high benchmark scores may mask underlying vulnerabilities to prompt variations. The findings advocate for prioritizing resilience and generalization over leaderboard performance.\n\n2. KEY HIGHLIGHTS:\n* The Llama family of LLMs showed significantly less performance degradation under prompt rephrasing compared to other families like Gemma and Qwen, indicating greater robustness.\n* Models with higher baseline accuracy on original prompts were found to experience larger performance drops when prompts were rephrased, suggesting that high scores may be driven by superficial cues.\n* Larger LLMs generally exhibited greater performance degradation when prompts were modified, implying that increased model size can lead to increased sensitivity to prompt variations.\n* The study identified specific LLM families and models (e.g., Llama, and models with lower baseline accuracy) that were more robust, showing statistically insignificant performance differences under rephrasing.\n* The findings challenge current LLM evaluation practices, emphasizing that high benchmark scores might not reflect true language understanding or generalization capabilities.",
  "total_iterations": 15,
  "status": "max_iterations_reached",
  "questions": [
    "How did the robustness of the Llama model family compare to other architectural families like Gemma and Qwen when subjected to the C-BOD framework's textual perturbations?",
    "The article states that larger models tend to exhibit greater performance degradation. How does this finding apply to the differential robustness observed across specific architectural families like Gemma and Qwen, as shown in the results?",
    "According to the findings, what is the relationship between a model family's baseline accuracy on the original MMLU benchmark and its robustness against prompt rephrasing?",
    "Besides the Llama family, which other models from different architectural families were found to have statistically insignificant performance differences, and what does this suggest about their robustness?",
    "What specific reasons does the article suggest for why models from the Qwen family failed on rephrased questions, and how does this illustrate a lack of robustness compared to more stable models?"
  ],
  "acu_questions": [
    "ACU. Which LLM family is noted for showing insignificant performance degradation, suggesting greater robustness?",
    "ACU. What is the name of the benchmark dataset used to compare the performance of the different LLM families?",
    "ACU. Out of the 26 LLMs evaluated, how many showed statistically significant performance differences when tested on rephrased prompts?"
  ]
}