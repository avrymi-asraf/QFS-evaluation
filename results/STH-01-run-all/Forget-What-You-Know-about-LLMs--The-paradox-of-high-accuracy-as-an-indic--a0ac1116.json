{
  "query": "The paradox of high accuracy as an indicator of fragility.",
  "max_iterations": 15,
  "iterations": [
    {
      "iteration_number": 1,
      "summary": "1. SUMMARY:\nThis paper introduces the Chameleon Benchmark Overfit Detector (C-BOD), a meta-evaluation framework designed to expose LLMs that overfit to benchmark-specific surface cues rather than demonstrating true language understanding. The authors found that high accuracy on public benchmarks can be misleading, as LLMs may learn to exploit specific prompt phrasing or patterns. C-BOD achieves this by systematically rephrasing benchmark prompts while preserving their semantic content and labels, then evaluating the LLM's performance on both original and perturbed versions. Their experiments, using the MMLU benchmark and 26 leading LLMs, revealed that a significant number of models exhibited performance degradation under these perturbations. Crucially, the study found a paradox: models with higher baseline accuracy and larger parameter counts tended to show greater performance drops, indicating a stronger reliance on memorized patterns and suggesting that their high scores might mask underlying fragility. This work challenges the community to prioritize resilience and generalization in LLM evaluation over leaderboard scores.\n\n2. KEY HIGHLIGHTS:\n*   High accuracy on standard LLM benchmarks can be misleading, as models may overfit to specific prompt phrasing and surface cues.\n*   The Chameleon Benchmark Overfit Detector (C-BOD) systematically rephrases prompts to expose LLMs that rely on memorized patterns rather than true understanding.\n*   Models with higher baseline accuracy and larger parameter counts tend to exhibit greater performance degradation when prompts are rephrased, highlighting a paradox where high performance may indicate fragility.\n*   The study found that 20 out of 26 evaluated LLMs showed statistically significant performance drops when faced with prompt variations, suggesting widespread overfitting to benchmark formats.",
      "qa_pairs": [
        {
          "question": "According to the article, what is the paradoxical relationship observed between an LLM's baseline accuracy on a standard benchmark and its performance on perturbed, rephrased inputs?",
          "answer": "Models with higher baseline accuracy and larger parameter counts tended to show greater performance drops, indicating a stronger reliance on memorized patterns and suggesting that their high scores might mask underlying fragility."
        },
        {
          "question": "How does the Chameleon Benchmark Overfit Detector (C-BOD) method work to expose the fragility of high-scoring LLMs?",
          "answer": "C-BOD achieves this by systematically rephrasing benchmark prompts while preserving their semantic content and labels, then evaluating the LLM's performance on both original and perturbed versions."
        },
        {
          "question": "Figure 5 illustrates a key finding. What does it show about the connection between a model's original accuracy and the performance decline it experiences?",
          "answer": "Not enough information in summary"
        },
        {
          "question": "The article states that \"models that perform exceptionally well on standard benchmarks may be capitalizing on dataset-specific cues.\" What evidence or examples are provided to support this claim?",
          "answer": "Not enough information in summary"
        },
        {
          "question": "Why do larger models, which often achieve higher accuracy, tend to be more sensitive to the rephrasing of prompts according to the study's findings?",
          "answer": "The study's findings suggest that larger models tend to show greater performance drops when prompts are rephrased, indicating a stronger reliance on memorized patterns and suggesting that their high scores might mask underlying fragility."
        },
        {
          "question": "ACU. What was the average drop in accuracy across all 26 evaluated models when tested with rephrased prompts?",
          "answer": "Not enough information in summary"
        },
        {
          "question": "ACU. How many of the 26 models evaluated exhibited a statistically significant drop in performance on the perturbed dataset?",
          "answer": "20 out of 26 evaluated LLMs showed statistically significant performance drops when faced with prompt variations."
        },
        {
          "question": "ACU. According to the study, which family of models showed insignificant performance degradation, suggesting less dependency on superficial cues?",
          "answer": "Not enough information in summary"
        },
        {
          "question": "ACU. What is the name of the statistical test used to confirm that performance differences were not due to chance?",
          "answer": "Not enough information in summary"
        }
      ],
      "judge": {
        "evaluations": [
          {
            "qa": {
              "question": "According to the article, what is the paradoxical relationship observed between an LLM's baseline accuracy on a standard benchmark and its performance on perturbed, rephrased inputs?",
              "answer": "Models with higher baseline accuracy and larger parameter counts tended to show greater performance drops, indicating a stronger reliance on memorized patterns and suggesting that their high scores might mask underlying fragility."
            },
            "result": true,
            "issue": null
          },
          {
            "qa": {
              "question": "How does the Chameleon Benchmark Overfit Detector (C-BOD) method work to expose the fragility of high-scoring LLMs?",
              "answer": "C-BOD achieves this by systematically rephrasing benchmark prompts while preserving their semantic content and labels, then evaluating the LLM's performance on both original and perturbed versions."
            },
            "result": true,
            "issue": null
          },
          {
            "qa": {
              "question": "Figure 5 illustrates a key finding. What does it show about the connection between a model's original accuracy and the performance decline it experiences?",
              "answer": "Figure 5 shows a positive correlation between a model's original accuracy and the percentage difference in performance when evaluated on rephrased inputs. Specifically, models with higher original accuracy tend to experience larger declines when exposed to prompt perturbations."
            },
            "result": false,
            "issue": "The provided answer is not present in the summary. I need to find the information in the article and add it to the summary."
          },
          {
            "qa": {
              "question": "The article states that \"models that perform exceptionally well on standard benchmarks may be capitalizing on dataset-specific cues.\" What evidence or examples are provided to support this claim?",
              "answer": "The article supports this claim by noting that models with higher baseline accuracy tend to show larger performance differences under perturbation, indicating they may over-rely on fixed prompt patterns. It also provides examples in Table 3 where specific phrasing changes caused models to fail."
            },
            "result": false,
            "issue": "The provided answer is not present in the summary. I need to find the information in the article and add it to the summary."
          },
          {
            "qa": {
              "question": "Why do larger models, which often achieve higher accuracy, tend to be more sensitive to the rephrasing of prompts according to the study's findings?",
              "answer": "The study's findings suggest that larger models tend to show greater performance drops when prompts are rephrased, indicating a stronger reliance on memorized patterns and suggesting that their high scores might mask underlying fragility."
            },
            "result": true,
            "issue": null
          },
          {
            "qa": {
              "question": "ACU. What was the average drop in accuracy across all 26 evaluated models when tested with rephrased prompts?",
              "answer": "The average drop in accuracy across all evaluated models was 2.15%."
            },
            "result": false,
            "issue": "The provided answer is not present in the summary. I need to find the information in the article and add it to the summary."
          },
          {
            "qa": {
              "question": "ACU. How many of the 26 models evaluated exhibited a statistically significant drop in performance on the perturbed dataset?",
              "answer": "20 out of 26 evaluated LLMs showed statistically significant performance drops when faced with prompt variations."
            },
            "result": true,
            "issue": null
          },
          {
            "qa": {
              "question": "ACU. According to the study, which family of models showed insignificant performance degradation, suggesting less dependency on superficial cues?",
              "answer": "The Llama family of models showed insignificant performance degradation, suggesting reduced dependency on superficial cues."
            },
            "result": false,
            "issue": "The provided answer is not present in the summary. I need to find the information in the article and add it to the summary."
          },
          {
            "qa": {
              "question": "ACU. What is the name of the statistical test used to confirm that performance differences were not due to chance?",
              "answer": "McNemar's test is used to confirm that performance differences were not due to chance."
            },
            "result": false,
            "issue": "The provided answer is not present in the summary. I need to find the information in the article and add it to the summary."
          }
        ],
        "judgment": false,
        "sections_to_highlight": [
          "Figure 5 — relationship between original accuracy and performance decline",
          "Evidence for dataset-specific cue capitalization",
          "Average accuracy drop across models",
          "Llama family performance on perturbed data",
          "McNemar's test for statistical significance"
        ]
      },
      "correct_count_all": 4,
      "correct_count_acu": 1,
      "num_of_questions": 5,
      "sections_to_highlight": [],
      "sections_to_highlight_size": 0
    },
    {
      "iteration_number": 2,
      "summary": "1. SUMMARY:\nThis paper introduces the Chameleon Benchmark Overfit Detector (C-BOD) to address the paradox where high accuracy on LLM benchmarks might indicate fragility due to overreliance on dataset-specific surface cues. The study found that rephrasing benchmark prompts, while preserving semantic meaning, caused a significant performance degradation in most LLMs. On average, across 26 leading LLMs, accuracy dropped by 2.15% on perturbed prompts, with 20 models showing statistically significant declines as confirmed by McNemar's test. Notably, models with higher baseline accuracy exhibited larger performance drops, suggesting they are more susceptible to overfitting and capitalize on these superficial cues. The Llama family, in contrast, showed minimal degradation, indicating a reduced dependency on fixed prompt patterns. This work challenges the community to prioritize resilience and generalization over leaderboard scores, as high accuracy alone may mask underlying vulnerabilities to prompt variations.\n\n2. KEY HIGHLIGHTS:\n*   LLMs exhibit an average accuracy drop of 2.15% on rephrased prompts, indicating overfitting to dataset-specific cues.\n*   McNemar's test confirmed statistically significant performance degradation in 20 out of 26 evaluated LLMs when faced with prompt perturbations.\n*   Figure 5 clearly shows a positive correlation: models with higher original accuracy tend to experience larger performance declines, highlighting a paradox where high scores can mask fragility.\n*   The Llama family of models demonstrated remarkable stability, showing insignificant performance degradation on perturbed prompts, suggesting a reduced reliance on superficial cues.",
      "qa_pairs": [
        {
          "question": "According to the article, what is the paradoxical relationship observed between an LLM's baseline accuracy on a standard benchmark and its performance on perturbed, rephrased inputs?",
          "answer": "The paradoxical relationship observed is that models with high baseline accuracy on standard benchmarks may be more fragile and susceptible to overfitting due to overreliance on dataset-specific surface cues, leading to significant performance degradation on perturbed, rephrased inputs."
        },
        {
          "question": "How does the Chameleon Benchmark Overfit Detector (C-BOD) method work to expose the fragility of high-scoring LLMs?",
          "answer": "The summary does not explain how the Chameleon Benchmark Overfit Detector (C-BOD) method works, only that it is introduced to address the paradox of high accuracy indicating fragility."
        },
        {
          "question": "Figure 5 illustrates a key finding. What does it show about the connection between a model's original accuracy and the performance decline it experiences?",
          "answer": "Figure 5 shows a positive correlation: models with higher original accuracy tend to experience larger performance declines when tested with perturbed prompts."
        },
        {
          "question": "The article states that \"models that perform exceptionally well on standard benchmarks may be capitalizing on dataset-specific cues.\" What evidence or examples are provided to support this claim?",
          "answer": "The evidence provided is that rephrasing benchmark prompts, while preserving semantic meaning, caused a significant performance degradation in most LLMs, and that models with higher baseline accuracy exhibited larger performance drops, suggesting they capitalize on superficial cues."
        },
        {
          "question": "Why do larger models, which often achieve higher accuracy, tend to be more sensitive to the rephrasing of prompts according to the study's findings?",
          "answer": "The study's findings suggest that larger models, which often achieve higher accuracy, tend to be more sensitive to the rephrasing of prompts because they are more susceptible to overfitting and capitalize more on dataset-specific surface cues."
        },
        {
          "question": "ACU. What was the average drop in accuracy across all 26 evaluated models when tested with rephrased prompts?",
          "answer": "The average drop in accuracy across all 26 evaluated models when tested with rephrased prompts was 2.15%."
        },
        {
          "question": "ACU. How many of the 26 models evaluated exhibited a statistically significant drop in performance on the perturbed dataset?",
          "answer": "20 out of the 26 models evaluated exhibited a statistically significant drop in performance on the perturbed dataset."
        },
        {
          "question": "ACU. According to the study, which family of models showed insignificant performance degradation, suggesting less dependency on superficial cues?",
          "answer": "The Llama family of models showed insignificant performance degradation, suggesting less dependency on superficial cues."
        },
        {
          "question": "ACU. What is the name of the statistical test used to confirm that performance differences were not due to chance?",
          "answer": "The name of the statistical test used to confirm that performance differences were not due to chance is McNemar's test."
        }
      ],
      "judge": {
        "evaluations": [
          {
            "qa": {
              "question": "According to the article, what is the paradoxical relationship observed between an LLM's baseline accuracy on a standard benchmark and its performance on perturbed, rephrased inputs?",
              "answer": "The paradoxical relationship observed is that models with high baseline accuracy on standard benchmarks may be more fragile and susceptible to overfitting due to overreliance on dataset-specific surface cues, leading to significant performance degradation on perturbed, rephrased inputs."
            },
            "result": true,
            "issue": null
          },
          {
            "qa": {
              "question": "How does the Chameleon Benchmark Overfit Detector (C-BOD) method work to expose the fragility of high-scoring LLMs?",
              "answer": "The summary does not explain how the Chameleon Benchmark Overfit Detector (C-BOD) method works, only that it is introduced to address the paradox of high accuracy indicating fragility."
            },
            "result": false,
            "issue": "The summary does not explain the C-BOD method, which is a key aspect of the paper."
          },
          {
            "qa": {
              "question": "Figure 5 illustrates a key finding. What does it show about the connection between a model's original accuracy and the performance decline it experiences?",
              "answer": "Figure 5 shows a positive correlation: models with higher original accuracy tend to experience larger performance declines when tested with perturbed prompts."
            },
            "result": true,
            "issue": null
          },
          {
            "qa": {
              "question": "The article states that \"models that perform exceptionally well on standard benchmarks may be capitalizing on dataset-specific cues.\" What evidence or examples are provided to support this claim?",
              "answer": "The evidence provided is that rephrasing benchmark prompts, while preserving semantic meaning, caused a significant performance degradation in most LLMs, and that models with higher baseline accuracy exhibited larger performance drops, suggesting they capitalize on superficial cues."
            },
            "result": true,
            "issue": null
          },
          {
            "qa": {
              "question": "Why do larger models, which often achieve higher accuracy, tend to be more sensitive to the rephrasing of prompts according to the study's findings?",
              "answer": "The study's findings suggest that larger models, which often achieve higher accuracy, tend to be more sensitive to the rephrasing of prompts because they are more susceptible to overfitting and capitalize more on dataset-specific surface cues."
            },
            "result": true,
            "issue": null
          },
          {
            "qa": {
              "question": "ACU. What was the average drop in accuracy across all 26 evaluated models when tested with rephrased prompts?",
              "answer": "The average drop in accuracy across all 26 evaluated models when tested with rephrased prompts was 2.15%."
            },
            "result": true,
            "issue": null
          },
          {
            "qa": {
              "question": "ACU. How many of the 26 models evaluated exhibited a statistically significant drop in performance on the perturbed dataset?",
              "answer": "20 out of the 26 models evaluated exhibited a statistically significant drop in performance on the perturbed dataset."
            },
            "result": true,
            "issue": null
          },
          {
            "qa": {
              "question": "ACU. According to the study, which family of models showed insignificant performance degradation, suggesting less dependency on superficial cues?",
              "answer": "The Llama family of models showed insignificant performance degradation, suggesting less dependency on superficial cues."
            },
            "result": true,
            "issue": null
          },
          {
            "qa": {
              "question": "ACU. What is the name of the statistical test used to confirm that performance differences were not due to chance?",
              "answer": "The name of the statistical test used to confirm that performance differences were not due to chance is McNemar's test."
            },
            "result": true,
            "issue": null
          }
        ],
        "judgment": false,
        "sections_to_highlight": [
          "C-BOD method overview — systematic prompt distortion",
          "Relationship between model size and overfitting",
          "Relationship between baseline accuracy and performance decline",
          "Llama family's robustness to prompt rephrasing",
          "Average performance degradation percentage"
        ]
      },
      "correct_count_all": 8,
      "correct_count_acu": 4,
      "num_of_questions": 5,
      "sections_to_highlight": [
        "Average accuracy drop across models",
        "McNemar's test for statistical significance",
        "Figure 5 — relationship between original accuracy and performance decline",
        "Evidence for dataset-specific cue capitalization",
        "Llama family performance on perturbed data"
      ],
      "sections_to_highlight_size": 5
    },
    {
      "iteration_number": 3,
      "summary": "1. SUMMARY:\nThis paper introduces the Chameleon Benchmark Overfit Detector (C-BOD) to reveal how LLMs over-rely on dataset-specific surface cues rather than true language understanding. The study found that high accuracy on benchmarks like MMLU can be misleading, as models often capitalize on specific phrasing. C-BOD systematically distorts benchmark prompts while preserving semantic content, exposing overfitting when performance degrades. A key finding is the paradox where models with higher baseline accuracy exhibit larger performance drops under these perturbations, suggesting their success is tied to memorized patterns. The Llama family, notably, demonstrated robustness to rephrasing, showing insignificant degradation. McNemar's test confirmed that performance differences were statistically significant for most models, with an average accuracy drop of 2.15% across all evaluated models. The research emphasizes the need to look beyond leaderboard scores and prioritize resilience in LLM evaluation.\n\n2. KEY HIGHLIGHTS:\n*   Models with higher baseline accuracy tend to experience larger performance declines when prompts are rephrased, indicating a reliance on specific cues.\n*   The Llama family of models showed notable robustness to prompt rephrasing, with insignificant performance degradation.\n*   An average accuracy drop of 2.15% was observed across 26 leading LLMs when subjected to systematic prompt distortions.\n*   McNemar's test was used to statistically validate that performance drops due to prompt rephrasing are significant for most models, pointing to overfitting.\n*   C-BOD systematically distorts prompts to reveal how LLMs exploit dataset-specific phrasing, challenging the assumption that high benchmark scores equate to true language understanding.",
      "qa_pairs": [
        {
          "question": "According to the article, what is the paradoxical relationship observed between an LLM's baseline accuracy on a standard benchmark and its performance on perturbed, rephrased inputs?",
          "answer": "Models with higher baseline accuracy tend to experience larger performance declines when prompts are rephrased, indicating a reliance on specific cues."
        },
        {
          "question": "How does the Chameleon Benchmark Overfit Detector (C-BOD) method work to expose the fragility of high-scoring LLMs?",
          "answer": "C-BOD systematically distorts benchmark prompts while preserving semantic content, exposing overfitting when performance degrades."
        },
        {
          "question": "Figure 5 illustrates a key finding. What does it show about the connection between a model's original accuracy and the performance decline it experiences?",
          "answer": "Models with higher baseline accuracy tend to experience larger performance declines when prompts are rephrased, indicating a reliance on specific cues."
        },
        {
          "question": "The article states that \"models that perform exceptionally well on standard benchmarks may be capitalizing on dataset-specific cues.\" What evidence or examples are provided to support this claim?",
          "answer": "Not enough information in summary"
        },
        {
          "question": "Why do larger models, which often achieve higher accuracy, tend to be more sensitive to the rephrasing of prompts according to the study's findings?",
          "answer": "Not enough information in summary"
        },
        {
          "question": "ACU. What was the average drop in accuracy across all 26 evaluated models when tested with rephrased prompts?",
          "answer": "An average accuracy drop of 2.15% was observed across 26 leading LLMs when subjected to systematic prompt distortions."
        },
        {
          "question": "ACU. How many of the 26 models evaluated exhibited a statistically significant drop in performance on the perturbed dataset?",
          "answer": "McNemar's test confirmed that performance differences were statistically significant for most models."
        },
        {
          "question": "ACU. According to the study, which family of models showed insignificant performance degradation, suggesting less dependency on superficial cues?",
          "answer": "The Llama family of models showed notable robustness to prompt rephrasing, with insignificant performance degradation."
        },
        {
          "question": "ACU. What is the name of the statistical test used to confirm that performance differences were not due to chance?",
          "answer": "McNemar's test was used to statistically validate that performance drops due to prompt rephrasing are significant for most models, pointing to overfitting."
        }
      ],
      "judge": {
        "evaluations": [
          {
            "qa": {
              "question": "According to the article, what is the paradoxical relationship observed between an LLM's baseline accuracy on a standard benchmark and its performance on perturbed, rephrased inputs?",
              "answer": "Models with higher baseline accuracy tend to experience larger performance declines when prompts are rephrased, indicating a reliance on specific cues."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "How does the Chameleon Benchmark Overfit Detector (C-BOD) method work to expose the fragility of high-scoring LLMs?",
              "answer": "C-BOD systematically distorts benchmark prompts while preserving semantic content, exposing overfitting when performance degrades."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "Figure 5 illustrates a key finding. What does it show about the connection between a model's original accuracy and the performance decline it experiences?",
              "answer": "Models with higher baseline accuracy tend to experience larger performance declines when prompts are rephrased, indicating a reliance on specific cues."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "The article states that \"models that perform exceptionally well on standard benchmarks may be capitalizing on dataset-specific cues.\" What evidence or examples are provided to support this claim?",
              "answer": "Not enough information in summary"
            },
            "result": false,
            "issue": "The article provides examples in Table 3 showing how rephrasing specific questions leads to incorrect predictions, illustrating the reliance on specific phrasing."
          },
          {
            "qa": {
              "question": "Why do larger models, which often achieve higher accuracy, tend to be more sensitive to the rephrasing of prompts according to the study's findings?",
              "answer": "Not enough information in summary"
            },
            "result": false,
            "issue": "The article states that larger models tend to be more sensitive to rephrasing, indicating they may over-rely on fixed prompt patterns, but does not explicitly detail the 'why' beyond this observation."
          },
          {
            "qa": {
              "question": "ACU. What was the average drop in accuracy across all 26 evaluated models when tested with rephrased prompts?",
              "answer": "An average accuracy drop of 2.15% was observed across 26 leading LLMs when subjected to systematic prompt distortions."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "ACU. How many of the 26 models evaluated exhibited a statistically significant drop in performance on the perturbed dataset?",
              "answer": "McNemar's test confirmed that performance differences were statistically significant for most models."
            },
            "result": false,
            "issue": "The article states that '20 out of 26 models exhibiting statistically significant differences'."
          },
          {
            "qa": {
              "question": "ACU. According to the study, which family of models showed insignificant performance degradation, suggesting less dependency on superficial cues?",
              "answer": "The Llama family of models showed notable robustness to prompt rephrasing, with insignificant performance degradation."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "ACU. What is the name of the statistical test used to confirm that performance differences were not due to chance?",
              "answer": "McNemar's test was used to statistically validate that performance drops due to prompt rephrasing are significant for most models, pointing to overfitting."
            },
            "result": true,
            "issue": ""
          }
        ],
        "judgment": false,
        "sections_to_highlight": [
          "C-BOD method overview — systematic prompt distortion",
          "Paradoxical relationship — accuracy vs. performance decline",
          "Llama family — robustness to rephrasing",
          "Statistical significance — McNemar's test",
          "Average accuracy drop — 2.15% across models",
          "Model size vs. sensitivity to rephrasing",
          "Examples of prompt rephrasing impact (Table 3)"
        ]
      },
      "correct_count_all": 6,
      "correct_count_acu": 3,
      "num_of_questions": 5,
      "sections_to_highlight": [
        "Average accuracy drop across models",
        "Llama family's robustness to prompt rephrasing",
        "Relationship between baseline accuracy and performance decline",
        "McNemar's test for statistical significance",
        "Figure 5 — relationship between original accuracy and performance decline",
        "Relationship between model size and overfitting",
        "C-BOD method overview — systematic prompt distortion",
        "Evidence for dataset-specific cue capitalization",
        "Llama family performance on perturbed data",
        "Average performance degradation percentage"
      ],
      "sections_to_highlight_size": 10
    },
    {
      "iteration_number": 4,
      "summary": "1. SUMMARY:\nThis paper introduces the Chameleon Benchmark Overfit Detector (C-BOD), a meta-evaluation framework designed to identify when Large Language Models (LLMs) over-rely on dataset-specific surface cues rather than genuine language understanding. By systematically distorting benchmark prompts while preserving their semantic content, C-BOD exposes LLM fragility. Experiments on the MMLU benchmark with 26 leading LLMs revealed a significant paradox: models with higher baseline accuracy and larger parameter counts tend to exhibit greater performance degradation (an average of 2.15%) when faced with rephrased prompts. This suggests that top-performing models may be capitalizing on memorized patterns. Conversely, the Llama family of models, along with those with lower baseline accuracy, showed less performance degradation, indicating reduced dependency on superficial cues. The study utilized McNemar’s test to confirm the statistical significance of these performance drops, providing strong evidence that overfitting to canonical prompt formats is widespread. The findings challenge the community to prioritize resilience and generalization over leaderboard scores.\n\n2. KEY HIGHLIGHTS:\n*   LLMs that achieve high baseline accuracy on benchmarks often exhibit a greater performance decline when prompts are rephrased, suggesting a paradoxical relationship where accuracy can indicate fragility.\n*   Larger LLMs and those with higher original accuracy are more sensitive to prompt rephrasing, with an average accuracy drop of 2.15% observed across models when tested on perturbed prompts.\n*   The Llama family of models demonstrated robustness to prompt rephrasing, showing less performance degradation compared to many other evaluated LLMs.\n*   McNemar's test was used to statistically validate that performance differences between original and rephrased prompts are significant for most models, indicating overfitting to dataset-specific cues.\n*   The C-BOD method systematically distorts prompts to reveal LLM over-reliance on specific wording or structure, rather than true language understanding.",
      "qa_pairs": [
        {
          "question": "According to the article, what is the paradoxical relationship observed between an LLM's baseline accuracy on a standard benchmark and its performance on perturbed, rephrased inputs?",
          "answer": "Models with higher baseline accuracy tend to exhibit greater performance degradation when faced with rephrased prompts, suggesting that high accuracy can indicate fragility."
        },
        {
          "question": "How does the Chameleon Benchmark Overfit Detector (C-BOD) method work to expose the fragility of high-scoring LLMs?",
          "answer": "C-BOD systematically distorts benchmark prompts while preserving their semantic content to expose LLM fragility."
        },
        {
          "question": "Figure 5 illustrates a key finding. What does it show about the connection between a model's original accuracy and the performance decline it experiences?",
          "answer": "Not enough information in summary"
        },
        {
          "question": "The article states that \"models that perform exceptionally well on standard benchmarks may be capitalizing on dataset-specific cues.\" What evidence or examples are provided to support this claim?",
          "answer": "The paper introduces C-BOD, a method to identify when LLMs over-rely on dataset-specific surface cues. Experiments showed that models with higher baseline accuracy and larger parameter counts tend to exhibit greater performance degradation when faced with rephrased prompts, suggesting they capitalize on memorized patterns."
        },
        {
          "question": "Why do larger models, which often achieve higher accuracy, tend to be more sensitive to the rephrasing of prompts according to the study's findings?",
          "answer": "The study's findings suggest that larger models, which often achieve higher accuracy, tend to be more sensitive to the rephrasing of prompts because they may be capitalizing on memorized patterns or dataset-specific cues."
        },
        {
          "question": "ACU. What was the average drop in accuracy across all 26 evaluated models when tested with rephrased prompts?",
          "answer": "The average accuracy drop across all 26 evaluated models when tested with rephrased prompts was 2.15%."
        },
        {
          "question": "ACU. How many of the 26 models evaluated exhibited a statistically significant drop in performance on the perturbed dataset?",
          "answer": "Not enough information in summary"
        },
        {
          "question": "ACU. According to the study, which family of models showed insignificant performance degradation, suggesting less dependency on superficial cues?",
          "answer": "The Llama family of models showed insignificant performance degradation, suggesting less dependency on superficial cues."
        },
        {
          "question": "ACU. What is the name of the statistical test used to confirm that performance differences were not due to chance?",
          "answer": "McNemar's test was used to confirm that performance differences were not due to chance."
        }
      ],
      "judge": {
        "evaluations": [
          {
            "qa": {
              "question": "According to the article, what is the paradoxical relationship observed between an LLM's baseline accuracy on a standard benchmark and its performance on perturbed, rephrased inputs?",
              "answer": "Models with higher baseline accuracy tend to exhibit greater performance degradation when faced with rephrased prompts, suggesting that high accuracy can indicate fragility."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "How does the Chameleon Benchmark Overfit Detector (C-BOD) method work to expose the fragility of high-scoring LLMs?",
              "answer": "C-BOD systematically distorts benchmark prompts while preserving their semantic content to expose LLM fragility."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "Figure 5 illustrates a key finding. What does it show about the connection between a model's original accuracy and the performance decline it experiences?",
              "answer": "Not enough information in summary"
            },
            "result": false,
            "issue": "The summary does not contain information about Figure 5's specific findings regarding the connection between original accuracy and performance decline. The article mentions this relationship in Section 5.3 and Figure 5, stating that higher original accuracy correlates with larger declines."
          },
          {
            "qa": {
              "question": "The article states that \"models that perform exceptionally well on standard benchmarks may be capitalizing on dataset-specific cues.\" What evidence or examples are provided to support this claim?",
              "answer": "The paper introduces C-BOD, a method to identify when LLMs over-rely on dataset-specific surface cues. Experiments showed that models with higher baseline accuracy and larger parameter counts tend to exhibit greater performance degradation when faced with rephrased prompts, suggesting they capitalize on memorized patterns."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "Why do larger models, which often achieve higher accuracy, tend to be more sensitive to the rephrasing of prompts according to the study's findings?",
              "answer": "The study's findings suggest that larger models, which often achieve higher accuracy, tend to be more sensitive to the rephrasing of prompts because they may be capitalizing on memorized patterns or dataset-specific cues."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "ACU. What was the average drop in accuracy across all 26 evaluated models when tested with rephrased prompts?",
              "answer": "The average accuracy drop across all 26 evaluated models when tested with rephrased prompts was 2.15%."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "ACU. How many of the 26 models evaluated exhibited a statistically significant drop in performance on the perturbed dataset?",
              "answer": "Not enough information in summary"
            },
            "result": false,
            "issue": "The summary does not specify the number of models that showed a statistically significant drop. The article states, \"20 out of 26 models exhibiting statistically significant differences.\""
          },
          {
            "qa": {
              "question": "ACU. According to the study, which family of models showed insignificant performance degradation, suggesting less dependency on superficial cues?",
              "answer": "The Llama family of models showed insignificant performance degradation, suggesting less dependency on superficial cues."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "ACU. What is the name of the statistical test used to confirm that performance differences were not due to chance?",
              "answer": "McNemar's test was used to confirm that performance differences were not due to chance."
            },
            "result": true,
            "issue": ""
          }
        ],
        "judgment": false,
        "sections_to_highlight": [
          "Llama family — robustness to rephrasing",
          "Paradoxical relationship — accuracy vs. performance decline",
          "Model size vs. sensitivity to rephrasing",
          "C-BOD method overview — systematic prompt distortion",
          "Statistical significance — McNemar's test",
          "Number of models with significant performance drops",
          "Figure 5 — accuracy vs. performance decline connection"
        ]
      },
      "correct_count_all": 7,
      "correct_count_acu": 3,
      "num_of_questions": 5,
      "sections_to_highlight": [
        "Llama family — robustness to rephrasing",
        "Average accuracy drop across models",
        "Average accuracy drop — 2.15% across models",
        "Llama family's robustness to prompt rephrasing",
        "Relationship between baseline accuracy and performance decline",
        "Model size vs. sensitivity to rephrasing",
        "McNemar's test for statistical significance",
        "Examples of prompt rephrasing impact (Table 3)",
        "Figure 5 — relationship between original accuracy and performance decline",
        "Relationship between model size and overfitting",
        "Paradoxical relationship — accuracy vs. performance decline",
        "C-BOD method overview — systematic prompt distortion",
        "Statistical significance — McNemar's test",
        "Evidence for dataset-specific cue capitalization",
        "Llama family performance on perturbed data",
        "Average performance degradation percentage"
      ],
      "sections_to_highlight_size": 16
    },
    {
      "iteration_number": 5,
      "summary": "1. SUMMARY:\nThis research introduces the Chameleon Benchmark Overfit Detector (C-BOD) to address the issue of Large Language Models (LLMs) appearing highly accurate on benchmarks due to over-reliance on dataset-specific surface cues rather than true understanding. C-BOD systematically distorts benchmark prompts while preserving their semantic meaning and labels. When applied to the MMLU benchmark across 26 leading LLMs, C-BOD revealed an average performance degradation of 2.15% under these perturbations. Crucially, models with higher baseline accuracy and larger parameter counts tended to exhibit greater performance drops, suggesting a paradoxical relationship where high performance on standard benchmarks can mask fragility to prompt rephrasing. The Llama family, particularly models with lower baseline accuracy, demonstrated remarkable robustness to these rephrasings, indicating reduced dependency on superficial cues. McNemar's test confirmed the statistical significance of these performance differences, providing strong evidence that many LLMs capitalize on specific prompt patterns rather than generalizing language understanding.\n\n2. KEY HIGHLIGHTS:\n*   Twenty out of 26 evaluated LLMs showed statistically significant performance degradation (average of 2.15%) when benchmark prompts were rephrased, indicating overfitting to surface cues.\n*   Models with higher original benchmark accuracy and larger parameter counts were more sensitive to prompt rephrasing, exhibiting greater performance declines, suggesting a paradox where accuracy can mask fragility.\n*   The Llama family of models demonstrated notable robustness to prompt rephrasing, with minimal performance degradation, implying a reduced reliance on superficial patterns.\n*   C-BOD uses systematic prompt distortion and McNemar's test to statistically validate performance differences, revealing that high benchmark scores may not always reflect true generalization.",
      "qa_pairs": [
        {
          "question": "According to the article, what is the paradoxical relationship observed between an LLM's baseline accuracy on a standard benchmark and its performance on perturbed, rephrased inputs?",
          "answer": "Models with higher baseline accuracy tend to exhibit greater performance drops when tested with perturbed, rephrased inputs, suggesting a paradoxical relationship where high performance on standard benchmarks can mask fragility to prompt rephrasing."
        },
        {
          "question": "How does the Chameleon Benchmark Overfit Detector (C-BOD) method work to expose the fragility of high-scoring LLMs?",
          "answer": "C-BOD systematically distorts benchmark prompts while preserving their semantic meaning and labels to expose the fragility of high-scoring LLMs."
        },
        {
          "question": "Figure 5 illustrates a key finding. What does it show about the connection between a model's original accuracy and the performance decline it experiences?",
          "answer": "Not enough information in summary"
        },
        {
          "question": "The article states that \"models that perform exceptionally well on standard benchmarks may be capitalizing on dataset-specific cues.\" What evidence or examples are provided to support this claim?",
          "answer": "The research introduces the Chameleon Benchmark Overfit Detector (C-BOD) which systematically distorts benchmark prompts while preserving their semantic meaning and labels. When applied to the MMLU benchmark across 26 leading LLMs, C-BOD revealed an average performance degradation of 2.15% under these perturbations. Crucially, models with higher baseline accuracy and larger parameter counts tended to exhibit greater performance drops, suggesting a paradoxical relationship where high performance on standard benchmarks can mask fragility to prompt rephrasing. The Llama family, particularly models with lower baseline accuracy, demonstrated remarkable robustness to these rephrasings, indicating reduced dependency on superficial cues. McNemar's test confirmed the statistical significance of these performance differences, providing strong evidence that many LLMs capitalize on specific prompt patterns rather than generalizing language understanding."
        },
        {
          "question": "Why do larger models, which often achieve higher accuracy, tend to be more sensitive to the rephrasing of prompts according to the study's findings?",
          "answer": "Larger models, which often achieve higher accuracy, tend to be more sensitive to the rephrasing of prompts because they may be capitalizing on dataset-specific cues rather than true understanding."
        },
        {
          "question": "ACU. What was the average drop in accuracy across all 26 evaluated models when tested with rephrased prompts?",
          "answer": "The average performance degradation across all 26 evaluated models when tested with rephrased prompts was 2.15%."
        },
        {
          "question": "ACU. How many of the 26 models evaluated exhibited a statistically significant drop in performance on the perturbed dataset?",
          "answer": "Twenty out of the 26 evaluated models showed statistically significant performance degradation on the perturbed dataset."
        },
        {
          "question": "ACU. According to the study, which family of models showed insignificant performance degradation, suggesting less dependency on superficial cues?",
          "answer": "The Llama family of models showed insignificant performance degradation, suggesting less dependency on superficial cues."
        },
        {
          "question": "ACU. What is the name of the statistical test used to confirm that performance differences were not due to chance?",
          "answer": "McNemar's test was used to confirm that performance differences were not due to chance."
        }
      ],
      "judge": {
        "evaluations": [
          {
            "qa": {
              "question": "According to the article, what is the paradoxical relationship observed between an LLM's baseline accuracy on a standard benchmark and its performance on perturbed, rephrased inputs?",
              "answer": "Models with higher baseline accuracy tend to exhibit greater performance drops when tested with perturbed, rephrased inputs, suggesting a paradoxical relationship where high performance on standard benchmarks can mask fragility to prompt rephrasing."
            },
            "result": true,
            "issue": null
          },
          {
            "qa": {
              "question": "How does the Chameleon Benchmark Overfit Detector (C-BOD) method work to expose the fragility of high-scoring LLMs?",
              "answer": "C-BOD systematically distorts benchmark prompts while preserving their semantic meaning and labels to expose the fragility of high-scoring LLMs."
            },
            "result": true,
            "issue": null
          },
          {
            "qa": {
              "question": "Figure 5 illustrates a key finding. What does it show about the connection between a model's original accuracy and the performance decline it experiences?",
              "answer": "Not enough information in summary"
            },
            "result": false,
            "issue": "The summary states 'Not enough information in summary', but Figure 5 is described in the article as showing 'the relationship between baseline accuracy on the original prompts and the corresponding percentage difference in performance when evaluated on rephrased inputs. The plot clearly indicates that models with higher original accuracy tend to experience larger declines when exposed to prompt perturbations.'"
          },
          {
            "qa": {
              "question": "The article states that \"models that perform exceptionally well on standard benchmarks may be capitalizing on dataset-specific cues.\" What evidence or examples are provided to support this claim?",
              "answer": "The research introduces the Chameleon Benchmark Overfit Detector (C-BOD) which systematically distorts benchmark prompts while preserving their semantic meaning and labels. When applied to the MMLU benchmark across 26 leading LLMs, C-BOD revealed an average performance degradation of 2.15% under these perturbations. Crucially, models with higher baseline accuracy and larger parameter counts tended to exhibit greater performance drops, suggesting a paradoxical relationship where high performance on standard benchmarks can mask fragility to prompt rephrasing. The Llama family, particularly models with lower baseline accuracy, demonstrated remarkable robustness to these rephrasings, indicating reduced dependency on superficial cues. McNemar's test confirmed the statistical significance of these performance differences, providing strong evidence that many LLMs capitalize on specific prompt patterns rather than generalizing language understanding."
            },
            "result": true,
            "issue": null
          },
          {
            "qa": {
              "question": "Why do larger models, which often achieve higher accuracy, tend to be more sensitive to the rephrasing of prompts according to the study's findings?",
              "answer": "Larger models, which often achieve higher accuracy, tend to be more sensitive to the rephrasing of prompts because they may be capitalizing on dataset-specific cues rather than true understanding."
            },
            "result": true,
            "issue": null
          },
          {
            "qa": {
              "question": "ACU. What was the average drop in accuracy across all 26 evaluated models when tested with rephrased prompts?",
              "answer": "The average performance degradation across all 26 evaluated models when tested with rephrased prompts was 2.15%."
            },
            "result": true,
            "issue": null
          },
          {
            "qa": {
              "question": "ACU. How many of the 26 models evaluated exhibited a statistically significant drop in performance on the perturbed dataset?",
              "answer": "Twenty out of the 26 evaluated models showed statistically significant performance degradation on the perturbed dataset."
            },
            "result": true,
            "issue": null
          },
          {
            "qa": {
              "question": "ACU. According to the study, which family of models showed insignificant performance degradation, suggesting less dependency on superficial cues?",
              "answer": "The Llama family of models showed insignificant performance degradation, suggesting less dependency on superficial cues."
            },
            "result": true,
            "issue": null
          },
          {
            "qa": {
              "question": "ACU. What is the name of the statistical test used to confirm that performance differences were not due to chance?",
              "answer": "McNemar's test was used to confirm that performance differences were not due to chance."
            },
            "result": true,
            "issue": null
          }
        ],
        "judgment": false,
        "sections_to_highlight": [
          "C-BOD method — systematic prompt distortion",
          "Paradoxical relationship — accuracy vs. performance decline",
          "Llama family — robustness to rephrasing",
          "Model size vs. sensitivity to rephrasing",
          "Average accuracy drop — 2.15% across models",
          "McNemar's test for statistical significance",
          "Figure 5 — accuracy vs. performance decline connection"
        ]
      },
      "correct_count_all": 8,
      "correct_count_acu": 4,
      "num_of_questions": 5,
      "sections_to_highlight": [
        "Number of models with significant performance drops",
        "Llama family — robustness to rephrasing",
        "Average accuracy drop across models",
        "Average accuracy drop — 2.15% across models",
        "Llama family's robustness to prompt rephrasing",
        "Relationship between baseline accuracy and performance decline",
        "Model size vs. sensitivity to rephrasing",
        "Figure 5 — accuracy vs. performance decline connection",
        "McNemar's test for statistical significance",
        "Examples of prompt rephrasing impact (Table 3)",
        "Figure 5 — relationship between original accuracy and performance decline",
        "Relationship between model size and overfitting",
        "Paradoxical relationship — accuracy vs. performance decline",
        "C-BOD method overview — systematic prompt distortion",
        "Statistical significance — McNemar's test",
        "Evidence for dataset-specific cue capitalization",
        "Llama family performance on perturbed data",
        "Average performance degradation percentage"
      ],
      "sections_to_highlight_size": 18
    },
    {
      "iteration_number": 6,
      "summary": "1. SUMMARY:\nThis paper introduces the Chameleon Benchmark Overfit Detector (C-BOD), a meta-evaluation framework designed to uncover LLM overreliance on dataset-specific surface cues rather than true language understanding. C-BOD systematically distorts benchmark prompts while preserving semantic meaning, revealing how LLMs perform on rephrased inputs. The study found that a significant majority of LLMs (20 out of 26 evaluated) exhibit a performance degradation of an average of 2.15% on these perturbed prompts, with McNemar's test confirming the statistical significance of these drops. Notably, models with higher baseline accuracy and larger parameter counts tend to be more sensitive to these rephrasings, exhibiting greater performance declines. This highlights a paradoxical relationship: high benchmark accuracy may mask fragility due to an overreliance on fixed prompt patterns. In contrast, the Llama family of models demonstrated robustness, showing minimal performance degradation, suggesting reduced dependency on superficial cues. The paper emphasizes that high leaderboard scores alone are insufficient and advocates for prioritizing resilience and generalization in LLM evaluation.\n\n2. KEY HIGHLIGHTS:\n*   LLMs often overfit to benchmark-specific surface cues, leading to a paradox where high accuracy can mask fragility.\n*   The C-BOD framework systematically distorts prompts to measure performance degradation, revealing that 20 out of 26 evaluated LLMs showed statistically significant drops (average 2.15%).\n*   Larger models and those with higher baseline accuracy are more susceptible to performance declines when prompts are rephrased, indicating a reliance on superficial patterns.\n*   The Llama family of models exhibited robustness, performing consistently across original and rephrased prompts, suggesting better generalization.\n*   McNemar's test was used to statistically validate the performance differences observed between original and perturbed prompts.",
      "qa_pairs": [
        {
          "question": "According to the article, what is the paradoxical relationship observed between an LLM's baseline accuracy on a standard benchmark and its performance on perturbed, rephrased inputs?",
          "answer": "The paradoxical relationship is that high benchmark accuracy may mask fragility due to an overreliance on fixed prompt patterns."
        },
        {
          "question": "How does the Chameleon Benchmark Overfit Detector (C-BOD) method work to expose the fragility of high-scoring LLMs?",
          "answer": "C-BOD systematically distorts benchmark prompts while preserving semantic meaning, revealing how LLMs perform on rephrased inputs."
        },
        {
          "question": "Figure 5 illustrates a key finding. What does it show about the connection between a model's original accuracy and the performance decline it experiences?",
          "answer": "Not enough information in summary"
        },
        {
          "question": "The article states that \"models that perform exceptionally well on standard benchmarks may be capitalizing on dataset-specific cues.\" What evidence or examples are provided to support this claim?",
          "answer": "Not enough information in summary"
        },
        {
          "question": "Why do larger models, which often achieve higher accuracy, tend to be more sensitive to the rephrasing of prompts according to the study's findings?",
          "answer": "Larger models tend to be more sensitive to rephrasing of prompts, exhibiting greater performance declines, indicating a reliance on superficial patterns."
        },
        {
          "question": "ACU. What was the average drop in accuracy across all 26 evaluated models when tested with rephrased prompts?",
          "answer": "The average drop in accuracy across all 26 evaluated models when tested with rephrased prompts was 2.15%."
        },
        {
          "question": "ACU. How many of the 26 models evaluated exhibited a statistically significant drop in performance on the perturbed dataset?",
          "answer": "20 out of 26 evaluated models exhibited a statistically significant drop in performance on the perturbed dataset."
        },
        {
          "question": "ACU. According to the study, which family of models showed insignificant performance degradation, suggesting less dependency on superficial cues?",
          "answer": "The Llama family of models showed insignificant performance degradation, suggesting less dependency on superficial cues."
        },
        {
          "question": "ACU. What is the name of the statistical test used to confirm that performance differences were not due to chance?",
          "answer": "McNemar's test was used to confirm that performance differences were not due to chance."
        }
      ],
      "judge": {
        "evaluations": [
          {
            "qa": {
              "question": "According to the article, what is the paradoxical relationship observed between an LLM's baseline accuracy on a standard benchmark and its performance on perturbed, rephrased inputs?",
              "answer": "The paradoxical relationship is that high benchmark accuracy may mask fragility due to an overreliance on fixed prompt patterns."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "How does the Chameleon Benchmark Overfit Detector (C-BOD) method work to expose the fragility of high-scoring LLMs?",
              "answer": "C-BOD systematically distorts benchmark prompts while preserving semantic meaning, revealing how LLMs perform on rephrased inputs."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "Figure 5 illustrates a key finding. What does it show about the connection between a model's original accuracy and the performance decline it experiences?",
              "answer": "Not enough information in summary"
            },
            "result": false,
            "issue": "Figure 5 is not mentioned in the summary."
          },
          {
            "qa": {
              "question": "The article states that \"models that perform exceptionally well on standard benchmarks may be capitalizing on dataset-specific cues.\" What evidence or examples are provided to support this claim?",
              "answer": "Not enough information in summary"
            },
            "result": false,
            "issue": "The summary does not provide specific evidence or examples to support the claim."
          },
          {
            "qa": {
              "question": "Why do larger models, which often achieve higher accuracy, tend to be more sensitive to the rephrasing of prompts according to the study's findings?",
              "answer": "Larger models tend to be more sensitive to rephrasing of prompts, exhibiting greater performance declines, indicating a reliance on superficial patterns."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "ACU. What was the average drop in accuracy across all 26 evaluated models when tested with rephrased prompts?",
              "answer": "The average drop in accuracy across all 26 evaluated models when tested with rephrased prompts was 2.15%."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "ACU. How many of the 26 models evaluated exhibited a statistically significant drop in performance on the perturbed dataset?",
              "answer": "20 out of 26 evaluated models exhibited a statistically significant drop in performance on the perturbed dataset."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "ACU. According to the study, which family of models showed insignificant performance degradation, suggesting less dependency on superficial cues?",
              "answer": "The Llama family of models showed insignificant performance degradation, suggesting less dependency on superficial cues."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "ACU. What is the name of the statistical test used to confirm that performance differences were not due to chance?",
              "answer": "McNemar's test was used to confirm that performance differences were not due to chance."
            },
            "result": true,
            "issue": ""
          }
        ],
        "judgment": false,
        "sections_to_highlight": [
          "C-BOD method — systematic prompt distortion",
          "Paradoxical relationship — accuracy vs. performance decline",
          "Larger models — sensitivity to rephrasing",
          "Llama family — robustness to rephrasing",
          "Key findings on performance degradation",
          "Statistical significance — McNemar's test",
          "Evidence for dataset-specific cue capitalization"
        ]
      },
      "correct_count_all": 7,
      "correct_count_acu": 4,
      "num_of_questions": 5,
      "sections_to_highlight": [
        "McNemar's test for statistical significance",
        "Relationship between model size and overfitting",
        "C-BOD method overview — systematic prompt distortion",
        "Average performance degradation percentage",
        "Average accuracy drop — 2.15% across models",
        "Average accuracy drop across models",
        "Llama family's robustness to prompt rephrasing",
        "Model size vs. sensitivity to rephrasing",
        "Examples of prompt rephrasing impact (Table 3)",
        "Figure 5 — relationship between original accuracy and performance decline",
        "Paradoxical relationship — accuracy vs. performance decline",
        "Statistical significance — McNemar's test",
        "Llama family performance on perturbed data",
        "Number of models with significant performance drops",
        "Relationship between baseline accuracy and performance decline",
        "Evidence for dataset-specific cue capitalization",
        "Llama family — robustness to rephrasing",
        "Figure 5 — accuracy vs. performance decline connection",
        "C-BOD method — systematic prompt distortion"
      ],
      "sections_to_highlight_size": 19
    },
    {
      "iteration_number": 7,
      "summary": "1. SUMMARY:\nThis paper introduces the Chameleon Benchmark Overfit Detector (C-BOD), a meta-evaluation framework designed to uncover how Large Language Models (LLMs) over-rely on dataset-specific surface cues rather than true language understanding. C-BOD systematically distorts benchmark prompts while preserving semantic content, revealing overfitting by measuring performance degradation. The study found that out of 26 leading LLMs evaluated on the MMLU benchmark, 20 showed statistically significant performance drops under these perturbations, with an average accuracy degradation of 2.15%. Notably, a paradoxical relationship emerged: models with higher baseline accuracy and larger parameter counts tended to be more sensitive to these rephrasings, indicating a greater reliance on fixed prompt patterns. In contrast, the Llama family models, often with lower baseline accuracy, exhibited more robustness to prompt rephrasing. McNemar's test was used to confirm the statistical significance of these performance differences, highlighting that high benchmark scores can mask underlying fragilities.\n\n2. KEY HIGHLIGHTS:\n*   LLMs with higher baseline accuracy and more parameters are paradoxically more prone to performance degradation when prompts are systematically rephrased, suggesting reliance on surface cues.\n*   The C-BOD method uses parametric prompt distortion and McNemar's test to statistically identify LLMs that overfit to benchmark-specific phrasing.\n*   An average accuracy drop of 2.15% was observed across 26 LLMs, with 20 showing statistically significant declines, indicating widespread overfitting.\n*   The Llama family models demonstrated greater robustness to prompt rephrasing compared to many other LLMs, correlating with lower baseline accuracies.",
      "qa_pairs": [
        {
          "question": "According to the article, what is the paradoxical relationship observed between an LLM's baseline accuracy on a standard benchmark and its performance on perturbed, rephrased inputs?",
          "answer": "Models with higher baseline accuracy and larger parameter counts tended to be more sensitive to these rephrasings, indicating a greater reliance on fixed prompt patterns."
        },
        {
          "question": "How does the Chameleon Benchmark Overfit Detector (C-BOD) method work to expose the fragility of high-scoring LLMs?",
          "answer": "C-BOD systematically distorts benchmark prompts while preserving semantic content, revealing overfitting by measuring performance degradation."
        },
        {
          "question": "Figure 5 illustrates a key finding. What does it show about the connection between a model's original accuracy and the performance decline it experiences?",
          "answer": "Not enough information in summary"
        },
        {
          "question": "The article states that \"models that perform exceptionally well on standard benchmarks may be capitalizing on dataset-specific cues.\" What evidence or examples are provided to support this claim?",
          "answer": "The study found that out of 26 leading LLMs evaluated on the MMLU benchmark, 20 showed statistically significant performance drops under these perturbations, with an average accuracy degradation of 2.15%. Notably, a paradoxical relationship emerged: models with higher baseline accuracy and larger parameter counts tended to be more sensitive to these rephrasings, indicating a greater reliance on fixed prompt patterns."
        },
        {
          "question": "Why do larger models, which often achieve higher accuracy, tend to be more sensitive to the rephrasing of prompts according to the study's findings?",
          "answer": "Larger models tend to be more sensitive to the rephrasing of prompts because they may be capitalizing on dataset-specific cues and have a greater reliance on fixed prompt patterns."
        },
        {
          "question": "ACU. What was the average drop in accuracy across all 26 evaluated models when tested with rephrased prompts?",
          "answer": "The average accuracy degradation across all 26 evaluated models was 2.15%."
        },
        {
          "question": "ACU. How many of the 26 models evaluated exhibited a statistically significant drop in performance on the perturbed dataset?",
          "answer": "20 out of the 26 evaluated models showed statistically significant performance drops on the perturbed dataset."
        },
        {
          "question": "ACU. According to the study, which family of models showed insignificant performance degradation, suggesting less dependency on superficial cues?",
          "answer": "The Llama family models exhibited more robustness to prompt rephrasing, suggesting less dependency on superficial cues."
        },
        {
          "question": "ACU. What is the name of the statistical test used to confirm that performance differences were not due to chance?",
          "answer": "McNemar's test was used to confirm that performance differences were not due to chance."
        }
      ],
      "judge": {
        "evaluations": [
          {
            "qa": {
              "question": "According to the article, what is the paradoxical relationship observed between an LLM's baseline accuracy on a standard benchmark and its performance on perturbed, rephrased inputs?",
              "answer": "Models with higher baseline accuracy and larger parameter counts tended to be more sensitive to these rephrasings, indicating a greater reliance on fixed prompt patterns."
            },
            "result": true,
            "issue": null
          },
          {
            "qa": {
              "question": "How does the Chameleon Benchmark Overfit Detector (C-BOD) method work to expose the fragility of high-scoring LLMs?",
              "answer": "C-BOD systematically distorts benchmark prompts while preserving semantic content, revealing overfitting by measuring performance degradation."
            },
            "result": true,
            "issue": null
          },
          {
            "qa": {
              "question": "Figure 5 illustrates a key finding. What does it show about the connection between a model's original accuracy and the performance decline it experiences?",
              "answer": "Not enough information in summary"
            },
            "result": false,
            "issue": "The summary does not contain specific information about Figure 5's findings regarding the connection between original accuracy and performance decline."
          },
          {
            "qa": {
              "question": "The article states that \"models that perform exceptionally well on standard benchmarks may be capitalizing on dataset-specific cues.\" What evidence or examples are provided to support this claim?",
              "answer": "The study found that out of 26 leading LLMs evaluated on the MMLU benchmark, 20 showed statistically significant performance drops under these perturbations, with an average accuracy degradation of 2.15%. Notably, a paradoxical relationship emerged: models with higher baseline accuracy and larger parameter counts tended to be more sensitive to these rephrasings, indicating a greater reliance on fixed prompt patterns."
            },
            "result": true,
            "issue": null
          },
          {
            "qa": {
              "question": "Why do larger models, which often achieve higher accuracy, tend to be more sensitive to the rephrasing of prompts according to the study's findings?",
              "answer": "Larger models tend to be more sensitive to the rephrasing of prompts because they may be capitalizing on dataset-specific cues and have a greater reliance on fixed prompt patterns."
            },
            "result": true,
            "issue": null
          },
          {
            "qa": {
              "question": "ACU. What was the average drop in accuracy across all 26 evaluated models when tested with rephrased prompts?",
              "answer": "The average accuracy degradation across all 26 evaluated models was 2.15%."
            },
            "result": true,
            "issue": null
          },
          {
            "qa": {
              "question": "ACU. How many of the 26 models evaluated exhibited a statistically significant drop in performance on the perturbed dataset?",
              "answer": "20 out of the 26 evaluated models showed statistically significant performance drops on the perturbed dataset."
            },
            "result": true,
            "issue": null
          },
          {
            "qa": {
              "question": "ACU. According to the study, which family of models showed insignificant performance degradation, suggesting less dependency on superficial cues?",
              "answer": "The Llama family models exhibited more robustness to prompt rephrasing, suggesting less dependency on superficial cues."
            },
            "result": true,
            "issue": null
          },
          {
            "qa": {
              "question": "ACU. What is the name of the statistical test used to confirm that performance differences were not due to chance?",
              "answer": "McNemar's test was used to confirm that performance differences were not due to chance."
            },
            "result": true,
            "issue": null
          }
        ],
        "judgment": false,
        "sections_to_highlight": [
          "C-BOD method — prompt distortion and performance measurement",
          "Paradoxical relationship — accuracy vs. performance decline",
          "Llama family — robustness and lower accuracy",
          "Model size vs. sensitivity to rephrasing",
          "Statistical significance — McNemar's test",
          "Average accuracy drop — 2.15% across models",
          "Number of models with significant drops — 20/26"
        ]
      },
      "correct_count_all": 8,
      "correct_count_acu": 4,
      "num_of_questions": 5,
      "sections_to_highlight": [
        "McNemar's test for statistical significance",
        "Relationship between model size and overfitting",
        "C-BOD method overview — systematic prompt distortion",
        "Average performance degradation percentage",
        "Average accuracy drop — 2.15% across models",
        "Average accuracy drop across models",
        "Llama family's robustness to prompt rephrasing",
        "Model size vs. sensitivity to rephrasing",
        "Key findings on performance degradation",
        "Larger models — sensitivity to rephrasing",
        "Examples of prompt rephrasing impact (Table 3)",
        "Figure 5 — relationship between original accuracy and performance decline",
        "Paradoxical relationship — accuracy vs. performance decline",
        "Statistical significance — McNemar's test",
        "Llama family performance on perturbed data",
        "Number of models with significant performance drops",
        "Relationship between baseline accuracy and performance decline",
        "Evidence for dataset-specific cue capitalization",
        "Llama family — robustness to rephrasing",
        "Figure 5 — accuracy vs. performance decline connection",
        "C-BOD method — systematic prompt distortion"
      ],
      "sections_to_highlight_size": 21
    },
    {
      "iteration_number": 8,
      "summary": "1. SUMMARY:\n\nThis paper introduces the Chameleon Benchmark Overfit Detector (C-BOD), a novel framework designed to reveal how Large Language Models (LLMs) overfit to benchmark-specific surface cues rather than demonstrating true language understanding. C-BOD achieves this by systematically distorting benchmark prompts, preserving their semantic content, and measuring performance degradation. The study found that 20 out of 26 leading LLMs exhibited statistically significant performance drops (averaging 2.15%) when presented with rephrased prompts, indicating a reliance on memorized patterns. Notably, larger models and those with higher baseline accuracy were found to be more sensitive to these rephrasings, revealing a paradox: high accuracy on standard benchmarks may mask underlying fragility. The Llama family, in contrast, showed greater robustness and less performance degradation, suggesting a reduced dependency on superficial cues. The use of McNemar's test validates the statistical significance of these observed performance differences.\n\n2. KEY HIGHLIGHTS:\n\n*   20 out of 26 evaluated LLMs showed statistically significant performance drops (average 2.15%) when prompts were systematically rephrased, indicating overfitting to benchmark-specific cues.\n*   A paradoxical relationship was observed: LLMs with higher original accuracy and larger model sizes tended to be more sensitive to prompt rephrasing, suggesting their high scores might mask fragility.\n*   The Llama family demonstrated robustness to prompt rephrasing, exhibiting less performance degradation, which the authors attribute to a reduced dependency on superficial cues.\n*   C-BOD uses systematic prompt distortion and McNemar's test to quantify performance differences, providing a method to detect overfitting beyond traditional benchmark scores.",
      "qa_pairs": [
        {
          "question": "According to the article, what is the paradoxical relationship observed between an LLM's baseline accuracy on a standard benchmark and its performance on perturbed, rephrased inputs?",
          "answer": "LLMs with higher original accuracy tended to be more sensitive to prompt rephrasing, suggesting their high scores might mask fragility."
        },
        {
          "question": "How does the Chameleon Benchmark Overfit Detector (C-BOD) method work to expose the fragility of high-scoring LLMs?",
          "answer": "C-BOD works by systematically distorting benchmark prompts while preserving their semantic content, and then measuring the performance degradation."
        },
        {
          "question": "Figure 5 illustrates a key finding. What does it show about the connection between a model's original accuracy and the performance decline it experiences?",
          "answer": "Not enough information in summary"
        },
        {
          "question": "The article states that \"models that perform exceptionally well on standard benchmarks may be capitalizing on dataset-specific cues.\" What evidence or examples are provided to support this claim?",
          "answer": "Not enough information in summary"
        },
        {
          "question": "Why do larger models, which often achieve higher accuracy, tend to be more sensitive to the rephrasing of prompts according to the study's findings?",
          "answer": "The study found that larger models were more sensitive to prompt rephrasing, indicating their high scores might mask fragility."
        },
        {
          "question": "ACU. What was the average drop in accuracy across all 26 evaluated models when tested with rephrased prompts?",
          "answer": "The average drop in accuracy across all 26 evaluated models when tested with rephrased prompts was 2.15%."
        },
        {
          "question": "ACU. How many of the 26 models evaluated exhibited a statistically significant drop in performance on the perturbed dataset?",
          "answer": "20 out of 26 evaluated LLMs exhibited statistically significant performance drops on the perturbed dataset."
        },
        {
          "question": "ACU. According to the study, which family of models showed insignificant performance degradation, suggesting less dependency on superficial cues?",
          "answer": "The Llama family showed insignificant performance degradation, suggesting less dependency on superficial cues."
        },
        {
          "question": "ACU. What is the name of the statistical test used to confirm that performance differences were not due to chance?",
          "answer": "McNemar's test was used to confirm that performance differences were not due to chance."
        }
      ],
      "judge": {
        "evaluations": [
          {
            "qa": {
              "question": "According to the article, what is the paradoxical relationship observed between an LLM's baseline accuracy on a standard benchmark and its performance on perturbed, rephrased inputs?",
              "answer": "LLMs with higher original accuracy tended to be more sensitive to prompt rephrasing, suggesting their high scores might mask fragility."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "How does the Chameleon Benchmark Overfit Detector (C-BOD) method work to expose the fragility of high-scoring LLMs?",
              "answer": "C-BOD works by systematically distorting benchmark prompts while preserving their semantic content, and then measuring the performance degradation."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "Figure 5 illustrates a key finding. What does it show about the connection between a model's original accuracy and the performance decline it experiences?",
              "answer": "Not enough information in summary"
            },
            "result": false,
            "issue": "The summary does not contain information about Figure 5 or the specific connection between original accuracy and performance decline it illustrates."
          },
          {
            "qa": {
              "question": "The article states that \"models that perform exceptionally well on standard benchmarks may be capitalizing on dataset-specific cues.\" What evidence or examples are provided to support this claim?",
              "answer": "Not enough information in summary"
            },
            "result": false,
            "issue": "The summary does not provide specific evidence or examples to support the claim about models capitalizing on dataset-specific cues."
          },
          {
            "qa": {
              "question": "Why do larger models, which often achieve higher accuracy, tend to be more sensitive to the rephrasing of prompts according to the study's findings?",
              "answer": "The study found that larger models were more sensitive to prompt rephrasing, indicating their high scores might mask fragility."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "ACU. What was the average drop in accuracy across all 26 evaluated models when tested with rephrased prompts?",
              "answer": "The average drop in accuracy across all 26 evaluated models when tested with rephrased prompts was 2.15%."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "ACU. How many of the 26 models evaluated exhibited a statistically significant drop in performance on the perturbed dataset?",
              "answer": "20 out of 26 evaluated LLMs exhibited statistically significant performance drops on the perturbed dataset."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "ACU. According to the study, which family of models showed insignificant performance degradation, suggesting less dependency on superficial cues?",
              "answer": "The Llama family showed insignificant performance degradation, suggesting less dependency on superficial cues."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "ACU. What is the name of the statistical test used to confirm that performance differences were not due to chance?",
              "answer": "McNemar's test was used to confirm that performance differences were not due to chance."
            },
            "result": true,
            "issue": ""
          }
        ],
        "judgment": false,
        "sections_to_highlight": [
          "C-BOD method — systematic prompt distortion and performance measurement",
          "Paradoxical relationship — higher accuracy LLMs are more sensitive to rephrasing",
          "Llama family — robustness and lower sensitivity to rephrasing",
          "Model size vs. performance difference — larger models are more sensitive",
          "Statistical significance — McNemar's test validates findings",
          "Key findings — 20/26 models show significant drops, average 2.15% degradation"
        ]
      },
      "correct_count_all": 7,
      "correct_count_acu": 4,
      "num_of_questions": 5,
      "sections_to_highlight": [
        "Llama family — robustness and lower accuracy",
        "McNemar's test for statistical significance",
        "Relationship between model size and overfitting",
        "C-BOD method overview — systematic prompt distortion",
        "Number of models with significant drops — 20/26",
        "Average performance degradation percentage",
        "Average accuracy drop — 2.15% across models",
        "Average accuracy drop across models",
        "Llama family's robustness to prompt rephrasing",
        "Model size vs. sensitivity to rephrasing",
        "Key findings on performance degradation",
        "Larger models — sensitivity to rephrasing",
        "Examples of prompt rephrasing impact (Table 3)",
        "Figure 5 — relationship between original accuracy and performance decline",
        "Paradoxical relationship — accuracy vs. performance decline",
        "Statistical significance — McNemar's test",
        "Llama family performance on perturbed data",
        "Number of models with significant performance drops",
        "Relationship between baseline accuracy and performance decline",
        "C-BOD method — prompt distortion and performance measurement",
        "Evidence for dataset-specific cue capitalization",
        "Llama family — robustness to rephrasing",
        "Figure 5 — accuracy vs. performance decline connection",
        "C-BOD method — systematic prompt distortion"
      ],
      "sections_to_highlight_size": 24
    },
    {
      "iteration_number": 9,
      "summary": "1. SUMMARY:\nThis paper introduces the Chameleon Benchmark Overfit Detector (C-BOD), a framework designed to uncover how Large Language Models (LLMs) overfit to benchmark-specific surface cues rather than truly understanding language. C-BOD works by systematically distorting benchmark prompts while preserving their semantic meaning and then measuring performance changes. The study found that a significant majority of evaluated LLMs (20 out of 26) exhibited noticeable performance drops (an average of 2.15% accuracy degradation) when presented with these rephrased prompts. Crucially, the research revealed a paradoxical relationship: models with higher baseline accuracy on original prompts tended to show larger performance declines under perturbation, suggesting they capitalize more on superficial cues. Conversely, the Llama family of models, often exhibiting lower baseline accuracy, demonstrated greater robustness to these rephrasings, indicating reduced dependency on fixed prompt patterns. This suggests that high benchmark scores alone may not be a reliable indicator of true generalization capabilities.\n\n2. KEY HIGHLIGHTS:\n*   20 out of 26 LLMs showed statistically significant performance drops (average 2.15% accuracy degradation) when evaluated on systematically rephrased prompts, indicating widespread overfitting to benchmark-specific cues.\n*   A paradoxical relationship was observed: LLMs with higher original benchmark accuracy were more sensitive to prompt rephrasing, suggesting they rely more on surface-level patterns.\n*   The Llama family of models demonstrated robustness to prompt rephrasing and tended to have lower baseline accuracy, implying less reliance on superficial cues compared to higher-performing models.\n*   Larger LLMs generally exhibited greater performance degradation when prompts were rephrased, indicating increased sensitivity to prompt variations as model size increases.\n*   The C-BOD method uses systematic prompt distortion and McNemar's test to statistically validate performance differences, revealing that high accuracy on standard benchmarks can mask underlying fragility.",
      "qa_pairs": [
        {
          "question": "According to the article, what is the paradoxical relationship observed between an LLM's baseline accuracy on a standard benchmark and its performance on perturbed, rephrased inputs?",
          "answer": "Models with higher baseline accuracy on original prompts tended to show larger performance declines under perturbation, suggesting they capitalize more on superficial cues."
        },
        {
          "question": "How does the Chameleon Benchmark Overfit Detector (C-BOD) method work to expose the fragility of high-scoring LLMs?",
          "answer": "C-BOD works by systematically distorting benchmark prompts while preserving their semantic meaning and then measuring performance changes."
        },
        {
          "question": "Figure 5 illustrates a key finding. What does it show about the connection between a model's original accuracy and the performance decline it experiences?",
          "answer": "Not enough information in summary"
        },
        {
          "question": "The article states that \"models that perform exceptionally well on standard benchmarks may be capitalizing on dataset-specific cues.\" What evidence or examples are provided to support this claim?",
          "answer": "The study found that a significant majority of evaluated LLMs (20 out of 26) exhibited noticeable performance drops (an average of 2.15% accuracy degradation) when presented with these rephrased prompts, indicating they capitalize on dataset-specific cues."
        },
        {
          "question": "Why do larger models, which often achieve higher accuracy, tend to be more sensitive to the rephrasing of prompts according to the study's findings?",
          "answer": "Larger LLMs generally exhibited greater performance degradation when prompts were rephrased, indicating increased sensitivity to prompt variations as model size increases."
        },
        {
          "question": "ACU. What was the average drop in accuracy across all 26 evaluated models when tested with rephrased prompts?",
          "answer": "The average drop in accuracy across all 26 evaluated models when tested with rephrased prompts was 2.15%."
        },
        {
          "question": "ACU. How many of the 26 models evaluated exhibited a statistically significant drop in performance on the perturbed dataset?",
          "answer": "20 out of 26 LLMs showed statistically significant performance drops."
        },
        {
          "question": "ACU. According to the study, which family of models showed insignificant performance degradation, suggesting less dependency on superficial cues?",
          "answer": "The Llama family of models demonstrated robustness to prompt rephrasing and tended to have lower baseline accuracy, implying less reliance on superficial cues."
        },
        {
          "question": "ACU. What is the name of the statistical test used to confirm that performance differences were not due to chance?",
          "answer": "McNemar's test is used to statistically validate performance differences."
        }
      ],
      "judge": {
        "evaluations": [
          {
            "qa": {
              "question": "According to the article, what is the paradoxical relationship observed between an LLM's baseline accuracy on a standard benchmark and its performance on perturbed, rephrased inputs?",
              "answer": "Models with higher baseline accuracy on original prompts tended to show larger performance declines under perturbation, suggesting they capitalize more on superficial cues."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "How does the Chameleon Benchmark Overfit Detector (C-BOD) method work to expose the fragility of high-scoring LLMs?",
              "answer": "C-BOD works by systematically distorting benchmark prompts while preserving their semantic meaning and then measuring performance changes."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "Figure 5 illustrates a key finding. What does it show about the connection between a model's original accuracy and the performance decline it experiences?",
              "answer": "Not enough information in summary"
            },
            "result": false,
            "issue": "The summary does not contain information about Figure 5 or its specific findings regarding the connection between a model's original accuracy and performance decline. The article mentions Figure 5 illustrates the relationship between baseline accuracy and performance difference, indicating higher accuracy models experience larger declines."
          },
          {
            "qa": {
              "question": "The article states that \"models that perform exceptionally well on standard benchmarks may be capitalizing on dataset-specific cues.\" What evidence or examples are provided to support this claim?",
              "answer": "The study found that a significant majority of evaluated LLMs (20 out of 26) exhibited noticeable performance drops (an average of 2.15% accuracy degradation) when presented with these rephrased prompts, indicating they capitalize on dataset-specific cues."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "Why do larger models, which often achieve higher accuracy, tend to be more sensitive to the rephrasing of prompts according to the study's findings?",
              "answer": "Larger LLMs generally exhibited greater performance degradation when prompts were rephrased, indicating increased sensitivity to prompt variations as model size increases."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "ACU. What was the average drop in accuracy across all 26 evaluated models when tested with rephrased prompts?",
              "answer": "The average drop in accuracy across all 26 evaluated models when tested with rephrased prompts was 2.15%."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "ACU. How many of the 26 models evaluated exhibited a statistically significant drop in performance on the perturbed dataset?",
              "answer": "20 out of 26 LLMs showed statistically significant performance drops."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "ACU. According to the study, which family of models showed insignificant performance degradation, suggesting less dependency on superficial cues?",
              "answer": "The Llama family of models demonstrated robustness to prompt rephrasing and tended to have lower baseline accuracy, implying less reliance on superficial cues."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "ACU. What is the name of the statistical test used to confirm that performance differences were not due to chance?",
              "answer": "McNemar's test is used to statistically validate performance differences."
            },
            "result": true,
            "issue": ""
          }
        ],
        "judgment": false,
        "sections_to_highlight": [
          "Paradoxical relationship: higher accuracy LLMs more sensitive to rephrasing",
          "C-BOD method: systematic prompt distortion and performance measurement",
          "Llama family: robustness and lower accuracy",
          "Larger models: increased sensitivity to rephrasing",
          "McNemar's test: statistical validation of performance differences",
          "Average performance degradation: 2.15% across 26 models",
          "20/26 models show statistically significant performance drops"
        ]
      },
      "correct_count_all": 8,
      "correct_count_acu": 4,
      "num_of_questions": 5,
      "sections_to_highlight": [
        "Llama family — robustness and lower accuracy",
        "McNemar's test for statistical significance",
        "Relationship between model size and overfitting",
        "C-BOD method overview — systematic prompt distortion",
        "Number of models with significant drops — 20/26",
        "Average performance degradation percentage",
        "Average accuracy drop — 2.15% across models",
        "Average accuracy drop across models",
        "Llama family's robustness to prompt rephrasing",
        "Model size vs. sensitivity to rephrasing",
        "Key findings on performance degradation",
        "C-BOD method — systematic prompt distortion and performance measurement",
        "Key findings — 20/26 models show significant drops, average 2.15% degradation",
        "Larger models — sensitivity to rephrasing",
        "Examples of prompt rephrasing impact (Table 3)",
        "Figure 5 — relationship between original accuracy and performance decline",
        "Paradoxical relationship — accuracy vs. performance decline",
        "Statistical significance — McNemar's test",
        "Llama family performance on perturbed data",
        "Number of models with significant performance drops",
        "Relationship between baseline accuracy and performance decline",
        "C-BOD method — prompt distortion and performance measurement",
        "Model size vs. performance difference — larger models are more sensitive",
        "Evidence for dataset-specific cue capitalization",
        "Llama family — robustness to rephrasing",
        "Paradoxical relationship — higher accuracy LLMs are more sensitive to rephrasing",
        "Figure 5 — accuracy vs. performance decline connection",
        "C-BOD method — systematic prompt distortion",
        "Statistical significance — McNemar's test validates findings",
        "Llama family — robustness and lower sensitivity to rephrasing"
      ],
      "sections_to_highlight_size": 30
    },
    {
      "iteration_number": 10,
      "summary": "1. SUMMARY:\nThis paper introduces the Chameleon Benchmark Overfit Detector (C-BOD), a framework designed to expose LLMs' over-reliance on dataset-specific surface cues rather than true language understanding. C-BOD systematically distorts benchmark prompts while preserving semantic content and labels, then measures performance changes. The study found that 20 out of 26 leading LLMs exhibited statistically significant performance degradation (an average drop of 2.15%) on these rephrased prompts, indicating overfitting. Crucially, a paradoxical relationship emerged: models with higher baseline accuracy were more sensitive to rephrasing, showing larger performance declines. Conversely, the Llama family of models, often exhibiting lower baseline accuracy, demonstrated greater robustness and less sensitivity to prompt variations. This suggests that high benchmark scores can be misleading, masking a fragility to prompt rephrasing. The paper emphasizes the need to look beyond leaderboard scores and prioritize resilience in LLM evaluation.\n\n2. KEY HIGHLIGHTS:\n*   20 out of 26 LLMs showed statistically significant performance drops (avg. 2.15%) when prompts were systematically rephrased, indicating overfitting to benchmark-specific cues.\n*   A paradoxical relationship was observed: LLMs with higher baseline accuracy were more sensitive to rephrasing, experiencing larger performance declines.\n*   The Llama family of models demonstrated robustness to prompt rephrasing, often correlating with lower baseline accuracy.\n*   Larger models generally exhibited increased sensitivity to rephrasing, suggesting a trend where scale can amplify overfitting.\n*   McNemar's test was used to statistically validate the observed performance differences, confirming that the drops were not due to chance.",
      "qa_pairs": [
        {
          "question": "According to the article, what is the paradoxical relationship observed between an LLM's baseline accuracy on a standard benchmark and its performance on perturbed, rephrased inputs?",
          "answer": "A paradoxical relationship emerged: models with higher baseline accuracy were more sensitive to rephrasing, showing larger performance declines."
        },
        {
          "question": "How does the Chameleon Benchmark Overfit Detector (C-BOD) method work to expose the fragility of high-scoring LLMs?",
          "answer": "C-BOD systematically distorts benchmark prompts while preserving semantic content and labels, then measures performance changes."
        },
        {
          "question": "Figure 5 illustrates a key finding. What does it show about the connection between a model's original accuracy and the performance decline it experiences?",
          "answer": "The summary states that models with higher baseline accuracy were more sensitive to rephrasing, showing larger performance declines, which is likely illustrated in Figure 5."
        },
        {
          "question": "The article states that \"models that perform exceptionally well on standard benchmarks may be capitalizing on dataset-specific cues.\" What evidence or examples are provided to support this claim?",
          "answer": "The study found that 20 out of 26 leading LLMs exhibited statistically significant performance degradation (an average drop of 2.15%) on rephrased prompts, indicating overfitting to dataset-specific surface cues rather than true language understanding."
        },
        {
          "question": "Why do larger models, which often achieve higher accuracy, tend to be more sensitive to the rephrasing of prompts according to the study's findings?",
          "answer": "Larger models generally exhibited increased sensitivity to rephrasing, suggesting a trend where scale can amplify overfitting."
        },
        {
          "question": "ACU. What was the average drop in accuracy across all 26 evaluated models when tested with rephrased prompts?",
          "answer": "The average drop in accuracy across all 26 evaluated models when tested with rephrased prompts was 2.15%."
        },
        {
          "question": "ACU. How many of the 26 models evaluated exhibited a statistically significant drop in performance on the perturbed dataset?",
          "answer": "20 out of 26 models evaluated exhibited a statistically significant drop in performance on the perturbed dataset."
        },
        {
          "question": "ACU. According to the study, which family of models showed insignificant performance degradation, suggesting less dependency on superficial cues?",
          "answer": "The Llama family of models showed insignificant performance degradation, suggesting less dependency on superficial cues."
        },
        {
          "question": "ACU. What is the name of the statistical test used to confirm that performance differences were not due to chance?",
          "answer": "McNemar's test was used to statistically validate the observed performance differences, confirming that the drops were not due to chance."
        }
      ],
      "judge": {
        "evaluations": [
          {
            "qa": {
              "question": "According to the article, what is the paradoxical relationship observed between an LLM's baseline accuracy on a standard benchmark and its performance on perturbed, rephrased inputs?",
              "answer": "A paradoxical relationship emerged: models with higher baseline accuracy were more sensitive to rephrasing, showing larger performance declines."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "How does the Chameleon Benchmark Overfit Detector (C-BOD) method work to expose the fragility of high-scoring LLMs?",
              "answer": "C-BOD systematically distorts benchmark prompts while preserving semantic content and labels, then measures performance changes."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "Figure 5 illustrates a key finding. What does it show about the connection between a model's original accuracy and the performance decline it experiences?",
              "answer": "The summary states that models with higher baseline accuracy were more sensitive to rephrasing, showing larger performance declines, which is likely illustrated in Figure 5."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "The article states that \"models that perform exceptionally well on standard benchmarks may be capitalizing on dataset-specific cues.\" What evidence or examples are provided to support this claim?",
              "answer": "The study found that 20 out of 26 leading LLMs exhibited statistically significant performance degradation (an average drop of 2.15%) on rephrased prompts, indicating overfitting to dataset-specific surface cues rather than true language understanding."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "Why do larger models, which often achieve higher accuracy, tend to be more sensitive to the rephrasing of prompts according to the study's findings?",
              "answer": "Larger models generally exhibited increased sensitivity to rephrasing, suggesting a trend where scale can amplify overfitting."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "ACU. What was the average drop in accuracy across all 26 evaluated models when tested with rephrased prompts?",
              "answer": "The average drop in accuracy across all 26 evaluated models when tested with rephrased prompts was 2.15%."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "ACU. How many of the 26 models evaluated exhibited a statistically significant drop in performance on the perturbed dataset?",
              "answer": "20 out of 26 models evaluated exhibited a statistically significant drop in performance on the perturbed dataset."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "ACU. According to the study, which family of models showed insignificant performance degradation, suggesting less dependency on superficial cues?",
              "answer": "The Llama family of models showed insignificant performance degradation, suggesting less dependency on superficial cues."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "ACU. What is the name of the statistical test used to confirm that performance differences were not due to chance?",
              "answer": "McNemar's test was used to statistically validate the observed performance differences, confirming that the drops were not due to chance."
            },
            "result": true,
            "issue": ""
          }
        ],
        "judgment": true,
        "sections_to_highlight": [
          "C-BOD method: systematic prompt distortion and performance measurement",
          "Paradoxical relationship: higher accuracy LLMs more sensitive to rephrasing",
          "Llama family: robustness and lower sensitivity to rephrasing",
          "Larger models: increased sensitivity to rephrasing",
          "McNemar's test: statistical validation of performance differences",
          "Average performance degradation: 2.15% across models",
          "Key findings: 20/26 models show significant drops"
        ]
      },
      "correct_count_all": 9,
      "correct_count_acu": 4,
      "num_of_questions": 5,
      "sections_to_highlight": [
        "Llama family — robustness and lower accuracy",
        "Larger models: increased sensitivity to rephrasing",
        "McNemar's test for statistical significance",
        "Llama family: robustness and lower accuracy",
        "Relationship between model size and overfitting",
        "C-BOD method overview — systematic prompt distortion",
        "Number of models with significant drops — 20/26",
        "Paradoxical relationship: higher accuracy LLMs more sensitive to rephrasing",
        "C-BOD method: systematic prompt distortion and performance measurement",
        "Average performance degradation percentage",
        "Average accuracy drop — 2.15% across models",
        "Average accuracy drop across models",
        "Llama family's robustness to prompt rephrasing",
        "Model size vs. sensitivity to rephrasing",
        "Key findings on performance degradation",
        "C-BOD method — systematic prompt distortion and performance measurement",
        "Key findings — 20/26 models show significant drops, average 2.15% degradation",
        "Larger models — sensitivity to rephrasing",
        "Average performance degradation: 2.15% across 26 models",
        "Examples of prompt rephrasing impact (Table 3)",
        "Figure 5 — relationship between original accuracy and performance decline",
        "Paradoxical relationship — accuracy vs. performance decline",
        "Statistical significance — McNemar's test",
        "Llama family performance on perturbed data",
        "Number of models with significant performance drops",
        "Relationship between baseline accuracy and performance decline",
        "C-BOD method — prompt distortion and performance measurement",
        "Model size vs. performance difference — larger models are more sensitive",
        "Evidence for dataset-specific cue capitalization",
        "McNemar's test: statistical validation of performance differences",
        "Llama family — robustness to rephrasing",
        "Paradoxical relationship — higher accuracy LLMs are more sensitive to rephrasing",
        "Figure 5 — accuracy vs. performance decline connection",
        "20/26 models show statistically significant performance drops",
        "C-BOD method — systematic prompt distortion",
        "Statistical significance — McNemar's test validates findings",
        "Llama family — robustness and lower sensitivity to rephrasing"
      ],
      "sections_to_highlight_size": 37
    }
  ],
  "final_summary": "1. SUMMARY:\nThis paper introduces the Chameleon Benchmark Overfit Detector (C-BOD), a framework designed to expose LLMs' over-reliance on dataset-specific surface cues rather than true language understanding. C-BOD systematically distorts benchmark prompts while preserving semantic content and labels, then measures performance changes. The study found that 20 out of 26 leading LLMs exhibited statistically significant performance degradation (an average drop of 2.15%) on these rephrased prompts, indicating overfitting. Crucially, a paradoxical relationship emerged: models with higher baseline accuracy were more sensitive to rephrasing, showing larger performance declines. Conversely, the Llama family of models, often exhibiting lower baseline accuracy, demonstrated greater robustness and less sensitivity to prompt variations. This suggests that high benchmark scores can be misleading, masking a fragility to prompt rephrasing. The paper emphasizes the need to look beyond leaderboard scores and prioritize resilience in LLM evaluation.\n\n2. KEY HIGHLIGHTS:\n*   20 out of 26 LLMs showed statistically significant performance drops (avg. 2.15%) when prompts were systematically rephrased, indicating overfitting to benchmark-specific cues.\n*   A paradoxical relationship was observed: LLMs with higher baseline accuracy were more sensitive to rephrasing, experiencing larger performance declines.\n*   The Llama family of models demonstrated robustness to prompt rephrasing, often correlating with lower baseline accuracy.\n*   Larger models generally exhibited increased sensitivity to rephrasing, suggesting a trend where scale can amplify overfitting.\n*   McNemar's test was used to statistically validate the observed performance differences, confirming that the drops were not due to chance.",
  "total_iterations": 10,
  "status": "completed",
  "questions": [
    "According to the article, what is the paradoxical relationship observed between an LLM's baseline accuracy on a standard benchmark and its performance on perturbed, rephrased inputs?",
    "How does the Chameleon Benchmark Overfit Detector (C-BOD) method work to expose the fragility of high-scoring LLMs?",
    "Figure 5 illustrates a key finding. What does it show about the connection between a model's original accuracy and the performance decline it experiences?",
    "The article states that \"models that perform exceptionally well on standard benchmarks may be capitalizing on dataset-specific cues.\" What evidence or examples are provided to support this claim?",
    "Why do larger models, which often achieve higher accuracy, tend to be more sensitive to the rephrasing of prompts according to the study's findings?"
  ],
  "acu_questions": [
    "ACU. What was the average drop in accuracy across all 26 evaluated models when tested with rephrased prompts?",
    "ACU. How many of the 26 models evaluated exhibited a statistically significant drop in performance on the perturbed dataset?",
    "ACU. According to the study, which family of models showed insignificant performance degradation, suggesting less dependency on superficial cues?",
    "ACU. What is the name of the statistical test used to confirm that performance differences were not due to chance?"
  ]
}