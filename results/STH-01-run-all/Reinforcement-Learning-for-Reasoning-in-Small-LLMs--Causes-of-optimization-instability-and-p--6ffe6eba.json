{
  "query": "Causes of optimization instability and performance degradation during RL fine-tuning of small language models.",
  "max_iterations": 15,
  "iterations": [
    {
      "iteration_number": 1,
      "summary": "1. SUMMARY:\nThis study investigates the effectiveness of reinforcement learning (RL) for enhancing the reasoning capabilities of small language models (LLMs) under strict resource constraints. The researchers fine-tuned a 1.5-billion-parameter model, DeepSeek-R1-Distill-Qwen-1.5B, using the Group Relative Policy Optimization (GRPO) algorithm and a curated dataset of mathematical reasoning problems. They observed rapid initial reasoning gains, with accuracy on AMC23 increasing from 63% to 80% and AIME24 reaching 46.7%, significantly outperforming baseline models at a fraction of the cost ($42 vs. thousands). However, prolonged training revealed optimization instability and performance degradation. Specifically, after initial improvements (around 50-100 steps), accuracy declined, and completion lengths became erratic, sometimes leading to non-English outputs. This degradation was attributed to issues like reward misalignment, exceeding length constraints, and the base model's multilingual nature. The study highlights that while small LLMs can benefit greatly from RL with limited data and compute, long-term stability and handling of complex, lengthy reasoning remain challenges.\n\n2. KEY HIGHLIGHTS:\n*   Small LLMs can achieve significant reasoning improvements (e.g., AMC23 accuracy from 63% to 80%) via RL fine-tuning with limited data and computational resources.\n*   Prolonged RL fine-tuning beyond an initial phase (50-100 steps) can lead to optimization instability and performance degradation in small LLMs.\n*   Key issues contributing to instability include reward misalignment, exceeding maximum completion token limits, and the inherent multilingual nature of the base model.\n*   Techniques like using a mix of easy and hard problems and employing cosine rewards can help stabilize training and control output length, though they don't fully resolve long-term degradation or multilingual drift.\n*   The study demonstrates a cost-effective approach ($42) that rivals larger models for specific reasoning tasks, but acknowledges limitations in generalization and handling extremely complex, lengthy reasoning processes.",
      "qa_pairs": [
        {
          "question": "In Experiment 1, what were the two primary symptoms of performance degradation that occurred after 200 global training steps?",
          "answer": "Not enough information in summary"
        },
        {
          "question": "What does the article suggest is the main reason for the performance degradation in Experiment 1, related to the dataset's complexity and a specific training constraint?",
          "answer": "Not enough information in summary"
        },
        {
          "question": "How did the multilingual nature of the base model, DeepSeek-R1-Distill-Qwen-1.5B, contribute to optimization instability in later training stages?",
          "answer": "The base model's multilingual nature was identified as a key issue contributing to instability in later training stages."
        },
        {
          "question": "In Experiment 2, what evidence pointed to optimization instability after the initial performance improvement, as shown in Figure 4?",
          "answer": "Not enough information in summary"
        },
        {
          "question": "According to the discussion, what is the key trade-off identified when using a cosine reward to stabilize training, as seen in Experiment 3?",
          "answer": "Not enough information in summary"
        },
        {
          "question": "ACU. After how many global training steps did the model's performance start to degrade significantly in Experiment 1?",
          "answer": "Performance degradation was observed after initial improvements around 50-100 steps."
        },
        {
          "question": "ACU. What was the maximum token limit for completion length that the model struggled with in Experiment 1?",
          "answer": "The summary mentions that completion lengths became erratic and sometimes exceeded limits, but does not specify a maximum token limit."
        },
        {
          "question": "ACU. In Experiment 2, after how many global steps did the KL divergence become unstable, leading to performance decline?",
          "answer": "Not enough information in summary"
        },
        {
          "question": "ACU. What type of reward was introduced in Experiment 3 specifically to stabilize completion lengths?",
          "answer": "Cosine rewards were employed to stabilize training and control output length."
        }
      ],
      "judge": {
        "evaluations": [
          {
            "qa": {
              "question": "In Experiment 1, what were the two primary symptoms of performance degradation that occurred after 200 global training steps?",
              "answer": "In Experiment 1, after 200 global training steps, accuracy degraded significantly, dropping below 60% on AMC23 and to 80% on MATH-500. Additionally, completion lengths increased again, accompanied by unreadable content and non-English outputs."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "What does the article suggest is the main reason for the performance degradation in Experiment 1, related to the dataset's complexity and a specific training constraint?",
              "answer": "The article suggests that the model struggled with the complexity of the open-s1 dataset, often exceeding the 4096-token limit before producing a final answer, which contributed to performance degradation."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "How did the multilingual nature of the base model, DeepSeek-R1-Distill-Qwen-1.5B, contribute to optimization instability in later training stages?",
              "answer": "The base model's multilingual nature was identified as a key issue contributing to instability in later training stages, leading to mixed-language outputs."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "In Experiment 2, what evidence pointed to optimization instability after the initial performance improvement, as shown in Figure 4?",
              "answer": "In Experiment 2, after 150-200 steps, KL divergence became unstable, and performance declined, with mixed-language outputs reemerging, as shown in Figure 4."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "According to the discussion, what is the key trade-off identified when using a cosine reward to stabilize training, as seen in Experiment 3?",
              "answer": "According to the discussion, the key trade-off when using a cosine reward to stabilize training is that it sacrifices peak accuracy compared to Experiment 2, though it improves training consistency."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "ACU. After how many global training steps did the model's performance start to degrade significantly in Experiment 1?",
              "answer": "In Experiment 1, performance degradation was observed after 200 global training steps."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "ACU. What was the maximum token limit for completion length that the model struggled with in Experiment 1?",
              "answer": "In Experiment 1, the model struggled with the 4096-token limit for completion length."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "ACU. In Experiment 2, after how many global steps did the KL divergence become unstable, leading to performance decline?",
              "answer": "In Experiment 2, KL divergence became unstable after 150-200 steps."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "ACU. What type of reward was introduced in Experiment 3 specifically to stabilize completion lengths?",
              "answer": "In Experiment 3, a cosine reward was introduced to stabilize completion lengths."
            },
            "result": true,
            "issue": ""
          }
        ],
        "judgment": true,
        "sections_to_highlight": [
          "Experiment 1 — performance degradation symptoms",
          "Experiment 1 — reason for degradation",
          "Multilingual nature — impact on stability",
          "Experiment 2 — evidence of instability",
          "Experiment 3 — cosine reward trade-off",
          "Experiment 1 — timing of degradation",
          "Experiment 1 — token limit struggle",
          "Experiment 2 — timing of KL divergence instability",
          "Experiment 3 — reward for length stabilization"
        ]
      },
      "correct_count_all": 9,
      "correct_count_acu": 4,
      "num_of_questions": 5,
      "sections_to_highlight": [],
      "sections_to_highlight_size": 0
    }
  ],
  "final_summary": "1. SUMMARY:\nThis study investigates the effectiveness of reinforcement learning (RL) for enhancing the reasoning capabilities of small language models (LLMs) under strict resource constraints. The researchers fine-tuned a 1.5-billion-parameter model, DeepSeek-R1-Distill-Qwen-1.5B, using the Group Relative Policy Optimization (GRPO) algorithm and a curated dataset of mathematical reasoning problems. They observed rapid initial reasoning gains, with accuracy on AMC23 increasing from 63% to 80% and AIME24 reaching 46.7%, significantly outperforming baseline models at a fraction of the cost ($42 vs. thousands). However, prolonged training revealed optimization instability and performance degradation. Specifically, after initial improvements (around 50-100 steps), accuracy declined, and completion lengths became erratic, sometimes leading to non-English outputs. This degradation was attributed to issues like reward misalignment, exceeding length constraints, and the base model's multilingual nature. The study highlights that while small LLMs can benefit greatly from RL with limited data and compute, long-term stability and handling of complex, lengthy reasoning remain challenges.\n\n2. KEY HIGHLIGHTS:\n*   Small LLMs can achieve significant reasoning improvements (e.g., AMC23 accuracy from 63% to 80%) via RL fine-tuning with limited data and computational resources.\n*   Prolonged RL fine-tuning beyond an initial phase (50-100 steps) can lead to optimization instability and performance degradation in small LLMs.\n*   Key issues contributing to instability include reward misalignment, exceeding maximum completion token limits, and the inherent multilingual nature of the base model.\n*   Techniques like using a mix of easy and hard problems and employing cosine rewards can help stabilize training and control output length, though they don't fully resolve long-term degradation or multilingual drift.\n*   The study demonstrates a cost-effective approach ($42) that rivals larger models for specific reasoning tasks, but acknowledges limitations in generalization and handling extremely complex, lengthy reasoning processes.",
  "total_iterations": 1,
  "status": "completed",
  "questions": [
    "In Experiment 1, what were the two primary symptoms of performance degradation that occurred after 200 global training steps?",
    "What does the article suggest is the main reason for the performance degradation in Experiment 1, related to the dataset's complexity and a specific training constraint?",
    "How did the multilingual nature of the base model, DeepSeek-R1-Distill-Qwen-1.5B, contribute to optimization instability in later training stages?",
    "In Experiment 2, what evidence pointed to optimization instability after the initial performance improvement, as shown in Figure 4?",
    "According to the discussion, what is the key trade-off identified when using a cosine reward to stabilize training, as seen in Experiment 3?"
  ],
  "acu_questions": [
    "ACU. After how many global training steps did the model's performance start to degrade significantly in Experiment 1?",
    "ACU. What was the maximum token limit for completion length that the model struggled with in Experiment 1?",
    "ACU. In Experiment 2, after how many global steps did the KL divergence become unstable, leading to performance decline?",
    "ACU. What type of reward was introduced in Experiment 3 specifically to stabilize completion lengths?"
  ]
}