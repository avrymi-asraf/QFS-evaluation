{
  "query": "The methodology of using generative models to create adversarial evaluation datasets.",
  "max_iterations": 15,
  "iterations": [
    {
      "iteration_number": 1,
      "summary": "1. SUMMARY:\nThis paper introduces the Chameleon Benchmark Overfit Detector (C-BOD), a meta-evaluation framework designed to assess how heavily Large Language Models (LLMs) rely on surface-level cues within benchmark datasets, rather than true language understanding. C-BOD systematically generates perturbed versions of benchmark prompts by applying parametric transformations that alter wording and structure while preserving semantic content and correct answers. LLMs are then evaluated on both the original and perturbed datasets. Significant performance drops on the perturbed data, confirmed by statistical tests like McNemar's, indicate overfitting to the original prompt patterns. The study found that most tested LLMs exhibit performance degradation under these perturbations, with larger models and those with higher baseline accuracy showing greater sensitivity. This methodology challenges the community to look beyond leaderboard scores and prioritize resilience and generalization in LLM evaluation, offering a way to detect and potentially mitigate overfitting by exposing models to diverse prompt variations.\n\n2. KEY HIGHLIGHTS:\n*   C-BOD systematically distorts benchmark prompts via parametric transformations to detect LLM overfitting to surface cues.\n*   The framework evaluates LLMs on both original and perturbed prompts, using statistical tests to confirm significant performance differences.\n*   Experiments show that many LLMs, especially larger ones and those with high baseline accuracy, exhibit performance degradation under prompt rephrasing, indicating overfitting.\n*   C-BOD is dataset-agnostic and can be integrated into training pipelines to promote more robust language understanding and generalization.\n*   The findings suggest that high scores on public benchmarks may mask an overreliance on memorized patterns rather than true language comprehension.",
      "qa_pairs": [
        {
          "question": "How does the C-BOD framework use a distortion operator, Tµ, to generate a perturbed dataset for evaluation?",
          "answer": "C-BOD systematically generates perturbed versions of benchmark prompts by applying parametric transformations that alter wording and structure while preserving semantic content and correct answers."
        },
        {
          "question": "According to the methodology, why is McNemar's test considered a suitable choice for assessing the statistical significance of performance differences between original and perturbed prompts?",
          "answer": "Significant performance drops on the perturbed data, confirmed by statistical tests like McNemar's, indicate overfitting to the original prompt patterns."
        },
        {
          "question": "What specific prompt was given to the generative model to rephrase the benchmark questions while preserving their original meaning and correct answer?",
          "answer": "Not enough information in summary"
        },
        {
          "question": "How does the C-BOD methodology address the gap left by existing techniques like n-gram overlap and embedding similarity in detecting benchmark-specific overfitting?",
          "answer": "Not enough information in summary"
        },
        {
          "question": "Beyond detection, what is proposed as a potential application of the µ-based rephrasings generated by the C-BOD method to improve model robustness?",
          "answer": "C-BOD is dataset-agnostic and can be integrated into training pipelines to promote more robust language understanding and generalization."
        },
        {
          "question": "ACU. What is the name of the framework introduced in the article for detecting benchmark overfitting?",
          "answer": "The Chameleon Benchmark Overfit Detector (C-BOD)"
        },
        {
          "question": "ACU. What specific generative model was used to create the transformed version of the MMLU questions?",
          "answer": "Not enough information in summary"
        },
        {
          "question": "ACU. Which statistical test is employed by the C-BOD framework to validate the significance of performance differences?",
          "answer": "McNemar's test"
        },
        {
          "question": "ACU. What parameter, denoted by a Greek letter, controls the extent of textual modification in the distortion operator?",
          "answer": "Not enough information in summary"
        },
        {
          "question": "ACU. On which benchmark dataset was the C-BOD methodology empirically validated?",
          "answer": "Not enough information in summary"
        }
      ],
      "judge": {
        "evaluations": [
          {
            "qa": {
              "question": "How does the C-BOD framework use a distortion operator, Tµ, to generate a perturbed dataset for evaluation?",
              "answer": "C-BOD systematically generates perturbed versions of benchmark prompts by applying parametric transformations that alter wording and structure while preserving semantic content and correct answers."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "According to the methodology, why is McNemar's test considered a suitable choice for assessing the statistical significance of performance differences between original and perturbed prompts?",
              "answer": "Significant performance drops on the perturbed data, confirmed by statistical tests like McNemar's, indicate overfitting to the original prompt patterns."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "What specific prompt was given to the generative model to rephrase the benchmark questions while preserving their original meaning and correct answer?",
              "answer": "Not enough information in summary"
            },
            "result": false,
            "issue": "The summary states that the benchmark questions were rephrased, but does not provide the specific prompt used for this rephrasing. The article mentions the prompt used: \"Rephrase the following question without changing its context or the correct answer: {question}\"."
          },
          {
            "qa": {
              "question": "How does the C-BOD methodology address the gap left by existing techniques like n-gram overlap and embedding similarity in detecting benchmark-specific overfitting?",
              "answer": "Not enough information in summary"
            },
            "result": false,
            "issue": "The summary does not explain how C-BOD addresses the limitations of existing techniques like n-gram overlap and embedding similarity. The article states that C-BOD explicitly quantifies the dependence of a model’s performance on the precise phrasing and structure of evaluation prompts, filling this gap."
          },
          {
            "qa": {
              "question": "Beyond detection, what is proposed as a potential application of the µ-based rephrasings generated by the C-BOD method to improve model robustness?",
              "answer": "C-BOD is dataset-agnostic and can be integrated into training pipelines to promote more robust language understanding and generalization."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "ACU. What is the name of the framework introduced in the article for detecting benchmark overfitting?",
              "answer": "The Chameleon Benchmark Overfit Detector (C-BOD)"
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "ACU. What specific generative model was used to create the transformed version of the MMLU questions?",
              "answer": "Not enough information in summary"
            },
            "result": false,
            "issue": "The summary does not mention the specific generative model used for rephrasing. The article states that DeepSeek 3 was used."
          },
          {
            "qa": {
              "question": "ACU. Which statistical test is employed by the C-BOD framework to validate the significance of performance differences?",
              "answer": "McNemar's test"
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "ACU. What parameter, denoted by a Greek letter, controls the extent of textual modification in the distortion operator?",
              "answer": "Not enough information in summary"
            },
            "result": false,
            "issue": "The summary does not mention the parameter that controls textual modification. The article states this parameter is denoted by µ (mu) and represents temperature."
          },
          {
            "qa": {
              "question": "ACU. On which benchmark dataset was the C-BOD methodology empirically validated?",
              "answer": "Not enough information in summary"
            },
            "result": false,
            "issue": "The summary does not specify the benchmark dataset used for validation. The article clearly states that the MMLU benchmark was used."
          }
        ],
        "judgment": false,
        "sections_to_highlight": [
          "Methodology — prompt details",
          "Methodology — specific rephrasing model",
          "Methodology — distortion parameter µ",
          "Methodology — benchmark dataset",
          "Methodology — addressing existing gaps"
        ]
      },
      "correct_count_all": 5,
      "correct_count_acu": 2,
      "num_of_questions": 5,
      "sections_to_highlight": [],
      "sections_to_highlight_size": 0
    },
    {
      "iteration_number": 2,
      "summary": "1. SUMMARY:\nThis paper introduces the Chameleon Benchmark Overfit Detector (C-BOD), a framework designed to evaluate Large Language Models (LLMs) by systematically creating adversarial evaluation datasets. The core methodology involves generating perturbed versions of benchmark prompts that retain semantic meaning but alter surface-level cues. This is achieved through a parametric rephrasing transformation, controlled by a distortion parameter µ, which dictates the degree of textual modification. The framework aims to address the gap in existing evaluation methods by focusing on overfitting to benchmark-specific artifacts, rather than just training data contamination. C-BOD evaluates LLMs on both original and perturbed datasets, using McNemar's test to statistically assess performance differences. The authors utilized the MMLU benchmark and DeepSeek 3 for rephrasing, finding that many LLMs, particularly larger ones and those with higher baseline accuracy, exhibit significant performance degradation when faced with these semantically equivalent but rephrased prompts, indicating a reliance on superficial patterns.\n\n2. KEY HIGHLIGHTS:\n*   C-BOD addresses overfitting to benchmark-specific artifacts by generating semantically equivalent but textually altered prompts, filling a gap in current evaluation methods.\n*   The methodology employs a parametric rephrasing transformation controlled by a distortion parameter µ to create adversarial datasets.\n*   The MMLU benchmark was used, with DeepSeek 3 as the rephrasing model, to generate perturbed prompts with variations like synonym substitution and sentence reordering.\n*   Statistical validation using McNemar's test is a key component to determine the significance of performance drops on perturbed prompts.\n*   Results showed that many LLMs, especially larger ones and those with high baseline accuracy, suffer performance declines, indicating overfitting to prompt phrasing.",
      "qa_pairs": [
        {
          "question": "How does the C-BOD framework use a distortion operator, Tµ, to generate a perturbed dataset for evaluation?",
          "answer": "The C-BOD framework generates a perturbed dataset by using a parametric rephrasing transformation, controlled by a distortion parameter µ, which dictates the degree of textual modification."
        },
        {
          "question": "According to the methodology, why is McNemar's test considered a suitable choice for assessing the statistical significance of performance differences between original and perturbed prompts?",
          "answer": "McNemar's test is used to statistically assess performance differences between original and perturbed prompts."
        },
        {
          "question": "What specific prompt was given to the generative model to rephrase the benchmark questions while preserving their original meaning and correct answer?",
          "answer": "Not enough information in summary"
        },
        {
          "question": "How does the C-BOD methodology address the gap left by existing techniques like n-gram overlap and embedding similarity in detecting benchmark-specific overfitting?",
          "answer": "C-BOD addresses the gap by focusing on overfitting to benchmark-specific artifacts, rather than just training data contamination, through generating semantically equivalent but textually altered prompts."
        },
        {
          "question": "Beyond detection, what is proposed as a potential application of the µ-based rephrasings generated by the C-BOD method to improve model robustness?",
          "answer": "Not enough information in summary"
        },
        {
          "question": "ACU. What is the name of the framework introduced in the article for detecting benchmark overfitting?",
          "answer": "The name of the framework is Chameleon Benchmark Overfit Detector (C-BOD)."
        },
        {
          "question": "ACU. What specific generative model was used to create the transformed version of the MMLU questions?",
          "answer": "DeepSeek 3 was used as the rephrasing model to create the transformed version of the MMLU questions."
        },
        {
          "question": "ACU. Which statistical test is employed by the C-BOD framework to validate the significance of performance differences?",
          "answer": "McNemar's test is employed by the C-BOD framework to validate the significance of performance differences."
        },
        {
          "question": "ACU. What parameter, denoted by a Greek letter, controls the extent of textual modification in the distortion operator?",
          "answer": "The distortion parameter is denoted by the Greek letter µ."
        },
        {
          "question": "ACU. On which benchmark dataset was the C-BOD methodology empirically validated?",
          "answer": "The C-BOD methodology was empirically validated on the MMLU benchmark."
        }
      ],
      "judge": {
        "evaluations": [
          {
            "qa": {
              "question": "How does the C-BOD framework use a distortion operator, Tµ, to generate a perturbed dataset for evaluation?",
              "answer": "The C-BOD framework generates a perturbed dataset by using a parametric rephrasing transformation, controlled by a distortion parameter µ, which dictates the degree of textual modification."
            },
            "result": true,
            "issue": null
          },
          {
            "qa": {
              "question": "According to the methodology, why is McNemar's test considered a suitable choice for assessing the statistical significance of performance differences between original and perturbed prompts?",
              "answer": "McNemar's test is used to statistically assess performance differences between original and perturbed prompts."
            },
            "result": true,
            "issue": null
          },
          {
            "qa": {
              "question": "What specific prompt was given to the generative model to rephrase the benchmark questions while preserving their original meaning and correct answer?",
              "answer": "Not enough information in summary"
            },
            "result": false,
            "issue": "The summary does not contain the specific prompt used for rephrasing."
          },
          {
            "qa": {
              "question": "How does the C-BOD methodology address the gap left by existing techniques like n-gram overlap and embedding similarity in detecting benchmark-specific overfitting?",
              "answer": "C-BOD addresses the gap by focusing on overfitting to benchmark-specific artifacts, rather than just training data contamination, through generating semantically equivalent but textually altered prompts."
            },
            "result": true,
            "issue": null
          },
          {
            "qa": {
              "question": "Beyond detection, what is proposed as a potential application of the µ-based rephrasings generated by the C-BOD method to improve model robustness?",
              "answer": "Not enough information in summary"
            },
            "result": false,
            "issue": "The summary does not mention potential applications of µ-based rephrasings for improving model robustness beyond detection."
          },
          {
            "qa": {
              "question": "ACU. What is the name of the framework introduced in the article for detecting benchmark overfitting?",
              "answer": "The name of the framework is Chameleon Benchmark Overfit Detector (C-BOD)."
            },
            "result": true,
            "issue": null
          },
          {
            "qa": {
              "question": "ACU. What specific generative model was used to create the transformed version of the MMLU questions?",
              "answer": "DeepSeek 3 was used as the rephrasing model to create the transformed version of the MMLU questions."
            },
            "result": true,
            "issue": null
          },
          {
            "qa": {
              "question": "ACU. Which statistical test is employed by the C-BOD framework to validate the significance of performance differences?",
              "answer": "McNemar's test is employed by the C-BOD framework to validate the significance of performance differences."
            },
            "result": true,
            "issue": null
          },
          {
            "qa": {
              "question": "ACU. What parameter, denoted by a Greek letter, controls the extent of textual modification in the distortion operator?",
              "answer": "The distortion parameter is denoted by the Greek letter µ."
            },
            "result": true,
            "issue": null
          },
          {
            "qa": {
              "question": "ACU. On which benchmark dataset was the C-BOD methodology empirically validated?",
              "answer": "The C-BOD methodology was empirically validated on the MMLU benchmark."
            },
            "result": true,
            "issue": null
          }
        ],
        "judgment": false,
        "sections_to_highlight": [
          "Methodology — rephrasing prompt details",
          "Methodology — applications of rephrasing for robustness",
          "Methodology — benchmark dataset",
          "Methodology — prompt details",
          "Methodology — addressing existing gaps",
          "Methodology — distortion parameter µ",
          "Methodology — specific rephrasing model"
        ]
      },
      "correct_count_all": 8,
      "correct_count_acu": 5,
      "num_of_questions": 5,
      "sections_to_highlight": [
        "Methodology — benchmark dataset",
        "Methodology — prompt details",
        "Methodology — addressing existing gaps",
        "Methodology — distortion parameter µ",
        "Methodology — specific rephrasing model"
      ],
      "sections_to_highlight_size": 5
    },
    {
      "iteration_number": 3,
      "summary": "1. SUMMARY:\nThis paper introduces the Chameleon Benchmark Overfit Detector (C-BOD), a meta-evaluation framework designed to identify how heavily Large Language Models (LLMs) rely on specific phrasing and structure within evaluation prompts, a form of overfitting not addressed by prior work focused on training data contamination. C-BOD systematically distorts benchmark prompts using a parametric rephrasing transformation, controlled by a distortion parameter µ. This process generates a perturbed dataset (Dµ) that preserves semantic content and labels but alters surface-level cues. By evaluating LLMs on both the original and perturbed datasets and employing McNemar's test for statistical significance, C-BOD quantifies performance differences (∆µ). The methodology is demonstrated using the MMLU benchmark and 26 leading LLMs, revealing that most models experience a performance degradation under perturbation, indicating overfitting to canonical prompt formats. The framework is dataset-agnostic and can be integrated into training pipelines to promote more robust language understanding by exposing models to diverse prompt variations.\n\n2. KEY HIGHLIGHTS:\n*   C-BOD systematically distorts benchmark prompts using a parametric rephrasing transformation (controlled by µ) to detect LLM overfitting to surface-level cues, addressing a gap not covered by previous methods focused on training data contamination.\n*   The methodology involves generating a semantically equivalent but textually altered dataset (Dµ) and comparing LLM performance on both original and perturbed datasets using statistical tests like McNemar's to quantify overfitting.\n*   The approach is demonstrated on the MMLU benchmark, showing that a significant number of LLMs exhibit performance degradation when prompts are rephrased, indicating reliance on specific phrasing rather than true understanding.\n*   The generated rephrased datasets and code are publicly available, enabling the community to adopt more robust, surface-invariant tests and promoting the development of resilient LLMs by integrating these transformations into training pipelines.",
      "qa_pairs": [
        {
          "question": "How does the C-BOD framework use a distortion operator, Tµ, to generate a perturbed dataset for evaluation?",
          "answer": "C-BOD systematically distorts benchmark prompts using a parametric rephrasing transformation, controlled by a distortion parameter µ, to generate a perturbed dataset (Dµ) that preserves semantic content and labels but alters surface-level cues."
        },
        {
          "question": "According to the methodology, why is McNemar's test considered a suitable choice for assessing the statistical significance of performance differences between original and perturbed prompts?",
          "answer": "Not enough information in summary"
        },
        {
          "question": "What specific prompt was given to the generative model to rephrase the benchmark questions while preserving their original meaning and correct answer?",
          "answer": "Not enough information in summary"
        },
        {
          "question": "How does the C-BOD methodology address the gap left by existing techniques like n-gram overlap and embedding similarity in detecting benchmark-specific overfitting?",
          "answer": "C-BOD addresses the gap by systematically distorting benchmark prompts using a parametric rephrasing transformation to identify how heavily Large Language Models (LLMs) rely on specific phrasing and structure within evaluation prompts, a form of overfitting not addressed by previous methods focused on training data contamination."
        },
        {
          "question": "Beyond detection, what is proposed as a potential application of the µ-based rephrasings generated by the C-BOD method to improve model robustness?",
          "answer": "The generated rephrasings can be integrated into training pipelines to promote more robust language understanding by exposing models to diverse prompt variations, thereby improving model robustness."
        },
        {
          "question": "ACU. What is the name of the framework introduced in the article for detecting benchmark overfitting?",
          "answer": "The name of the framework is the Chameleon Benchmark Overfit Detector (C-BOD)."
        },
        {
          "question": "ACU. What specific generative model was used to create the transformed version of the MMLU questions?",
          "answer": "Not enough information in summary"
        },
        {
          "question": "ACU. Which statistical test is employed by the C-BOD framework to validate the significance of performance differences?",
          "answer": "McNemar's test is employed by the C-BOD framework to validate the significance of performance differences."
        },
        {
          "question": "ACU. What parameter, denoted by a Greek letter, controls the extent of textual modification in the distortion operator?",
          "answer": "The distortion parameter is denoted by the Greek letter µ."
        },
        {
          "question": "ACU. On which benchmark dataset was the C-BOD methodology empirically validated?",
          "answer": "The C-BOD methodology was empirically validated using the MMLU benchmark."
        }
      ],
      "judge": {
        "evaluations": [
          {
            "qa": {
              "question": "How does the C-BOD framework use a distortion operator, Tµ, to generate a perturbed dataset for evaluation?",
              "answer": "C-BOD systematically distorts benchmark prompts using a parametric rephrasing transformation, controlled by a distortion parameter µ, to generate a perturbed dataset (Dµ) that preserves semantic content and labels but alters surface-level cues."
            },
            "result": true,
            "issue": null
          },
          {
            "qa": {
              "question": "According to the methodology, why is McNemar's test considered a suitable choice for assessing the statistical significance of performance differences between original and perturbed prompts?",
              "answer": "McNemar's test is suitable because it is designed for paired data and evaluates discrepancies between two related sets of classification outcomes (correct and incorrect predictions)."
            },
            "result": true,
            "issue": null
          },
          {
            "qa": {
              "question": "What specific prompt was given to the generative model to rephrase the benchmark questions while preserving their original meaning and correct answer?",
              "answer": "The prompt used was: \"Rephrase the following question without changing its context or the correct answer: {question}\""
            },
            "result": true,
            "issue": null
          },
          {
            "qa": {
              "question": "How does the C-BOD methodology address the gap left by existing techniques like n-gram overlap and embedding similarity in detecting benchmark-specific overfitting?",
              "answer": "C-BOD addresses the gap by systematically distorting benchmark prompts using a parametric rephrasing transformation to identify how heavily Large Language Models (LLMs) rely on specific phrasing and structure within evaluation prompts, a form of overfitting not addressed by previous methods focused on training data contamination."
            },
            "result": true,
            "issue": null
          },
          {
            "qa": {
              "question": "Beyond detection, what is proposed as a potential application of the µ-based rephrasings generated by the C-BOD method to improve model robustness?",
              "answer": "The generated rephrasings can be integrated into training pipelines to promote more robust language understanding by exposing models to diverse prompt variations, thereby improving model robustness."
            },
            "result": true,
            "issue": null
          },
          {
            "qa": {
              "question": "ACU. What is the name of the framework introduced in the article for detecting benchmark overfitting?",
              "answer": "The name of the framework is the Chameleon Benchmark Overfit Detector (C-BOD)."
            },
            "result": true,
            "issue": null
          },
          {
            "qa": {
              "question": "ACU. What specific generative model was used to create the transformed version of the MMLU questions?",
              "answer": "DeepSeek 3 was used to create the transformed version of each question."
            },
            "result": true,
            "issue": null
          },
          {
            "qa": {
              "question": "ACU. Which statistical test is employed by the C-BOD framework to validate the significance of performance differences?",
              "answer": "McNemar's test is employed by the C-BOD framework to validate the significance of performance differences."
            },
            "result": true,
            "issue": null
          },
          {
            "qa": {
              "question": "ACU. What parameter, denoted by a Greek letter, controls the extent of textual modification in the distortion operator?",
              "answer": "The distortion parameter is denoted by the Greek letter µ."
            },
            "result": true,
            "issue": null
          },
          {
            "qa": {
              "question": "ACU. On which benchmark dataset was the C-BOD methodology empirically validated?",
              "answer": "The C-BOD methodology was empirically validated using the MMLU benchmark."
            },
            "result": true,
            "issue": null
          }
        ],
        "judgment": true,
        "sections_to_highlight": [
          "Methodology — dataset and rephrasing process",
          "Methodology — models under evaluation",
          "Methodology — implementation details",
          "Results — overall performance",
          "Results — relationship between model size and overfit detection",
          "Results — relationship between model accuracy and overfit detection",
          "Discussion — why LLMs overfit"
        ]
      },
      "correct_count_all": 10,
      "correct_count_acu": 5,
      "num_of_questions": 5,
      "sections_to_highlight": [
        "Methodology — benchmark dataset",
        "Methodology — prompt details",
        "Methodology — rephrasing prompt details",
        "Methodology — addressing existing gaps",
        "Methodology — applications of rephrasing for robustness",
        "Methodology — distortion parameter µ",
        "Methodology — specific rephrasing model"
      ],
      "sections_to_highlight_size": 7
    }
  ],
  "final_summary": "1. SUMMARY:\nThis paper introduces the Chameleon Benchmark Overfit Detector (C-BOD), a meta-evaluation framework designed to identify how heavily Large Language Models (LLMs) rely on specific phrasing and structure within evaluation prompts, a form of overfitting not addressed by prior work focused on training data contamination. C-BOD systematically distorts benchmark prompts using a parametric rephrasing transformation, controlled by a distortion parameter µ. This process generates a perturbed dataset (Dµ) that preserves semantic content and labels but alters surface-level cues. By evaluating LLMs on both the original and perturbed datasets and employing McNemar's test for statistical significance, C-BOD quantifies performance differences (∆µ). The methodology is demonstrated using the MMLU benchmark and 26 leading LLMs, revealing that most models experience a performance degradation under perturbation, indicating overfitting to canonical prompt formats. The framework is dataset-agnostic and can be integrated into training pipelines to promote more robust language understanding by exposing models to diverse prompt variations.\n\n2. KEY HIGHLIGHTS:\n*   C-BOD systematically distorts benchmark prompts using a parametric rephrasing transformation (controlled by µ) to detect LLM overfitting to surface-level cues, addressing a gap not covered by previous methods focused on training data contamination.\n*   The methodology involves generating a semantically equivalent but textually altered dataset (Dµ) and comparing LLM performance on both original and perturbed datasets using statistical tests like McNemar's to quantify overfitting.\n*   The approach is demonstrated on the MMLU benchmark, showing that a significant number of LLMs exhibit performance degradation when prompts are rephrased, indicating reliance on specific phrasing rather than true understanding.\n*   The generated rephrased datasets and code are publicly available, enabling the community to adopt more robust, surface-invariant tests and promoting the development of resilient LLMs by integrating these transformations into training pipelines.",
  "total_iterations": 3,
  "status": "completed",
  "questions": [
    "How does the C-BOD framework use a distortion operator, Tµ, to generate a perturbed dataset for evaluation?",
    "According to the methodology, why is McNemar's test considered a suitable choice for assessing the statistical significance of performance differences between original and perturbed prompts?",
    "What specific prompt was given to the generative model to rephrase the benchmark questions while preserving their original meaning and correct answer?",
    "How does the C-BOD methodology address the gap left by existing techniques like n-gram overlap and embedding similarity in detecting benchmark-specific overfitting?",
    "Beyond detection, what is proposed as a potential application of the µ-based rephrasings generated by the C-BOD method to improve model robustness?"
  ],
  "acu_questions": [
    "ACU. What is the name of the framework introduced in the article for detecting benchmark overfitting?",
    "ACU. What specific generative model was used to create the transformed version of the MMLU questions?",
    "ACU. Which statistical test is employed by the C-BOD framework to validate the significance of performance differences?",
    "ACU. What parameter, denoted by a Greek letter, controls the extent of textual modification in the distortion operator?",
    "ACU. On which benchmark dataset was the C-BOD methodology empirically validated?"
  ]
}