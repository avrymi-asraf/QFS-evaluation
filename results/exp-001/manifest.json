{
  "timestamp": "2025-08-16T07:47:33Z",
  "run_name": "exp-001",
  "branch": "main",
  "commit": "4c9f022f108f6cb362d7ea2d3e77cbaab275573b",
  "requests": [
    {
      "article": "/home/avreymi/code/QFS-code-and-evaluation/Query-Focused-Summarization/articals/Attention-Is-All-You-Need.pdf",
      "query": "What are the core innovations of the Transformer architecture and how do they replace recurrence?"
    },
    {
      "article": "/home/avreymi/code/QFS-code-and-evaluation/Query-Focused-Summarization/articals/The Linear Representation Hypothesis.pdf",
      "query": "Summarize the linear representation hypothesis and its implications for neural network interpretability."
    }
  ],
  "max_iterations": null,
  "meta": {}
}