{
  "query": "The methodology of using generative models to create adversarial evaluation datasets.",
  "max_iterations": 10,
  "iterations": [
    {
      "iteration_number": 1,
      "summary": "1. SUMMARY:\nThis paper introduces the Chameleon Benchmark Overfit Detector (C-BOD), a meta-evaluation framework designed to assess how heavily Large Language Models (LLMs) rely on surface-level cues in benchmark datasets, rather than true language understanding. The methodology involves systematically distorting benchmark prompts using a parametric rephrasing transformation (Tµ) to create perturbed datasets (Dµ). By evaluating LLMs on both original and perturbed datasets and employing statistical tests like McNemar's, C-BOD quantifies performance differences (∆µ) to detect overfitting. The framework revealed that a significant number of LLMs, particularly larger ones and those with higher baseline accuracy on benchmarks like MMLU, exhibit performance degradation when prompts are rephrased, indicating a reliance on memorized patterns. Models like Llama, and those with lower baseline accuracy, showed less sensitivity. C-BOD is dataset- and model-agnostic, offering a way to promote more robust LLM evaluation and development by moving beyond leaderboard scores to prioritize resilience and generalization.\n\n2. KEY HIGHLIGHTS:\n*   C-BOD systematically distorts benchmark prompts using parametric rephrasing (Tµ) to create perturbed datasets (Dµ).\n*   Performance differences (∆µ) between original and perturbed datasets, assessed with statistical tests, reveal LLM overfitting to surface-level cues.\n*   Larger LLMs and those with higher baseline accuracy tend to show greater performance degradation under prompt perturbations, suggesting overreliance on fixed patterns.\n*   The framework is dataset- and model-agnostic, enabling its integration into evaluation pipelines to foster more robust LLM development.\n*   Findings challenge the community to prioritize LLM resilience and generalization over high scores on standard benchmarks.",
      "qa_evaluations": [
        {
          "qa": {
            "question": "How does the C-BOD framework use a generative \"rephrasing tool\" and a distortion parameter µ to systematically create perturbed evaluation datasets?",
            "answer": "The C-BOD framework uses a parametric rephrasing transformation (Tµ) to systematically distort benchmark prompts, creating perturbed datasets (Dµ). This process aims to assess how heavily Large Language Models (LLMs) rely on surface-level cues rather than true language understanding."
          },
          "result": true,
          "issue": null
        },
        {
          "qa": {
            "question": "The article claims the C-BOD methodology is \"dataset-agnostic.\" Based on the description, what aspects of the generative process allow it to be applied to benchmarks other than MMLU?",
            "answer": "The summary states that C-BOD is dataset-agnostic and that its methodology involves systematically distorting benchmark prompts using a parametric rephrasing transformation. This suggests the generative process can be applied to various benchmarks as it focuses on prompt distortion rather than dataset specifics."
          },
          "result": true,
          "issue": null
        },
        {
          "qa": {
            "question": "What is the role of McNemar's test within the C-BOD methodology after a generative model has created a perturbed dataset?",
            "answer": "McNemar's test is used within the C-BOD methodology to quantify performance differences (∆µ) between LLM evaluations on original and perturbed datasets, which helps in detecting overfitting."
          },
          "result": true,
          "issue": null
        },
        {
          "qa": {
            "question": "Explain the process and purpose of using the specific query prompt, \"Rephrase the following question without changing its context or the correct answer,\" to guide the generative model in creating the adversarial dataset.",
            "answer": "Not enough information in summary"
          },
          "result": false,
          "issue": "Not enough information"
        },
        {
          "qa": {
            "question": "According to the authors, how does using a generative model to create rephrased datasets fill a gap left by existing methods like n-gram overlap and embedding similarity for detecting overfitting?",
            "answer": "Not enough information in summary"
          },
          "result": false,
          "issue": "Not enough information"
        },
        {
          "qa": {
            "question": "ACU. What is the name of the framework introduced in the article for creating perturbed evaluation datasets?",
            "answer": "The framework introduced in the article for creating perturbed evaluation datasets is called the Chameleon Benchmark Overfit Detector (C-BOD)."
          },
          "result": true,
          "issue": null
        },
        {
          "qa": {
            "question": "ACU. What specific generative model was used to create the transformed version of the MMLU dataset?",
            "answer": "Not enough information in summary"
          },
          "result": false,
          "issue": "Not enough information"
        },
        {
          "qa": {
            "question": "ACU. What is the name of the parameter that controls the extent of textual modification in the distortion operator?",
            "answer": "The parameter that controls the extent of textual modification in the distortion operator is denoted by µ."
          },
          "result": true,
          "issue": null
        },
        {
          "qa": {
            "question": "ACU. What specific benchmark dataset was used to generate a perturbed version for the experiments?",
            "answer": "The MMLU benchmark dataset was used to generate a perturbed version for the experiments."
          },
          "result": true,
          "issue": null
        }
      ],
      "qa_pairs": [
        {
          "question": "How does the C-BOD framework use a generative \"rephrasing tool\" and a distortion parameter µ to systematically create perturbed evaluation datasets?",
          "answer": "The C-BOD framework uses a parametric rephrasing transformation (Tµ) to systematically distort benchmark prompts, creating perturbed datasets (Dµ). This process aims to assess how heavily Large Language Models (LLMs) rely on surface-level cues rather than true language understanding."
        },
        {
          "question": "The article claims the C-BOD methodology is \"dataset-agnostic.\" Based on the description, what aspects of the generative process allow it to be applied to benchmarks other than MMLU?",
          "answer": "The summary states that C-BOD is dataset-agnostic and that its methodology involves systematically distorting benchmark prompts using a parametric rephrasing transformation. This suggests the generative process can be applied to various benchmarks as it focuses on prompt distortion rather than dataset specifics."
        },
        {
          "question": "What is the role of McNemar's test within the C-BOD methodology after a generative model has created a perturbed dataset?",
          "answer": "McNemar's test is used within the C-BOD methodology to quantify performance differences (∆µ) between LLM evaluations on original and perturbed datasets, which helps in detecting overfitting."
        },
        {
          "question": "Explain the process and purpose of using the specific query prompt, \"Rephrase the following question without changing its context or the correct answer,\" to guide the generative model in creating the adversarial dataset.",
          "answer": "Not enough information in summary"
        },
        {
          "question": "According to the authors, how does using a generative model to create rephrased datasets fill a gap left by existing methods like n-gram overlap and embedding similarity for detecting overfitting?",
          "answer": "Not enough information in summary"
        },
        {
          "question": "ACU. What is the name of the framework introduced in the article for creating perturbed evaluation datasets?",
          "answer": "The framework introduced in the article for creating perturbed evaluation datasets is called the Chameleon Benchmark Overfit Detector (C-BOD)."
        },
        {
          "question": "ACU. What specific generative model was used to create the transformed version of the MMLU dataset?",
          "answer": "Not enough information in summary"
        },
        {
          "question": "ACU. What is the name of the parameter that controls the extent of textual modification in the distortion operator?",
          "answer": "The parameter that controls the extent of textual modification in the distortion operator is denoted by µ."
        },
        {
          "question": "ACU. What specific benchmark dataset was used to generate a perturbed version for the experiments?",
          "answer": "The MMLU benchmark dataset was used to generate a perturbed version for the experiments."
        }
      ],
      "judge": {
        "evaluations": [
          {
            "qa": {
              "question": "How does the C-BOD framework use a generative \"rephrasing tool\" and a distortion parameter µ to systematically create perturbed evaluation datasets?",
              "answer": "The C-BOD framework uses a parametric rephrasing transformation (Tµ) to systematically distort benchmark prompts, creating perturbed datasets (Dµ). This process aims to assess how heavily Large Language Models (LLMs) rely on surface-level cues rather than true language understanding."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "The article claims the C-BOD methodology is \"dataset-agnostic.\" Based on the description, what aspects of the generative process allow it to be applied to benchmarks other than MMLU?",
              "answer": "The summary states that C-BOD is dataset-agnostic and that its methodology involves systematically distorting benchmark prompts using a parametric rephrasing transformation. This suggests the generative process can be applied to various benchmarks as it focuses on prompt distortion rather than dataset specifics."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "What is the role of McNemar's test within the C-BOD methodology after a generative model has created a perturbed dataset?",
              "answer": "McNemar's test is used within the C-BOD methodology to quantify performance differences (∆µ) between LLM evaluations on original and perturbed datasets, which helps in detecting overfitting."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "Explain the process and purpose of using the specific query prompt, \"Rephrase the following question without changing its context or the correct answer,\" to guide the generative model in creating the adversarial dataset.",
              "answer": "Not enough information in summary"
            },
            "result": false,
            "issue": "The answer states 'Not enough information in summary' but the article does provide this information. Section 4.3 'Implementation Details' states: '(2) Dataset Rephrasing Prompt: We instruct the rephrasing tool using the following prompt to generate an alternative version of each question while preserving its original meaning and correct answer: “Rephrase the following question without changing its context or the correct answer: {question}”'"
          },
          {
            "qa": {
              "question": "According to the authors, how does using a generative model to create rephrased datasets fill a gap left by existing methods like n-gram overlap and embedding similarity for detecting overfitting?",
              "answer": "Not enough information in summary"
            },
            "result": false,
            "issue": "The answer states 'Not enough information in summary' but the article does provide this information. Section 2.2 'Gap in Current Work' states: 'Existing techniques such as n-gram overlap and embedding similarity fail to capture this subtle form of overfitting. In contrast, our approach explicitly quantifies the dependence of a model’s performance on the precise phrasing and structure of evaluation prompts, thereby filling this gap in current evaluation methodologies.'"
          },
          {
            "qa": {
              "question": "ACU. What is the name of the framework introduced in the article for creating perturbed evaluation datasets?",
              "answer": "The framework introduced in the article for creating perturbed evaluation datasets is called the Chameleon Benchmark Overfit Detector (C-BOD)."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "ACU. What specific generative model was used to create the transformed version of the MMLU dataset?",
              "answer": "Not enough information in summary"
            },
            "result": false,
            "issue": "The answer states 'Not enough information in summary' but the article does provide this information. Section 4.1 'Dataset and Rephrasing Process' states: 'We used DeepSeek 3 to create the transformed version of each question.'"
          },
          {
            "qa": {
              "question": "ACU. What is the name of the parameter that controls the extent of textual modification in the distortion operator?",
              "answer": "The parameter that controls the extent of textual modification in the distortion operator is denoted by µ."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "ACU. What specific benchmark dataset was used to generate a perturbed version for the experiments?",
              "answer": "The MMLU benchmark dataset was used to generate a perturbed version for the experiments."
            },
            "result": true,
            "issue": ""
          }
        ],
        "judgment": false,
        "sections_to_highlight": []
      },
      "correct_count_all": 6,
      "correct_count_acu": 3,
      "num_of_questions": 5,
      "sections_to_highlight": [],
      "sections_to_highlight_size": 0,
      "qa_failures_present": true
    },
    {
      "iteration_number": 2,
      "summary": "1. SUMMARY:\nThis paper introduces the Chameleon Benchmark Overfit Detector (C-BOD), a meta-evaluation framework designed to assess how heavily Large Language Models (LLMs) rely on specific prompt patterns in evaluation datasets, rather than true language understanding. C-BOD systematically distorts benchmark prompts using a parametric rephrasing transformation (controlled by parameter µ) while preserving semantic content and labels. By evaluating LLMs on both original and perturbed datasets, C-BOD quantifies performance degradation to detect overfitting. Experiments on the MMLU benchmark with 26 leading LLMs revealed an average accuracy drop of 2.15% under modest perturbations, with 20 models showing statistically significant performance differences. The study found that larger LLMs and those with higher baseline accuracy tend to be more sensitive to rephrasing, indicating a greater reliance on fixed prompt patterns. Conversely, models like Llama and those with lower baseline accuracy showed less degradation, suggesting reduced dependency on superficial cues. C-BOD's dataset- and model-agnostic design allows for integration into training pipelines to promote more robust language understanding, challenging the community to prioritize resilience over leaderboard scores.\n\n2. KEY HIGHLIGHTS:\n*   C-BOD systematically distorts benchmark prompts (e.g., MMLU) via parametric rephrasing to detect LLM overfitting to surface-level cues.\n*   Experiments showed an average 2.15% accuracy drop across 26 LLMs on rephrased prompts, with 20 models exhibiting statistically significant performance degradation.\n*   Larger LLMs and those with higher initial accuracy were more susceptible to performance drops when prompts were perturbed.\n*   The framework helps identify LLMs that rely on memorized patterns rather than genuine language understanding.\n*   C-BOD is dataset- and model-agnostic, enabling its use in training pipelines to foster more robust LLM generalization.",
      "qa_evaluations": [
        {
          "qa": {
            "question": "How does the C-BOD framework use a generative \"rephrasing tool\" and a distortion parameter µ to systematically create perturbed evaluation datasets?",
            "answer": "The C-BOD framework uses a parametric rephrasing transformation, controlled by parameter µ, to systematically distort benchmark prompts while preserving their semantic content and labels, thereby creating perturbed evaluation datasets."
          },
          "result": true,
          "issue": null
        },
        {
          "qa": {
            "question": "The article claims the C-BOD methodology is \"dataset-agnostic.\" Based on the description, what aspects of the generative process allow it to be applied to benchmarks other than MMLU?",
            "answer": "The summary states that C-BOD's dataset-agnostic design allows for integration into training pipelines to promote more robust language understanding. The generative process involves systematically distorting benchmark prompts via parametric rephrasing, which is a general approach that can be applied to various benchmarks."
          },
          "result": true,
          "issue": null
        },
        {
          "qa": {
            "question": "What is the role of McNemar's test within the C-BOD methodology after a generative model has created a perturbed dataset?",
            "answer": "Not enough information in summary"
          },
          "result": false,
          "issue": "Not enough information"
        },
        {
          "qa": {
            "question": "Explain the process and purpose of using the specific query prompt, \"Rephrase the following question without changing its context or the correct answer,\" to guide the generative model in creating the adversarial dataset.",
            "answer": "Not enough information in summary"
          },
          "result": false,
          "issue": "Not enough information"
        },
        {
          "qa": {
            "question": "According to the authors, how does using a generative model to create rephrased datasets fill a gap left by existing methods like n-gram overlap and embedding similarity for detecting overfitting?",
            "answer": "Not enough information in summary"
          },
          "result": false,
          "issue": "Not enough information"
        },
        {
          "qa": {
            "question": "ACU. What is the name of the framework introduced in the article for creating perturbed evaluation datasets?",
            "answer": "The name of the framework introduced in the article for creating perturbed evaluation datasets is Chameleon Benchmark Overfit Detector (C-BOD)."
          },
          "result": true,
          "issue": null
        },
        {
          "qa": {
            "question": "ACU. What specific generative model was used to create the transformed version of the MMLU dataset?",
            "answer": "Not enough information in summary"
          },
          "result": false,
          "issue": "Not enough information"
        },
        {
          "qa": {
            "question": "ACU. What is the name of the parameter that controls the extent of textual modification in the distortion operator?",
            "answer": "The name of the parameter that controls the extent of textual modification in the distortion operator is µ."
          },
          "result": true,
          "issue": null
        },
        {
          "qa": {
            "question": "ACU. What specific benchmark dataset was used to generate a perturbed version for the experiments?",
            "answer": "The specific benchmark dataset used to generate a perturbed version for the experiments was MMLU."
          },
          "result": true,
          "issue": null
        }
      ],
      "qa_pairs": [
        {
          "question": "How does the C-BOD framework use a generative \"rephrasing tool\" and a distortion parameter µ to systematically create perturbed evaluation datasets?",
          "answer": "The C-BOD framework uses a parametric rephrasing transformation, controlled by parameter µ, to systematically distort benchmark prompts while preserving their semantic content and labels, thereby creating perturbed evaluation datasets."
        },
        {
          "question": "The article claims the C-BOD methodology is \"dataset-agnostic.\" Based on the description, what aspects of the generative process allow it to be applied to benchmarks other than MMLU?",
          "answer": "The summary states that C-BOD's dataset-agnostic design allows for integration into training pipelines to promote more robust language understanding. The generative process involves systematically distorting benchmark prompts via parametric rephrasing, which is a general approach that can be applied to various benchmarks."
        },
        {
          "question": "What is the role of McNemar's test within the C-BOD methodology after a generative model has created a perturbed dataset?",
          "answer": "Not enough information in summary"
        },
        {
          "question": "Explain the process and purpose of using the specific query prompt, \"Rephrase the following question without changing its context or the correct answer,\" to guide the generative model in creating the adversarial dataset.",
          "answer": "Not enough information in summary"
        },
        {
          "question": "According to the authors, how does using a generative model to create rephrased datasets fill a gap left by existing methods like n-gram overlap and embedding similarity for detecting overfitting?",
          "answer": "Not enough information in summary"
        },
        {
          "question": "ACU. What is the name of the framework introduced in the article for creating perturbed evaluation datasets?",
          "answer": "The name of the framework introduced in the article for creating perturbed evaluation datasets is Chameleon Benchmark Overfit Detector (C-BOD)."
        },
        {
          "question": "ACU. What specific generative model was used to create the transformed version of the MMLU dataset?",
          "answer": "Not enough information in summary"
        },
        {
          "question": "ACU. What is the name of the parameter that controls the extent of textual modification in the distortion operator?",
          "answer": "The name of the parameter that controls the extent of textual modification in the distortion operator is µ."
        },
        {
          "question": "ACU. What specific benchmark dataset was used to generate a perturbed version for the experiments?",
          "answer": "The specific benchmark dataset used to generate a perturbed version for the experiments was MMLU."
        }
      ],
      "judge": {
        "evaluations": [
          {
            "qa": {
              "question": "How does the C-BOD framework use a generative \"rephrasing tool\" and a distortion parameter µ to systematically create perturbed evaluation datasets?",
              "answer": "The C-BOD framework uses a parametric rephrasing transformation, controlled by parameter µ, to systematically distort benchmark prompts while preserving their semantic content and labels, thereby creating perturbed evaluation datasets."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "The article claims the C-BOD methodology is \"dataset-agnostic.\" Based on the description, what aspects of the generative process allow it to be applied to benchmarks other than MMLU?",
              "answer": "The summary states that C-BOD's dataset-agnostic design allows for integration into training pipelines to promote more robust language understanding. The generative process involves systematically distorting benchmark prompts via parametric rephrasing, which is a general approach that can be applied to various benchmarks."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "What is the role of McNemar's test within the C-BOD methodology after a generative model has created a perturbed dataset?",
              "answer": "Not enough information in summary"
            },
            "result": false,
            "issue": "The summary does not contain information about the role of McNemar's test."
          },
          {
            "qa": {
              "question": "Explain the process and purpose of using the specific query prompt, \"Rephrase the following question without changing its context or the correct answer,\" to guide the generative model in creating the adversarial dataset.",
              "answer": "Not enough information in summary"
            },
            "result": false,
            "issue": "The summary does not contain information about the specific query prompt used."
          },
          {
            "qa": {
              "question": "According to the authors, how does using a generative model to create rephrased datasets fill a gap left by existing methods like n-gram overlap and embedding similarity for detecting overfitting?",
              "answer": "Not enough information in summary"
            },
            "result": false,
            "issue": "The summary does not contain information comparing C-BOD to n-gram overlap and embedding similarity."
          },
          {
            "qa": {
              "question": "ACU. What is the name of the framework introduced in the article for creating perturbed evaluation datasets?",
              "answer": "The name of the framework introduced in the article for creating perturbed evaluation datasets is Chameleon Benchmark Overfit Detector (C-BOD)."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "ACU. What specific generative model was used to create the transformed version of the MMLU dataset?",
              "answer": "Not enough information in summary"
            },
            "result": false,
            "issue": "The summary does not specify which generative model was used."
          },
          {
            "qa": {
              "question": "ACU. What is the name of the parameter that controls the extent of textual modification in the distortion operator?",
              "answer": "The name of the parameter that controls the extent of textual modification in the distortion operator is µ."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "ACU. What specific benchmark dataset was used to generate a perturbed version for the experiments?",
              "answer": "The specific benchmark dataset used to generate a perturbed version for the experiments was MMLU."
            },
            "result": true,
            "issue": ""
          }
        ],
        "judgment": false,
        "sections_to_highlight": [
          "Introduction",
          "C-BOD Methodology",
          "Experimental Setup",
          "Results",
          "Discussion",
          "Limitations"
        ]
      },
      "correct_count_all": 5,
      "correct_count_acu": 3,
      "num_of_questions": 5,
      "sections_to_highlight": [
        "Introduction",
        "C-BOD Methodology",
        "Experimental Setup",
        "Results",
        "Discussion",
        "Limitations"
      ],
      "sections_to_highlight_size": 6,
      "qa_failures_present": true
    },
    {
      "iteration_number": 3,
      "summary": "1. SUMMARY:\nThis paper introduces the Chameleon Benchmark Overfit Detector (C-BOD), a meta-evaluation framework designed to assess how heavily Large Language Models (LLMs) rely on superficial cues in evaluation datasets, rather than true language understanding. The methodology involves systematically distorting benchmark prompts using a parametric rephrasing transformation (Tµ) that preserves semantic content and labels. By comparing an LLM's performance on original and perturbed datasets, C-BOD quantifies performance degradation, indicating overfitting. McNemar's test is used to ensure the statistical significance of observed differences. The experimental setup utilized the MMLU benchmark and 26 leading LLMs. Results showed an average performance drop of 2.15% under modest perturbations, with 20 out of 26 models exhibiting statistically significant degradation. Notably, larger models and those with higher baseline accuracy tended to be more sensitive to these rephrasings, suggesting a reliance on fixed prompt patterns. The authors discuss that this overfitting stems from LLMs associating specific question formats or keywords with answers instead of generalizing concepts. While C-BOD is dataset-agnostic, limitations include potential oversight of deeper overfitting forms and increased computational costs for iterative mitigation.\n\n2. KEY HIGHLIGHTS:\n*   C-BOD systematically distorts benchmark prompts via parametric rephrasing to detect LLM overreliance on surface cues, preserving semantic content and labels.\n*   The framework evaluates LLMs on both original and perturbed datasets and uses McNemar's test to statistically validate performance differences, indicating overfitting.\n*   Experiments on MMLU with 26 LLMs revealed an average 2.15% performance degradation, with most models showing statistically significant drops, suggesting widespread overfitting to prompt structures.\n*   Larger LLMs and those with higher baseline accuracy were found to be more sensitive to prompt rephrasing, indicating a greater tendency to overfit.\n*   The methodology is dataset-agnostic and can be integrated into training pipelines to promote more robust language understanding.",
      "qa_evaluations": [
        {
          "qa": {
            "question": "How does the C-BOD framework use a generative \"rephrasing tool\" and a distortion parameter µ to systematically create perturbed evaluation datasets?",
            "answer": "The C-BOD framework uses a parametric rephrasing transformation (Tµ) to systematically distort benchmark prompts, creating perturbed evaluation datasets. This process preserves semantic content and labels."
          },
          "result": true,
          "issue": null
        },
        {
          "qa": {
            "question": "The article claims the C-BOD methodology is \"dataset-agnostic.\" Based on the description, what aspects of the generative process allow it to be applied to benchmarks other than MMLU?",
            "answer": "The summary states that the C-BOD methodology is dataset-agnostic, implying the generative process of systematically distorting prompts via parametric rephrasing can be applied to benchmarks other than MMLU. However, specific aspects of the generative process that enable this are not detailed."
          },
          "result": false,
          "issue": "Not enough information in summary"
        },
        {
          "qa": {
            "question": "What is the role of McNemar's test within the C-BOD methodology after a generative model has created a perturbed dataset?",
            "answer": "McNemar's test is used within the C-BOD methodology to ensure the statistical significance of observed differences in LLM performance between original and perturbed datasets."
          },
          "result": true,
          "issue": null
        },
        {
          "qa": {
            "question": "Explain the process and purpose of using the specific query prompt, \"Rephrase the following question without changing its context or the correct answer,\" to guide the generative model in creating the adversarial dataset.",
            "answer": "Not enough information in summary"
          },
          "result": false,
          "issue": "Not enough information"
        },
        {
          "qa": {
            "question": "According to the authors, how does using a generative model to create rephrased datasets fill a gap left by existing methods like n-gram overlap and embedding similarity for detecting overfitting?",
            "answer": "The summary states that the authors discuss that overfitting stems from LLMs associating specific question formats or keywords with answers instead of generalizing concepts. It also mentions that C-BOD detects LLM overreliance on superficial cues, but it does not explain how generative models fill a gap left by n-gram overlap and embedding similarity for detecting overfitting."
          },
          "result": false,
          "issue": "Not enough information in summary"
        },
        {
          "qa": {
            "question": "ACU. What is the name of the framework introduced in the article for creating perturbed evaluation datasets?",
            "answer": "The name of the framework introduced in the article for creating perturbed evaluation datasets is the Chameleon Benchmark Overfit Detector (C-BOD)."
          },
          "result": true,
          "issue": null
        },
        {
          "qa": {
            "question": "ACU. What specific generative model was used to create the transformed version of the MMLU dataset?",
            "answer": "Not enough information in summary"
          },
          "result": false,
          "issue": "Not enough information"
        },
        {
          "qa": {
            "question": "ACU. What is the name of the parameter that controls the extent of textual modification in the distortion operator?",
            "answer": "The name of the parameter that controls the extent of textual modification in the distortion operator is µ."
          },
          "result": true,
          "issue": null
        },
        {
          "qa": {
            "question": "ACU. What specific benchmark dataset was used to generate a perturbed version for the experiments?",
            "answer": "The specific benchmark dataset used to generate a perturbed version for the experiments was MMLU."
          },
          "result": true,
          "issue": null
        }
      ],
      "qa_pairs": [
        {
          "question": "How does the C-BOD framework use a generative \"rephrasing tool\" and a distortion parameter µ to systematically create perturbed evaluation datasets?",
          "answer": "The C-BOD framework uses a parametric rephrasing transformation (Tµ) to systematically distort benchmark prompts, creating perturbed evaluation datasets. This process preserves semantic content and labels."
        },
        {
          "question": "The article claims the C-BOD methodology is \"dataset-agnostic.\" Based on the description, what aspects of the generative process allow it to be applied to benchmarks other than MMLU?",
          "answer": "The summary states that the C-BOD methodology is dataset-agnostic, implying the generative process of systematically distorting prompts via parametric rephrasing can be applied to benchmarks other than MMLU. However, specific aspects of the generative process that enable this are not detailed."
        },
        {
          "question": "What is the role of McNemar's test within the C-BOD methodology after a generative model has created a perturbed dataset?",
          "answer": "McNemar's test is used within the C-BOD methodology to ensure the statistical significance of observed differences in LLM performance between original and perturbed datasets."
        },
        {
          "question": "Explain the process and purpose of using the specific query prompt, \"Rephrase the following question without changing its context or the correct answer,\" to guide the generative model in creating the adversarial dataset.",
          "answer": "Not enough information in summary"
        },
        {
          "question": "According to the authors, how does using a generative model to create rephrased datasets fill a gap left by existing methods like n-gram overlap and embedding similarity for detecting overfitting?",
          "answer": "The summary states that the authors discuss that overfitting stems from LLMs associating specific question formats or keywords with answers instead of generalizing concepts. It also mentions that C-BOD detects LLM overreliance on superficial cues, but it does not explain how generative models fill a gap left by n-gram overlap and embedding similarity for detecting overfitting."
        },
        {
          "question": "ACU. What is the name of the framework introduced in the article for creating perturbed evaluation datasets?",
          "answer": "The name of the framework introduced in the article for creating perturbed evaluation datasets is the Chameleon Benchmark Overfit Detector (C-BOD)."
        },
        {
          "question": "ACU. What specific generative model was used to create the transformed version of the MMLU dataset?",
          "answer": "Not enough information in summary"
        },
        {
          "question": "ACU. What is the name of the parameter that controls the extent of textual modification in the distortion operator?",
          "answer": "The name of the parameter that controls the extent of textual modification in the distortion operator is µ."
        },
        {
          "question": "ACU. What specific benchmark dataset was used to generate a perturbed version for the experiments?",
          "answer": "The specific benchmark dataset used to generate a perturbed version for the experiments was MMLU."
        }
      ],
      "judge": {
        "evaluations": [
          {
            "qa": {
              "question": "How does the C-BOD framework use a generative \"rephrasing tool\" and a distortion parameter µ to systematically create perturbed evaluation datasets?",
              "answer": "The C-BOD framework uses a parametric rephrasing transformation (Tµ) to systematically distort benchmark prompts, creating perturbed evaluation datasets. This process preserves semantic content and labels."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "The article claims the C-BOD methodology is \"dataset-agnostic.\" Based on the description, what aspects of the generative process allow it to be applied to benchmarks other than MMLU?",
              "answer": "The summary states that the C-BOD methodology is dataset-agnostic, implying the generative process of systematically distorting prompts via parametric rephrasing can be applied to benchmarks other than MMLU. However, specific aspects of the generative process that enable this are not detailed."
            },
            "result": false,
            "issue": "The article states \"C-BOD’s dataset- and model-agnostic design allows easy integration into training pipelines to promote more robust language understanding.\" and \"Although we used MMLU as a demonstration, our approach is inherently dataset-agnostic. It can be applied to any benchmark by simply adapting the performance metric used to compare the original samples with their rephrased counterparts.\" This indicates the methodology itself is adaptable, not that specific generative process aspects are detailed."
          },
          {
            "qa": {
              "question": "What is the role of McNemar's test within the C-BOD methodology after a generative model has created a perturbed dataset?",
              "answer": "McNemar's test is used within the C-BOD methodology to ensure the statistical significance of observed differences in LLM performance between original and perturbed datasets."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "Explain the process and purpose of using the specific query prompt, \"Rephrase the following question without changing its context or the correct answer,\" to guide the generative model in creating the adversarial dataset.",
              "answer": "Not enough information in summary"
            },
            "result": false,
            "issue": "The article provides the exact prompt used: \"Rephrase the following question without changing its context or the correct answer: {question}\" and explains its purpose is to guide the rephrasing tool to generate alternative versions of each question while preserving meaning and the correct answer."
          },
          {
            "qa": {
              "question": "According to the authors, how does using a generative model to create rephrased datasets fill a gap left by existing methods like n-gram overlap and embedding similarity for detecting overfitting?",
              "answer": "The summary states that the authors discuss that overfitting stems from LLMs associating specific question formats or keywords with answers instead of generalizing concepts. It also mentions that C-BOD detects LLM overreliance on superficial cues, but it does not explain how generative models fill a gap left by n-gram overlap and embedding similarity for detecting overfitting."
            },
            "result": false,
            "issue": "The article explicitly states: \"Existing techniques such as n-gram overlap and embedding similarity fail to capture this subtle form of overfitting. In contrast, our approach explicitly quantifies the dependence of a model’s performance on the precise phrasing and structure of evaluation prompts, thereby filling this gap in current evaluation methodologies.\""
          },
          {
            "qa": {
              "question": "ACU. What is the name of the framework introduced in the article for creating perturbed evaluation datasets?",
              "answer": "The name of the framework introduced in the article for creating perturbed evaluation datasets is the Chameleon Benchmark Overfit Detector (C-BOD)."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "ACU. What specific generative model was used to create the transformed version of the MMLU dataset?",
              "answer": "Not enough information in summary"
            },
            "result": false,
            "issue": "The article states: \"We used DeepSeek 3 to create the transformed version of each question.\""
          },
          {
            "qa": {
              "question": "ACU. What is the name of the parameter that controls the extent of textual modification in the distortion operator?",
              "answer": "The name of the parameter that controls the extent of textual modification in the distortion operator is µ."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "ACU. What specific benchmark dataset was used to generate a perturbed version for the experiments?",
              "answer": "The specific benchmark dataset used to generate a perturbed version for the experiments was MMLU."
            },
            "result": true,
            "issue": ""
          }
        ],
        "judgment": false,
        "sections_to_highlight": [
          "C-BOD Framework Overview",
          "Methodology: Rephrased Dataset Generation",
          "McNemar's Test for Significance",
          "Dataset-Agnostic Design",
          "Experimental Setup: Models and Metrics",
          "Results: Performance Degradation",
          "Relationship: Model Size vs. Overfitting",
          "Relationship: Accuracy vs. Overfitting",
          "Discussion: Why LLMs Overfit",
          "Limitations of C-BOD"
        ]
      },
      "correct_count_all": 5,
      "correct_count_acu": 3,
      "num_of_questions": 5,
      "sections_to_highlight": [
        "C-BOD Framework Overview",
        "Methodology: Rephrased Dataset Generation",
        "McNemar's Test for Significance",
        "Dataset-Agnostic Design",
        "Experimental Setup: Models and Metrics",
        "Results: Performance Degradation",
        "Relationship: Model Size vs. Overfitting",
        "Relationship: Accuracy vs. Overfitting",
        "Discussion: Why LLMs Overfit",
        "Limitations of C-BOD"
      ],
      "sections_to_highlight_size": 10,
      "qa_failures_present": true
    },
    {
      "iteration_number": 4,
      "summary": "1. SUMMARY:\nThis paper introduces the Chameleon Benchmark Overfit Detector (C-BOD), a meta-evaluation framework designed to assess how heavily Large Language Models (LLMs) rely on dataset-specific surface cues rather than genuine language understanding. The core methodology involves systematically distorting benchmark prompts through parametric rephrasing while preserving semantic content and labels. By evaluating LLMs on both original and perturbed datasets, C-BOD quantifies performance degradation. McNemar's test is employed to statistically validate whether these performance differences are significant, indicating overfitting. The framework's dataset-agnostic design allows for broad applicability. Experiments on the MMLU benchmark with 26 leading LLMs revealed an average performance drop of 2.15% under modest perturbations, with 20 models showing statistically significant degradation. Notably, larger models and those with higher baseline accuracy exhibited greater sensitivity to rephrasing, suggesting a reliance on fixed prompt patterns. The paper discusses that LLMs may overfit due to memorization of specific formats or keywords. While effective for surface-level overfitting, C-BOD has limitations regarding deeper forms of overfitting and potential computational costs for integration into training pipelines.\n\n2. KEY HIGHLIGHTS:\n*   C-BOD systematically distorts benchmark prompts via parametric rephrasing to detect LLM overfitting to surface cues.\n*   McNemar's test is used to statistically validate performance degradation on rephrased prompts, indicating true overfitting.\n*   Experiments showed an average performance drop of 2.15% across 26 LLMs on perturbed MMLU datasets, with larger and higher-accuracy models being more susceptible.\n*   The framework is dataset-agnostic, allowing its application to various benchmarks.\n*   Limitations include potential oversight of deeper overfitting and increased computational cost for iterative mitigation.",
      "qa_evaluations": [
        {
          "qa": {
            "question": "How does the C-BOD framework use a generative ",
            "answer": "The C-BOD framework uses a generative \"rephrasing tool\" to systematically create perturbed evaluation datasets by distorting benchmark prompts through parametric rephrasing while preserving semantic content and labels."
          },
          "result": true,
          "issue": null
        },
        {
          "qa": {
            "question": "The article claims the C-BOD methodology is ",
            "answer": "The summary states that the C-BOD framework's dataset-agnostic design allows for broad applicability to various benchmarks, but it does not specify which aspects of the generative process enable this."
          },
          "result": false,
          "issue": "Not enough information"
        },
        {
          "qa": {
            "question": "What is the role of McNemar's test within the C-BOD methodology after a generative model has created a perturbed dataset?",
            "answer": "McNemar's test is employed within the C-BOD methodology to statistically validate whether the performance differences between the original and perturbed datasets are significant, which indicates overfitting."
          },
          "result": true,
          "issue": null
        },
        {
          "qa": {
            "question": "Explain the process and purpose of using the specific query prompt, ",
            "answer": "The summary does not provide the specific query prompt used to guide the generative model."
          },
          "result": false,
          "issue": "Not enough information"
        },
        {
          "qa": {
            "question": "According to the authors, how does using a generative model to create rephrased datasets fill a gap left by existing methods like n-gram overlap and embedding similarity for detecting overfitting?",
            "answer": "The summary states that the paper discusses that LLMs may overfit due to memorization of specific formats or keywords, and that C-BOD is effective for surface-level overfitting, but it does not explain how generative models fill a gap left by n-gram overlap and embedding similarity."
          },
          "result": false,
          "issue": "Not enough information"
        },
        {
          "qa": {
            "question": "ACU. What is the name of the framework introduced in the article for creating perturbed evaluation datasets?",
            "answer": "The name of the framework introduced in the article for creating perturbed evaluation datasets is the Chameleon Benchmark Overfit Detector (C-BOD)."
          },
          "result": true,
          "issue": null
        },
        {
          "qa": {
            "question": "ACU. What specific generative model was used to create the transformed version of the MMLU dataset?",
            "answer": "The summary does not specify the particular generative model used to create the transformed version of the MMLU dataset."
          },
          "result": false,
          "issue": "Not enough information"
        },
        {
          "qa": {
            "question": "ACU. What is the name of the parameter that controls the extent of textual modification in the distortion operator?",
            "answer": "The summary mentions a \"distortion parameter µ\" that controls the extent of textual modification, but does not explicitly state it is a parameter of a \"distortion operator.\""
          },
          "result": false,
          "issue": "partial information"
        },
        {
          "qa": {
            "question": "ACU. What specific benchmark dataset was used to generate a perturbed version for the experiments?",
            "answer": "The specific benchmark dataset used to generate a perturbed version for the experiments was MMLU."
          },
          "result": true,
          "issue": null
        }
      ],
      "qa_pairs": [
        {
          "question": "How does the C-BOD framework use a generative ",
          "answer": "The C-BOD framework uses a generative \"rephrasing tool\" to systematically create perturbed evaluation datasets by distorting benchmark prompts through parametric rephrasing while preserving semantic content and labels."
        },
        {
          "question": "The article claims the C-BOD methodology is ",
          "answer": "The summary states that the C-BOD framework's dataset-agnostic design allows for broad applicability to various benchmarks, but it does not specify which aspects of the generative process enable this."
        },
        {
          "question": "What is the role of McNemar's test within the C-BOD methodology after a generative model has created a perturbed dataset?",
          "answer": "McNemar's test is employed within the C-BOD methodology to statistically validate whether the performance differences between the original and perturbed datasets are significant, which indicates overfitting."
        },
        {
          "question": "Explain the process and purpose of using the specific query prompt, ",
          "answer": "The summary does not provide the specific query prompt used to guide the generative model."
        },
        {
          "question": "According to the authors, how does using a generative model to create rephrased datasets fill a gap left by existing methods like n-gram overlap and embedding similarity for detecting overfitting?",
          "answer": "The summary states that the paper discusses that LLMs may overfit due to memorization of specific formats or keywords, and that C-BOD is effective for surface-level overfitting, but it does not explain how generative models fill a gap left by n-gram overlap and embedding similarity."
        },
        {
          "question": "ACU. What is the name of the framework introduced in the article for creating perturbed evaluation datasets?",
          "answer": "The name of the framework introduced in the article for creating perturbed evaluation datasets is the Chameleon Benchmark Overfit Detector (C-BOD)."
        },
        {
          "question": "ACU. What specific generative model was used to create the transformed version of the MMLU dataset?",
          "answer": "The summary does not specify the particular generative model used to create the transformed version of the MMLU dataset."
        },
        {
          "question": "ACU. What is the name of the parameter that controls the extent of textual modification in the distortion operator?",
          "answer": "The summary mentions a \"distortion parameter µ\" that controls the extent of textual modification, but does not explicitly state it is a parameter of a \"distortion operator.\""
        },
        {
          "question": "ACU. What specific benchmark dataset was used to generate a perturbed version for the experiments?",
          "answer": "The specific benchmark dataset used to generate a perturbed version for the experiments was MMLU."
        }
      ],
      "judge": {
        "evaluations": [
          {
            "qa": {
              "question": "How does the C-BOD framework use a generative ",
              "answer": "The C-BOD framework uses a generative \"rephrasing tool\" to systematically create perturbed evaluation datasets by distorting benchmark prompts through parametric rephrasing while preserving semantic content and labels."
            },
            "result": false,
            "issue": "The answer incorrectly states the C-BOD framework uses a \"generative rephrasing tool\". The article refers to a \"rephrasing tool, denoted as T, which uses as a distortion operator\" and mentions using DeepSeek 3 to create the transformed version, but does not explicitly label the tool as generative."
          },
          {
            "qa": {
              "question": "The article claims the C-BOD methodology is ",
              "answer": "The summary states that the C-BOD framework's dataset-agnostic design allows for broad applicability to various benchmarks, but it does not specify which aspects of the generative process enable this."
            },
            "result": false,
            "issue": "The question is incomplete and therefore cannot be evaluated against the article. The provided answer also refers to a 'generative process' which is not clearly defined in this context."
          },
          {
            "qa": {
              "question": "What is the role of McNemar's test within the C-BOD methodology after a generative model has created a perturbed dataset?",
              "answer": "McNemar's test is employed within the C-BOD methodology to statistically validate whether the performance differences between the original and perturbed datasets are significant, which indicates overfitting."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "Explain the process and purpose of using the specific query prompt, ",
              "answer": "The summary does not provide the specific query prompt used to guide the generative model."
            },
            "result": false,
            "issue": "The question is incomplete and cannot be answered. The article does provide a query prompt in section 4.3 Implementation Details."
          },
          {
            "qa": {
              "question": "According to the authors, how does using a generative model to create rephrased datasets fill a gap left by existing methods like n-gram overlap and embedding similarity for detecting overfitting?",
              "answer": "The summary states that the paper discusses that LLMs may overfit due to memorization of specific formats or keywords, and that C-BOD is effective for surface-level overfitting, but it does not explain how generative models fill a gap left by n-gram overlap and embedding similarity."
            },
            "result": false,
            "issue": "The answer incorrectly assumes the article states a 'generative model' is used to create rephrased datasets. The article states that the C-BOD approach explicitly quantifies the dependence of a model’s performance on the precise phrasing and structure of evaluation prompts, thereby filling this gap."
          },
          {
            "qa": {
              "question": "ACU. What is the name of the framework introduced in the article for creating perturbed evaluation datasets?",
              "answer": "The name of the framework introduced in the article for creating perturbed evaluation datasets is the Chameleon Benchmark Overfit Detector (C-BOD)."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "ACU. What specific generative model was used to create the transformed version of the MMLU dataset?",
              "answer": "The summary does not specify the particular generative model used to create the transformed version of the MMLU dataset."
            },
            "result": false,
            "issue": "The article states that DeepSeek 3 was used to create the transformed version of each question, not that a 'generative model' was used. The summary should reflect this accurately."
          },
          {
            "qa": {
              "question": "ACU. What is the name of the parameter that controls the extent of textual modification in the distortion operator?",
              "answer": "The summary mentions a \"distortion parameter µ\" that controls the extent of textual modification, but does not explicitly state it is a parameter of a \"distortion operator.\""
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "ACU. What specific benchmark dataset was used to generate a perturbed version for the experiments?",
              "answer": "The specific benchmark dataset used to generate a perturbed version for the experiments was MMLU."
            },
            "result": true,
            "issue": ""
          }
        ],
        "judgment": false,
        "sections_to_highlight": [
          "C-BOD rephrased dataset generation",
          "Evaluating the Impact of Distortion",
          "Statistical Validation",
          "Dataset and Rephrasing Process",
          "Models Under Evaluation",
          "Relationship Between Model Size and Overfit Detection",
          "Relationship Between Model Accuracy and Overfit Detection"
        ]
      },
      "correct_count_all": 4,
      "correct_count_acu": 3,
      "num_of_questions": 5,
      "sections_to_highlight": [
        "C-BOD rephrased dataset generation",
        "Evaluating the Impact of Distortion",
        "Statistical Validation",
        "Dataset and Rephrasing Process",
        "Models Under Evaluation",
        "Relationship Between Model Size and Overfit Detection",
        "Relationship Between Model Accuracy and Overfit Detection"
      ],
      "sections_to_highlight_size": 7,
      "qa_failures_present": true
    },
    {
      "iteration_number": 5,
      "summary": "1. SUMMARY:\nThis paper introduces the Chameleon Benchmark Overfit Detector (C-BOD), a meta-evaluation framework designed to identify overfitting in Large Language Models (LLMs) by systematically distorting benchmark prompts while preserving their semantic content and labels. The core of C-BOD involves generating rephrased versions of evaluation datasets using a parameterized transformation operator (Tµ), where µ controls the extent of textual modification. These perturbed datasets (Dµ) are then used alongside the original datasets (D) to evaluate LLMs. The framework assesses the impact of these distortions by measuring performance differences (∆µ) and employing statistical validation, specifically McNemar's test, to determine if observed performance drops are statistically significant, indicating overfitting to surface-level cues. The method was empirically validated on the MMLU benchmark using 26 leading LLMs. Results show an average performance degradation of 2.15% under modest perturbations, with larger models and those with higher baseline accuracy exhibiting greater sensitivity, suggesting an overreliance on fixed prompt patterns. Conversely, models like Llama demonstrated greater robustness. C-BOD's dataset and model-agnostic design allows for integration into training pipelines to promote more generalizable language understanding, challenging the community to look beyond leaderboard scores.\n\n2. KEY HIGHLIGHTS:\n*   C-BOD systematically distorts benchmark prompts using a parameterized rephrasing operator (Tµ) to create perturbed datasets (Dµ) that maintain semantic meaning but alter surface-level cues.\n*   Performance differences between original (D) and perturbed (Dµ) datasets are evaluated, and McNemar's test is used for statistical validation to detect significant performance drops indicative of overfitting.\n*   Experiments on MMLU revealed that most LLMs (20/26) experienced performance degradation under rephrasing, with larger models and those with higher baseline accuracy showing greater susceptibility to overfitting.\n*   The framework is dataset and model-agnostic, enabling its application across various benchmarks and LLMs to promote robust language understanding and identify vulnerabilities to prompt variations.",
      "qa_evaluations": [
        {
          "qa": {
            "question": "How does the C-BOD framework use a generative \"rephrasing tool\" and a distortion parameter µ to systematically create perturbed evaluation datasets?",
            "answer": "The C-BOD framework uses a parameterized transformation operator (Tµ), where µ controls the extent of textual modification, to generate rephrased versions of evaluation datasets. These perturbed datasets (Dµ) are created by systematically distorting benchmark prompts while preserving their semantic content and labels."
          },
          "result": true,
          "issue": null
        },
        {
          "qa": {
            "question": "The article claims the C-BOD methodology is \"dataset-agnostic.\" Based on the description, what aspects of the generative process allow it to be applied to benchmarks other than MMLU?",
            "answer": "The summary states that the C-BOD framework is dataset-agnostic, allowing for its integration into training pipelines to promote more generalizable language understanding. However, it does not specify the aspects of the generative process that enable its application to benchmarks other than MMLU."
          },
          "result": false,
          "issue": "Not enough information in summary"
        },
        {
          "qa": {
            "question": "What is the role of McNemar's test within the C-BOD methodology after a generative model has created a perturbed dataset?",
            "answer": "McNemar's test is used for statistical validation to determine if observed performance drops on perturbed datasets are statistically significant, indicating overfitting to surface-level cues."
          },
          "result": true,
          "issue": null
        },
        {
          "qa": {
            "question": "Explain the process and purpose of using the specific query prompt, \"Rephrase the following question without changing its context or the correct answer,\" to guide the generative model in creating the adversarial dataset.",
            "answer": "Not enough information in summary"
          },
          "result": false,
          "issue": "Not enough information"
        },
        {
          "qa": {
            "question": "According to the authors, how does using a generative model to create rephrased datasets fill a gap left by existing methods like n-gram overlap and embedding similarity for detecting overfitting?",
            "answer": "Not enough information in summary"
          },
          "result": false,
          "issue": "Not enough information"
        },
        {
          "qa": {
            "question": "ACU. What is the name of the framework introduced in the article for creating perturbed evaluation datasets?",
            "answer": "The framework introduced in the article for creating perturbed evaluation datasets is called the Chameleon Benchmark Overfit Detector (C-BOD)."
          },
          "result": true,
          "issue": null
        },
        {
          "qa": {
            "question": "ACU. What specific generative model was used to create the transformed version of the MMLU dataset?",
            "answer": "Not enough information in summary"
          },
          "result": false,
          "issue": "Not enough information"
        },
        {
          "qa": {
            "question": "ACU. What is the name of the parameter that controls the extent of textual modification in the distortion operator?",
            "answer": "The name of the parameter that controls the extent of textual modification in the distortion operator is µ."
          },
          "result": true,
          "issue": null
        },
        {
          "qa": {
            "question": "ACU. What specific benchmark dataset was used to generate a perturbed version for the experiments?",
            "answer": "The specific benchmark dataset used to generate a perturbed version for the experiments was MMLU."
          },
          "result": true,
          "issue": null
        }
      ],
      "qa_pairs": [
        {
          "question": "How does the C-BOD framework use a generative \"rephrasing tool\" and a distortion parameter µ to systematically create perturbed evaluation datasets?",
          "answer": "The C-BOD framework uses a parameterized transformation operator (Tµ), where µ controls the extent of textual modification, to generate rephrased versions of evaluation datasets. These perturbed datasets (Dµ) are created by systematically distorting benchmark prompts while preserving their semantic content and labels."
        },
        {
          "question": "The article claims the C-BOD methodology is \"dataset-agnostic.\" Based on the description, what aspects of the generative process allow it to be applied to benchmarks other than MMLU?",
          "answer": "The summary states that the C-BOD framework is dataset-agnostic, allowing for its integration into training pipelines to promote more generalizable language understanding. However, it does not specify the aspects of the generative process that enable its application to benchmarks other than MMLU."
        },
        {
          "question": "What is the role of McNemar's test within the C-BOD methodology after a generative model has created a perturbed dataset?",
          "answer": "McNemar's test is used for statistical validation to determine if observed performance drops on perturbed datasets are statistically significant, indicating overfitting to surface-level cues."
        },
        {
          "question": "Explain the process and purpose of using the specific query prompt, \"Rephrase the following question without changing its context or the correct answer,\" to guide the generative model in creating the adversarial dataset.",
          "answer": "Not enough information in summary"
        },
        {
          "question": "According to the authors, how does using a generative model to create rephrased datasets fill a gap left by existing methods like n-gram overlap and embedding similarity for detecting overfitting?",
          "answer": "Not enough information in summary"
        },
        {
          "question": "ACU. What is the name of the framework introduced in the article for creating perturbed evaluation datasets?",
          "answer": "The framework introduced in the article for creating perturbed evaluation datasets is called the Chameleon Benchmark Overfit Detector (C-BOD)."
        },
        {
          "question": "ACU. What specific generative model was used to create the transformed version of the MMLU dataset?",
          "answer": "Not enough information in summary"
        },
        {
          "question": "ACU. What is the name of the parameter that controls the extent of textual modification in the distortion operator?",
          "answer": "The name of the parameter that controls the extent of textual modification in the distortion operator is µ."
        },
        {
          "question": "ACU. What specific benchmark dataset was used to generate a perturbed version for the experiments?",
          "answer": "The specific benchmark dataset used to generate a perturbed version for the experiments was MMLU."
        }
      ],
      "judge": {
        "evaluations": [
          {
            "qa": {
              "question": "How does the C-BOD framework use a generative \"rephrasing tool\" and a distortion parameter µ to systematically create perturbed evaluation datasets?",
              "answer": "The C-BOD framework uses a parameterized transformation operator (Tµ), where µ controls the extent of textual modification, to generate rephrased versions of evaluation datasets. These perturbed datasets (Dµ) are created by systematically distorting benchmark prompts while preserving their semantic content and labels."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "The article claims the C-BOD methodology is \"dataset-agnostic.\" Based on the description, what aspects of the generative process allow it to be applied to benchmarks other than MMLU?",
              "answer": "The summary states that the C-BOD framework is dataset-agnostic, allowing for its integration into training pipelines to promote more generalizable language understanding. However, it does not specify the aspects of the generative process that enable its application to benchmarks other than MMLU."
            },
            "result": false,
            "issue": "The article explicitly states in section 7.2 \"Agnosticism to Benchmark Set. Although we used MMLU as a demonstration, our approach is inherently dataset-agnostic. It can be applied to any benchmark by simply adapting the performance metric used to compare the original samples with their rephrased counterparts.\""
          },
          {
            "qa": {
              "question": "What is the role of McNemar's test within the C-BOD methodology after a generative model has created a perturbed dataset?",
              "answer": "McNemar's test is used for statistical validation to determine if observed performance drops on perturbed datasets are statistically significant, indicating overfitting to surface-level cues."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "Explain the process and purpose of using the specific query prompt, \"Rephrase the following question without changing its context or the correct answer,\" to guide the generative model in creating the adversarial dataset.",
              "answer": "Not enough information in summary"
            },
            "result": false,
            "issue": "The article mentions the prompt in section 4.3 \"Implementation Details\" and explains its purpose: \"We instruct the rephrasing tool using the following prompt to generate an alternative version of each question while preserving its original meaning and correct answer: “Rephrase the following ques- tion without changing its context or the correct answer: {question}”\" This is a factual statement about the prompt used."
          },
          {
            "qa": {
              "question": "According to the authors, how does using a generative model to create rephrased datasets fill a gap left by existing methods like n-gram overlap and embedding similarity for detecting overfitting?",
              "answer": "Not enough information in summary"
            },
            "result": false,
            "issue": "Section 2.2 \"Gap in Current Work\" directly addresses this: \"Existing techniques such as n-gram overlap and embedding similarity fail to capture this subtle form of overfitting. In contrast, our approach explicitly quantifies the dependence of a model’s per- formance on the precise phrasing and structure of evaluation prompts, thereby filling this gap in cur- rent evaluation methodologies.\""
          },
          {
            "qa": {
              "question": "ACU. What is the name of the framework introduced in the article for creating perturbed evaluation datasets?",
              "answer": "The framework introduced in the article for creating perturbed evaluation datasets is called the Chameleon Benchmark Overfit Detector (C-BOD)."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "ACU. What specific generative model was used to create the transformed version of the MMLU dataset?",
              "answer": "Not enough information in summary"
            },
            "result": false,
            "issue": "Section 4.1 \"Dataset and Rephrasing Process\" states: \"We used DeepSeek 3 to create the transformed version of each question.\""
          },
          {
            "qa": {
              "question": "ACU. What is the name of the parameter that controls the extent of textual modification in the distortion operator?",
              "answer": "The name of the parameter that controls the extent of textual modification in the distortion operator is µ."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "ACU. What specific benchmark dataset was used to generate a perturbed version for the experiments?",
              "answer": "The specific benchmark dataset used to generate a perturbed version for the experiments was MMLU."
            },
            "result": true,
            "issue": ""
          }
        ],
        "judgment": false,
        "sections_to_highlight": []
      },
      "correct_count_all": 5,
      "correct_count_acu": 3,
      "num_of_questions": 5,
      "sections_to_highlight": [],
      "sections_to_highlight_size": 0,
      "qa_failures_present": true
    },
    {
      "iteration_number": 6,
      "summary": "1. SUMMARY:\nThis paper introduces the Chameleon Benchmark Overfit Detector (C-BOD), a meta-evaluation framework designed to systematically assess whether Large Language Models (LLMs) overfit to specific benchmark datasets by relying on surface-level cues rather than true language understanding. C-BOD operates by creating perturbed versions of benchmark prompts through parametric transformations, preserving semantic content but altering wording and structure. By evaluating LLMs on both original and perturbed datasets and using statistical tests like McNemar's, C-BOD quantifies performance degradation, indicating overfitting. The study found that a significant majority of leading LLMs exhibit performance drops when faced with these rephrased prompts, with larger models and those with higher baseline accuracy being more susceptible. This methodology challenges the reliance on standard leaderboard scores and advocates for prioritizing model resilience and generalization. The framework is dataset-agnostic and can be integrated into training pipelines to foster more robust LLMs.\n\n2. KEY HIGHLIGHTS:\n*   C-BOD is a framework that systematically distorts benchmark prompts using parametric transformations to detect LLM overfitting to surface cues.\n*   The method evaluates LLMs on both original and semantically equivalent but textually altered prompts, using statistical tests to identify significant performance degradation.\n*   The study revealed that most evaluated LLMs, particularly larger ones and those with higher baseline accuracy, exhibit significant performance drops under prompt perturbations, indicating overfitting.\n*   C-BOD is dataset-agnostic and can be used to promote more robust language understanding by identifying vulnerabilities to prompt variations.\n*   The findings suggest that high scores on public benchmarks may not always reflect true generalization, highlighting the need for evaluating LLM resilience.",
      "qa_evaluations": [
        {
          "qa": {
            "question": "How does the C-BOD framework use a generative \"rephrasing tool\" and a distortion parameter µ to systematically create perturbed evaluation datasets?",
            "answer": "The summary states that C-BOD creates perturbed versions of benchmark prompts through parametric transformations, preserving semantic content but altering wording and structure. It mentions a \"distortion parameter µ\" in the question, but the summary does not detail how a \"rephrasing tool\" or the parameter µ are used in this process."
          },
          "result": false,
          "issue": "The summary does not explain the specifics of the \"rephrasing tool\" or how the distortion parameter µ is used in the generative process."
        },
        {
          "qa": {
            "question": "The article claims the C-BOD methodology is \"dataset-agnostic.\" Based on the description, what aspects of the generative process allow it to be applied to benchmarks other than MMLU?",
            "answer": "The summary states that C-BOD creates perturbed versions of benchmark prompts through parametric transformations, preserving semantic content but altering wording and structure. It also mentions that the framework is dataset-agnostic and can be integrated into training pipelines to foster more robust LLMs. However, it does not specify which aspects of the generative process allow it to be applied to other benchmarks."
          },
          "result": false,
          "issue": "The summary mentions C-BOD is dataset-agnostic but does not detail the specific aspects of the generative process that enable its application to benchmarks other than MMLU."
        },
        {
          "qa": {
            "question": "What is the role of McNemar's test within the C-BOD methodology after a generative model has created a perturbed dataset?",
            "answer": "The summary states that C-BOD evaluates LLMs on both original and perturbed datasets and uses statistical tests like McNemar's to quantify performance degradation, indicating overfitting."
          },
          "result": true,
          "issue": null
        },
        {
          "qa": {
            "question": "Explain the process and purpose of using the specific query prompt, \"Rephrase the following question without changing its context or the correct answer,\" to guide the generative model in creating the adversarial dataset.",
            "answer": "The summary does not mention the specific query prompt used to guide the generative model."
          },
          "result": false,
          "issue": "Not enough information in summary"
        },
        {
          "qa": {
            "question": "According to the authors, how does using a generative model to create rephrased datasets fill a gap left by existing methods like n-gram overlap and embedding similarity for detecting overfitting?",
            "answer": "The summary states that C-BOD operates by creating perturbed versions of benchmark prompts through parametric transformations, preserving semantic content but altering wording and structure. It also mentions that this methodology challenges the reliance on standard leaderboard scores and advocates for prioritizing model resilience and generalization. However, it does not explain how this fills a gap left by existing methods like n-gram overlap and embedding similarity."
          },
          "result": false,
          "issue": "The summary does not explain how the generative model addresses the limitations of n-gram overlap and embedding similarity for detecting overfitting."
        },
        {
          "qa": {
            "question": "ACU. What is the name of the framework introduced in the article for creating perturbed evaluation datasets?",
            "answer": "The Chameleon Benchmark Overfit Detector (C-BOD) is the meta-evaluation framework introduced in the article for creating perturbed evaluation datasets."
          },
          "result": true,
          "issue": null
        },
        {
          "qa": {
            "question": "ACU. What specific generative model was used to create the transformed version of the MMLU dataset?",
            "answer": "The summary does not specify which generative model was used to create the transformed version of the MMLU dataset."
          },
          "result": false,
          "issue": "Not enough information in summary"
        },
        {
          "qa": {
            "question": "ACU. What is the name of the parameter that controls the extent of textual modification in the distortion operator?",
            "answer": "The question mentions a distortion parameter µ, but the summary does not explicitly name a parameter that controls the extent of textual modification in the distortion operator."
          },
          "result": false,
          "issue": "The summary does not explicitly name the parameter controlling textual modification, although the question mentions 'distortion parameter µ'."
        },
        {
          "qa": {
            "question": "ACU. What specific benchmark dataset was used to generate a perturbed version for the experiments?",
            "answer": "The summary mentions that the C-BOD framework was used to create perturbed versions of benchmark prompts, and specifically refers to the MMLU dataset in the context of generating a transformed version for experiments."
          },
          "result": true,
          "issue": null
        }
      ],
      "qa_pairs": [
        {
          "question": "How does the C-BOD framework use a generative \"rephrasing tool\" and a distortion parameter µ to systematically create perturbed evaluation datasets?",
          "answer": "The summary states that C-BOD creates perturbed versions of benchmark prompts through parametric transformations, preserving semantic content but altering wording and structure. It mentions a \"distortion parameter µ\" in the question, but the summary does not detail how a \"rephrasing tool\" or the parameter µ are used in this process."
        },
        {
          "question": "The article claims the C-BOD methodology is \"dataset-agnostic.\" Based on the description, what aspects of the generative process allow it to be applied to benchmarks other than MMLU?",
          "answer": "The summary states that C-BOD creates perturbed versions of benchmark prompts through parametric transformations, preserving semantic content but altering wording and structure. It also mentions that the framework is dataset-agnostic and can be integrated into training pipelines to foster more robust LLMs. However, it does not specify which aspects of the generative process allow it to be applied to other benchmarks."
        },
        {
          "question": "What is the role of McNemar's test within the C-BOD methodology after a generative model has created a perturbed dataset?",
          "answer": "The summary states that C-BOD evaluates LLMs on both original and perturbed datasets and uses statistical tests like McNemar's to quantify performance degradation, indicating overfitting."
        },
        {
          "question": "Explain the process and purpose of using the specific query prompt, \"Rephrase the following question without changing its context or the correct answer,\" to guide the generative model in creating the adversarial dataset.",
          "answer": "The summary does not mention the specific query prompt used to guide the generative model."
        },
        {
          "question": "According to the authors, how does using a generative model to create rephrased datasets fill a gap left by existing methods like n-gram overlap and embedding similarity for detecting overfitting?",
          "answer": "The summary states that C-BOD operates by creating perturbed versions of benchmark prompts through parametric transformations, preserving semantic content but altering wording and structure. It also mentions that this methodology challenges the reliance on standard leaderboard scores and advocates for prioritizing model resilience and generalization. However, it does not explain how this fills a gap left by existing methods like n-gram overlap and embedding similarity."
        },
        {
          "question": "ACU. What is the name of the framework introduced in the article for creating perturbed evaluation datasets?",
          "answer": "The Chameleon Benchmark Overfit Detector (C-BOD) is the meta-evaluation framework introduced in the article for creating perturbed evaluation datasets."
        },
        {
          "question": "ACU. What specific generative model was used to create the transformed version of the MMLU dataset?",
          "answer": "The summary does not specify which generative model was used to create the transformed version of the MMLU dataset."
        },
        {
          "question": "ACU. What is the name of the parameter that controls the extent of textual modification in the distortion operator?",
          "answer": "The question mentions a distortion parameter µ, but the summary does not explicitly name a parameter that controls the extent of textual modification in the distortion operator."
        },
        {
          "question": "ACU. What specific benchmark dataset was used to generate a perturbed version for the experiments?",
          "answer": "The summary mentions that the C-BOD framework was used to create perturbed versions of benchmark prompts, and specifically refers to the MMLU dataset in the context of generating a transformed version for experiments."
        }
      ],
      "judge": {
        "evaluations": [
          {
            "qa": {
              "question": "How does the C-BOD framework use a generative \"rephrasing tool\" and a distortion parameter µ to systematically create perturbed evaluation datasets?",
              "answer": "The summary states that C-BOD creates perturbed versions of benchmark prompts through parametric transformations, preserving semantic content but altering wording and structure. It mentions a \"distortion parameter µ\" in the question, but the summary does not detail how a \"rephrasing tool\" or the parameter µ are used in this process."
            },
            "result": false,
            "issue": "The summary does not detail how the rephrasing tool and parameter µ are used."
          },
          {
            "qa": {
              "question": "The article claims the C-BOD methodology is \"dataset-agnostic.\" Based on the description, what aspects of the generative process allow it to be applied to benchmarks other than MMLU?",
              "answer": "The summary states that C-BOD creates perturbed versions of benchmark prompts through parametric transformations, preserving semantic content but altering wording and structure. It also mentions that the framework is dataset-agnostic and can be integrated into training pipelines to foster more robust LLMs. However, it does not specify which aspects of the generative process allow it to be applied to other benchmarks."
            },
            "result": false,
            "issue": "The summary does not specify which aspects of the generative process make it dataset-agnostic."
          },
          {
            "qa": {
              "question": "What is the role of McNemar's test within the C-BOD methodology after a generative model has created a perturbed dataset?",
              "answer": "The summary states that C-BOD evaluates LLMs on both original and perturbed datasets and uses statistical tests like McNemar's to quantify performance degradation, indicating overfitting."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "Explain the process and purpose of using the specific query prompt, \"Rephrase the following question without changing its context or the correct answer,\" to guide the generative model in creating the adversarial dataset.",
              "answer": "The summary does not mention the specific query prompt used to guide the generative model."
            },
            "result": false,
            "issue": "The summary does not mention the specific query prompt."
          },
          {
            "qa": {
              "question": "According to the authors, how does using a generative model to create rephrased datasets fill a gap left by existing methods like n-gram overlap and embedding similarity for detecting overfitting?",
              "answer": "The summary states that C-BOD operates by creating perturbed versions of benchmark prompts through parametric transformations, preserving semantic content but altering wording and structure. It also mentions that this methodology challenges the reliance on standard leaderboard scores and advocates for prioritizing model resilience and generalization. However, it does not explain how this fills a gap left by existing methods like n-gram overlap and embedding similarity."
            },
            "result": false,
            "issue": "The summary does not explain how C-BOD fills a gap left by existing methods."
          },
          {
            "qa": {
              "question": "ACU. What is the name of the framework introduced in the article for creating perturbed evaluation datasets?",
              "answer": "The Chameleon Benchmark Overfit Detector (C-BOD) is the meta-evaluation framework introduced in the article for creating perturbed evaluation datasets."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "ACU. What specific generative model was used to create the transformed version of the MMLU dataset?",
              "answer": "The summary does not specify which generative model was used to create the transformed version of the MMLU dataset."
            },
            "result": false,
            "issue": "The summary does not specify the generative model used."
          },
          {
            "qa": {
              "question": "ACU. What is the name of the parameter that controls the extent of textual modification in the distortion operator?",
              "answer": "The question mentions a distortion parameter µ, but the summary does not explicitly name a parameter that controls the extent of textual modification in the distortion operator."
            },
            "result": false,
            "issue": "The summary does not explicitly name the parameter."
          },
          {
            "qa": {
              "question": "ACU. What specific benchmark dataset was used to generate a perturbed version for the experiments?",
              "answer": "The summary mentions that the C-BOD framework was used to create perturbed versions of benchmark prompts, and specifically refers to the MMLU dataset in the context of generating a transformed version for experiments."
            },
            "result": true,
            "issue": ""
          }
        ],
        "judgment": false,
        "sections_to_highlight": [
          "C-BOD: Overview and Purpose",
          "Methodology: Parametric Transformations",
          "McNemar's Test Role",
          "Dataset Agnosticism",
          "Generative Model Details",
          "Comparison to Existing Methods",
          "Key Findings: Model Susceptibility"
        ]
      },
      "correct_count_all": 3,
      "correct_count_acu": 2,
      "num_of_questions": 5,
      "sections_to_highlight": [
        "C-BOD: Overview and Purpose",
        "Methodology: Parametric Transformations",
        "McNemar's Test Role",
        "Dataset Agnosticism",
        "Generative Model Details",
        "Comparison to Existing Methods",
        "Key Findings: Model Susceptibility"
      ],
      "sections_to_highlight_size": 7,
      "qa_failures_present": true
    },
    {
      "iteration_number": 7,
      "summary": "1. SUMMARY:\nThis paper introduces the Chameleon Benchmark Overfit Detector (C-BOD), a meta-evaluation framework designed to identify whether Large Language Models (LLMs) overfit to specific prompt patterns in evaluation datasets, rather than demonstrating true language understanding. C-BOD systematically distorts benchmark prompts using a parametric transformation (controlled by a distortion parameter µ) to create perturbed versions of the original dataset. The LLM is then evaluated on both the original and perturbed datasets. McNemar's test is employed to statistically assess the performance difference between the two versions. A significant performance drop on the perturbed dataset indicates overfitting to surface cues. The framework is dataset-agnostic, meaning it can be applied to any benchmark by adapting the performance metric. Unlike existing methods that focus on training data contamination, C-BOD specifically targets overfitting to the structure and phrasing of evaluation prompts. Key findings reveal that most evaluated LLMs, particularly larger ones and those with higher baseline accuracy, exhibit significant performance degradation under these perturbations, suggesting a widespread reliance on memorized patterns.\n\n2. KEY HIGHLIGHTS:\n*   C-BOD uses parametric transformations to create perturbed evaluation datasets, preserving semantic content while altering prompt phrasing to detect LLM overfitting.\n*   McNemar's test is used to statistically validate performance drops on perturbed datasets, indicating reliance on surface-level cues rather than true understanding.\n*   The C-BOD framework is dataset-agnostic, allowing its application to various NLP benchmarks.\n*   Larger LLMs and those with higher baseline accuracy are more susceptible to performance degradation when prompts are rephrased, highlighting a common overfitting behavior.\n*   C-BOD addresses overfitting to benchmark-specific artifacts, a gap not fully covered by existing methods focused on training data contamination.",
      "qa_evaluations": [
        {
          "qa": {
            "question": "How does the C-BOD framework use a generative \"rephrasing tool\" and a distortion parameter µ to systematically create perturbed evaluation datasets?",
            "answer": "The C-BOD framework uses a parametric transformation, controlled by a distortion parameter µ, to systematically distort benchmark prompts and create perturbed versions of the original dataset."
          },
          "result": true,
          "issue": null
        },
        {
          "qa": {
            "question": "The article claims the C-BOD methodology is \"dataset-agnostic.\" Based on the description, what aspects of the generative process allow it to be applied to benchmarks other than MMLU?",
            "answer": "The C-BOD framework is dataset-agnostic because it can be applied to any benchmark by adapting the performance metric."
          },
          "result": true,
          "issue": null
        },
        {
          "qa": {
            "question": "What is the role of McNemar's test within the C-BOD methodology after a generative model has created a perturbed dataset?",
            "answer": "McNemar's test is employed to statistically assess the performance difference between the original and perturbed datasets. A significant performance drop on the perturbed dataset indicates overfitting to surface cues."
          },
          "result": true,
          "issue": null
        },
        {
          "qa": {
            "question": "Explain the process and purpose of using the specific query prompt, \"Rephrase the following question without changing its context or the correct answer,\" to guide the generative model in creating the adversarial dataset.",
            "answer": "Not enough information in summary"
          },
          "result": false,
          "issue": "Not enough information"
        },
        {
          "qa": {
            "question": "According to the authors, how does using a generative model to create rephrased datasets fill a gap left by existing methods like n-gram overlap and embedding similarity for detecting overfitting?",
            "answer": "C-BOD specifically targets overfitting to the structure and phrasing of evaluation prompts, a gap not fully covered by existing methods focused on training data contamination."
          },
          "result": true,
          "issue": null
        },
        {
          "qa": {
            "question": "ACU. What is the name of the framework introduced in the article for creating perturbed evaluation datasets?",
            "answer": "The framework introduced in the article for creating perturbed evaluation datasets is the Chameleon Benchmark Overfit Detector (C-BOD)."
          },
          "result": true,
          "issue": null
        },
        {
          "qa": {
            "question": "ACU. What specific generative model was used to create the transformed version of the MMLU dataset?",
            "answer": "Not enough information in summary"
          },
          "result": false,
          "issue": "Not enough information"
        },
        {
          "qa": {
            "question": "ACU. What is the name of the parameter that controls the extent of textual modification in the distortion operator?",
            "answer": "The name of the parameter that controls the extent of textual modification in the distortion operator is µ."
          },
          "result": true,
          "issue": null
        },
        {
          "qa": {
            "question": "ACU. What specific benchmark dataset was used to generate a perturbed version for the experiments?",
            "answer": "The summary mentions that the MMLU dataset was used to generate a perturbed version for the experiments."
          },
          "result": true,
          "issue": null
        }
      ],
      "qa_pairs": [
        {
          "question": "How does the C-BOD framework use a generative \"rephrasing tool\" and a distortion parameter µ to systematically create perturbed evaluation datasets?",
          "answer": "The C-BOD framework uses a parametric transformation, controlled by a distortion parameter µ, to systematically distort benchmark prompts and create perturbed versions of the original dataset."
        },
        {
          "question": "The article claims the C-BOD methodology is \"dataset-agnostic.\" Based on the description, what aspects of the generative process allow it to be applied to benchmarks other than MMLU?",
          "answer": "The C-BOD framework is dataset-agnostic because it can be applied to any benchmark by adapting the performance metric."
        },
        {
          "question": "What is the role of McNemar's test within the C-BOD methodology after a generative model has created a perturbed dataset?",
          "answer": "McNemar's test is employed to statistically assess the performance difference between the original and perturbed datasets. A significant performance drop on the perturbed dataset indicates overfitting to surface cues."
        },
        {
          "question": "Explain the process and purpose of using the specific query prompt, \"Rephrase the following question without changing its context or the correct answer,\" to guide the generative model in creating the adversarial dataset.",
          "answer": "Not enough information in summary"
        },
        {
          "question": "According to the authors, how does using a generative model to create rephrased datasets fill a gap left by existing methods like n-gram overlap and embedding similarity for detecting overfitting?",
          "answer": "C-BOD specifically targets overfitting to the structure and phrasing of evaluation prompts, a gap not fully covered by existing methods focused on training data contamination."
        },
        {
          "question": "ACU. What is the name of the framework introduced in the article for creating perturbed evaluation datasets?",
          "answer": "The framework introduced in the article for creating perturbed evaluation datasets is the Chameleon Benchmark Overfit Detector (C-BOD)."
        },
        {
          "question": "ACU. What specific generative model was used to create the transformed version of the MMLU dataset?",
          "answer": "Not enough information in summary"
        },
        {
          "question": "ACU. What is the name of the parameter that controls the extent of textual modification in the distortion operator?",
          "answer": "The name of the parameter that controls the extent of textual modification in the distortion operator is µ."
        },
        {
          "question": "ACU. What specific benchmark dataset was used to generate a perturbed version for the experiments?",
          "answer": "The summary mentions that the MMLU dataset was used to generate a perturbed version for the experiments."
        }
      ],
      "judge": {
        "evaluations": [
          {
            "qa": {
              "question": "How does the C-BOD framework use a generative \"rephrasing tool\" and a distortion parameter µ to systematically create perturbed evaluation datasets?",
              "answer": "The C-BOD framework uses a parametric transformation, controlled by a distortion parameter µ, to systematically distort benchmark prompts and create perturbed versions of the original dataset."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "The article claims the C-BOD methodology is \"dataset-agnostic.\" Based on the description, what aspects of the generative process allow it to be applied to benchmarks other than MMLU?",
              "answer": "The C-BOD framework is dataset-agnostic because it can be applied to any benchmark by adapting the performance metric."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "What is the role of McNemar's test within the C-BOD methodology after a generative model has created a perturbed dataset?",
              "answer": "McNemar's test is employed to statistically assess the performance difference between the original and perturbed datasets. A significant performance drop on the perturbed dataset indicates overfitting to surface cues."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "Explain the process and purpose of using the specific query prompt, \"Rephrase the following question without changing its context or the correct answer,\" to guide the generative model in creating the adversarial dataset.",
              "answer": "Not enough information in summary"
            },
            "result": false,
            "issue": "The answer states there is not enough information in the summary. The article mentions the prompt used: \"Rephrase the following question without changing its context or the correct answer: {question}\". The purpose is to guide the generative model to create alternative versions of questions while preserving their original meaning and correct answers, which is crucial for C-BOD's methodology."
          },
          {
            "qa": {
              "question": "According to the authors, how does using a generative model to create rephrased datasets fill a gap left by existing methods like n-gram overlap and embedding similarity for detecting overfitting?",
              "answer": "C-BOD specifically targets overfitting to the structure and phrasing of evaluation prompts, a gap not fully covered by existing methods focused on training data contamination."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "ACU. What is the name of the framework introduced in the article for creating perturbed evaluation datasets?",
              "answer": "The framework introduced in the article for creating perturbed evaluation datasets is the Chameleon Benchmark Overfit Detector (C-BOD)."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "ACU. What specific generative model was used to create the transformed version of the MMLU dataset?",
              "answer": "Not enough information in summary"
            },
            "result": false,
            "issue": "The answer states there is not enough information in the summary. The article explicitly states in Section 4.1: \"We used DeepSeek 3 to create the transformed version of each question.\""
          },
          {
            "qa": {
              "question": "ACU. What is the name of the parameter that controls the extent of textual modification in the distortion operator?",
              "answer": "The name of the parameter that controls the extent of textual modification in the distortion operator is µ."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "ACU. What specific benchmark dataset was used to generate a perturbed version for the experiments?",
              "answer": "The summary mentions that the MMLU dataset was used to generate a perturbed version for the experiments."
            },
            "result": true,
            "issue": ""
          }
        ],
        "judgment": false,
        "sections_to_highlight": [
          "Introduction to LLM Evaluation Challenges",
          "C-BOD Framework Overview",
          "Methodology: Dataset Generation and Evaluation",
          "Statistical Validation with McNemar's Test",
          "Experimental Setup and Models",
          "Results: Performance Degradation and Correlations",
          "Discussion: Causes and Implications of Overfitting",
          "Limitations and Future Work"
        ]
      },
      "correct_count_all": 7,
      "correct_count_acu": 3,
      "num_of_questions": 5,
      "sections_to_highlight": [
        "Introduction to LLM Evaluation Challenges",
        "C-BOD Framework Overview",
        "Methodology: Dataset Generation and Evaluation",
        "Statistical Validation with McNemar's Test",
        "Experimental Setup and Models",
        "Results: Performance Degradation and Correlations",
        "Discussion: Causes and Implications of Overfitting",
        "Limitations and Future Work"
      ],
      "sections_to_highlight_size": 8,
      "qa_failures_present": true
    },
    {
      "iteration_number": 8,
      "summary": "1. SUMMARY:\nThis paper addresses a critical challenge in Large Language Model (LLM) evaluation: the tendency for models to overfit to specific benchmark formats, leading to inflated performance scores that don't reflect true understanding. The authors introduce the Chameleon Benchmark Overfit Detector (C-BOD), a meta-evaluation framework designed to systematically expose this overfitting. C-BOD works by generating perturbed versions of benchmark prompts (like MMLU) through parametric transformations that preserve semantic content but alter surface-level cues. LLMs are then evaluated on both original and perturbed datasets. A key component is the use of McNemar's test to statistically validate performance differences, identifying if a decline is significant or due to chance. Their experiments on 26 leading LLMs revealed that most models exhibit a notable performance drop (average 2.15%) when faced with rephrased prompts, indicating a reliance on memorized patterns. Interestingly, larger models and those with higher baseline accuracy showed a greater susceptibility to this overfitting. This work emphasizes the need to move beyond simple leaderboard scores towards evaluating LLM resilience and generalization.\n\n2. KEY HIGHLIGHTS:\n*   C-BOD is a meta-evaluation framework that detects LLM overfitting to benchmark-specific surface cues by evaluating models on semantically equivalent but textually perturbed prompts.\n*   The methodology involves generating perturbed datasets using a parametric transformation (µ) and statistically validating performance differences using McNemar's test.\n*   Experiments showed that a majority of LLMs (20 out of 26) experienced performance degradation on rephrased prompts, averaging 2.15%, indicating reliance on memorized patterns.\n*   Larger LLMs and those with higher initial accuracy were found to be more prone to overfitting and exhibited greater performance drops under prompt perturbations.\n*   The findings challenge the community to prioritize LLM resilience and generalization over superficial benchmark scores.",
      "qa_evaluations": [
        {
          "qa": {
            "question": "How does the C-BOD framework use a generative \"rephrasing tool\" and a distortion parameter µ to systematically create perturbed evaluation datasets?",
            "answer": "The summary states that C-BOD works by generating perturbed versions of benchmark prompts through parametric transformations that preserve semantic content but alter surface-level cues, and that a distortion parameter µ is involved in this process. However, it does not detail the specific functioning of a \"rephrasing tool\" or how the parameter µ is used in conjunction with it to create the datasets."
          },
          "result": false,
          "issue": "The summary explains the general process but lacks specific details on the 'rephrasing tool' and the exact mechanism of the distortion parameter µ."
        },
        {
          "qa": {
            "question": "The article claims the C-BOD methodology is \"dataset-agnostic.\" Based on the description, what aspects of the generative process allow it to be applied to benchmarks other than MMLU?",
            "answer": "The summary does not provide information on what aspects of the generative process allow C-BOD to be applied to benchmarks other than MMLU, only that it is dataset-agnostic."
          },
          "result": false,
          "issue": "Not enough information in summary"
        },
        {
          "qa": {
            "question": "What is the role of McNemar's test within the C-BOD methodology after a generative model has created a perturbed dataset?",
            "answer": "McNemar's test is used to statistically validate performance differences between the original and perturbed datasets, helping to identify if a performance decline is significant or due to chance."
          },
          "result": true,
          "issue": null
        },
        {
          "qa": {
            "question": "Explain the process and purpose of using the specific query prompt, \"Rephrase the following question without changing its context or the correct answer,\" to guide the generative model in creating the adversarial dataset.",
            "answer": "The summary does not mention the specific query prompt used to guide the generative model."
          },
          "result": false,
          "issue": "Not enough information in summary"
        },
        {
          "qa": {
            "question": "According to the authors, how does using a generative model to create rephrased datasets fill a gap left by existing methods like n-gram overlap and embedding similarity for detecting overfitting?",
            "answer": "The summary does not explain how using a generative model to create rephrased datasets fills a gap left by existing methods like n-gram overlap and embedding similarity for detecting overfitting."
          },
          "result": false,
          "issue": "Not enough information in summary"
        },
        {
          "qa": {
            "question": "ACU. What is the name of the framework introduced in the article for creating perturbed evaluation datasets?",
            "answer": "The framework introduced in the article for creating perturbed evaluation datasets is called Chameleon Benchmark Overfit Detector (C-BOD)."
          },
          "result": true,
          "issue": null
        },
        {
          "qa": {
            "question": "ACU. What specific generative model was used to create the transformed version of the MMLU dataset?",
            "answer": "The summary does not specify the particular generative model used to create the transformed version of the MMLU dataset."
          },
          "result": false,
          "issue": "Not enough information in summary"
        },
        {
          "qa": {
            "question": "ACU. What is the name of the parameter that controls the extent of textual modification in the distortion operator?",
            "answer": "The name of the parameter that controls the extent of textual modification in the distortion operator is µ."
          },
          "result": true,
          "issue": null
        },
        {
          "qa": {
            "question": "ACU. What specific benchmark dataset was used to generate a perturbed version for the experiments?",
            "answer": "The MMLU dataset was used to generate a perturbed version for the experiments."
          },
          "result": true,
          "issue": null
        }
      ],
      "qa_pairs": [
        {
          "question": "How does the C-BOD framework use a generative \"rephrasing tool\" and a distortion parameter µ to systematically create perturbed evaluation datasets?",
          "answer": "The summary states that C-BOD works by generating perturbed versions of benchmark prompts through parametric transformations that preserve semantic content but alter surface-level cues, and that a distortion parameter µ is involved in this process. However, it does not detail the specific functioning of a \"rephrasing tool\" or how the parameter µ is used in conjunction with it to create the datasets."
        },
        {
          "question": "The article claims the C-BOD methodology is \"dataset-agnostic.\" Based on the description, what aspects of the generative process allow it to be applied to benchmarks other than MMLU?",
          "answer": "The summary does not provide information on what aspects of the generative process allow C-BOD to be applied to benchmarks other than MMLU, only that it is dataset-agnostic."
        },
        {
          "question": "What is the role of McNemar's test within the C-BOD methodology after a generative model has created a perturbed dataset?",
          "answer": "McNemar's test is used to statistically validate performance differences between the original and perturbed datasets, helping to identify if a performance decline is significant or due to chance."
        },
        {
          "question": "Explain the process and purpose of using the specific query prompt, \"Rephrase the following question without changing its context or the correct answer,\" to guide the generative model in creating the adversarial dataset.",
          "answer": "The summary does not mention the specific query prompt used to guide the generative model."
        },
        {
          "question": "According to the authors, how does using a generative model to create rephrased datasets fill a gap left by existing methods like n-gram overlap and embedding similarity for detecting overfitting?",
          "answer": "The summary does not explain how using a generative model to create rephrased datasets fills a gap left by existing methods like n-gram overlap and embedding similarity for detecting overfitting."
        },
        {
          "question": "ACU. What is the name of the framework introduced in the article for creating perturbed evaluation datasets?",
          "answer": "The framework introduced in the article for creating perturbed evaluation datasets is called Chameleon Benchmark Overfit Detector (C-BOD)."
        },
        {
          "question": "ACU. What specific generative model was used to create the transformed version of the MMLU dataset?",
          "answer": "The summary does not specify the particular generative model used to create the transformed version of the MMLU dataset."
        },
        {
          "question": "ACU. What is the name of the parameter that controls the extent of textual modification in the distortion operator?",
          "answer": "The name of the parameter that controls the extent of textual modification in the distortion operator is µ."
        },
        {
          "question": "ACU. What specific benchmark dataset was used to generate a perturbed version for the experiments?",
          "answer": "The MMLU dataset was used to generate a perturbed version for the experiments."
        }
      ],
      "judge": {
        "evaluations": [
          {
            "qa": {
              "question": "How does the C-BOD framework use a generative \"rephrasing tool\" and a distortion parameter µ to systematically create perturbed evaluation datasets?",
              "answer": "The summary states that C-BOD works by generating perturbed versions of benchmark prompts through parametric transformations that preserve semantic content but alter surface-level cues, and that a distortion parameter µ is involved in this process. However, it does not detail the specific functioning of a \"rephrasing tool\" or how the parameter µ is used in conjunction with it to create the datasets."
            },
            "result": false,
            "issue": "The summary does not detail the specific functioning of a \"rephrasing tool\" or how the parameter µ is used in conjunction with it to create the datasets."
          },
          {
            "qa": {
              "question": "The article claims the C-BOD methodology is \"dataset-agnostic.\" Based on the description, what aspects of the generative process allow it to be applied to benchmarks other than MMLU?",
              "answer": "The summary does not provide information on what aspects of the generative process allow C-BOD to be applied to benchmarks other than MMLU, only that it is dataset-agnostic."
            },
            "result": false,
            "issue": "The summary does not provide information on what aspects of the generative process allow C-BOD to be applied to benchmarks other than MMLU."
          },
          {
            "qa": {
              "question": "What is the role of McNemar's test within the C-BOD methodology after a generative model has created a perturbed dataset?",
              "answer": "McNemar's test is used to statistically validate performance differences between the original and perturbed datasets, helping to identify if a performance decline is significant or due to chance."
            },
            "result": true,
            "issue": null
          },
          {
            "qa": {
              "question": "Explain the process and purpose of using the specific query prompt, \"Rephrase the following question without changing its context or the correct answer,\" to guide the generative model in creating the adversarial dataset.",
              "answer": "The summary does not mention the specific query prompt used to guide the generative model."
            },
            "result": false,
            "issue": "The summary does not mention the specific query prompt used to guide the generative model."
          },
          {
            "qa": {
              "question": "According to the authors, how does using a generative model to create rephrased datasets fill a gap left by existing methods like n-gram overlap and embedding similarity for detecting overfitting?",
              "answer": "The summary does not explain how using a generative model to create rephrased datasets fills a gap left by existing methods like n-gram overlap and embedding similarity for detecting overfitting."
            },
            "result": false,
            "issue": "The summary does not explain how using a generative model to create rephrased datasets fills a gap left by existing methods like n-gram overlap and embedding similarity for detecting overfitting."
          },
          {
            "qa": {
              "question": "ACU. What is the name of the framework introduced in the article for creating perturbed evaluation datasets?",
              "answer": "The framework introduced in the article for creating perturbed evaluation datasets is called Chameleon Benchmark Overfit Detector (C-BOD)."
            },
            "result": true,
            "issue": null
          },
          {
            "qa": {
              "question": "ACU. What specific generative model was used to create the transformed version of the MMLU dataset?",
              "answer": "The summary does not specify the particular generative model used to create the transformed version of the MMLU dataset."
            },
            "result": false,
            "issue": "The summary does not specify the particular generative model used to create the transformed version of the MMLU dataset."
          },
          {
            "qa": {
              "question": "ACU. What is the name of the parameter that controls the extent of textual modification in the distortion operator?",
              "answer": "The name of the parameter that controls the extent of textual modification in the distortion operator is µ."
            },
            "result": true,
            "issue": null
          },
          {
            "qa": {
              "question": "ACU. What specific benchmark dataset was used to generate a perturbed version for the experiments?",
              "answer": "The MMLU dataset was used to generate a perturbed version for the experiments."
            },
            "result": true,
            "issue": null
          }
        ],
        "judgment": false,
        "sections_to_highlight": [
          "C-BOD Framework Overview",
          "Parametric Transformation (µ)",
          "McNemar's Test for Significance",
          "LLM Performance Degradation",
          "Model Size vs. Overfitting",
          "Accuracy vs. Overfitting",
          "Limitations of C-BOD"
        ]
      },
      "correct_count_all": 4,
      "correct_count_acu": 3,
      "num_of_questions": 5,
      "sections_to_highlight": [
        "C-BOD Framework Overview",
        "Parametric Transformation (µ)",
        "McNemar's Test for Significance",
        "LLM Performance Degradation",
        "Model Size vs. Overfitting",
        "Accuracy vs. Overfitting",
        "Limitations of C-BOD"
      ],
      "sections_to_highlight_size": 7,
      "qa_failures_present": true
    },
    {
      "iteration_number": 9,
      "summary": "1. SUMMARY:\nThe article introduces the Chameleon Benchmark Overfit Detector (C-BOD), a meta-evaluation framework designed to identify overfitting in Large Language Models (LLMs) to benchmark-specific surface cues. C-BOD systematically perturbs benchmark prompts using a parametric transformation, controlled by a distortion parameter 'µ', while preserving semantic content and labels. This allows for the detection of whether an LLM's high performance is due to memorized patterns rather than true language understanding. The framework evaluates LLMs on both original and perturbed datasets and employs McNemar's test to statistically validate performance differences. Experiments on the MMLU benchmark with 26 LLMs revealed an average performance degradation of 2.15% under modest perturbations, with 20 models showing statistically significant drops. Notably, larger models and those with higher baseline accuracy exhibited greater degradation, suggesting a reliance on fixed prompt patterns. C-BOD's dataset and model-agnostic design facilitates integration into training pipelines to promote more robust language understanding, challenging the community to prioritize resilience over leaderboard scores.\n\n2. KEY HIGHLIGHTS:\n*   C-BOD systematically distorts benchmark prompts using a parametric transformation (µ) to detect LLM overfitting to surface cues.\n*   McNemar's test is used to statistically validate performance differences between original and perturbed prompts, indicating overfitting.\n*   LLMs showed an average performance degradation of 2.15% on perturbed data, with higher-accuracy and larger models exhibiting more significant drops.\n*   The framework is dataset and model-agnostic, enabling its application to various benchmarks and LLMs to promote robustness.\n*   Limitations include potential oversight of deeper overfitting forms and increased computational cost for integration into training pipelines.",
      "qa_evaluations": [
        {
          "qa": {
            "question": "How does the C-BOD framework use a generative \"rephrasing tool\" and a distortion parameter µ to systematically create perturbed evaluation datasets?",
            "answer": "The C-BOD framework systematically perturbs benchmark prompts using a parametric transformation, controlled by a distortion parameter 'µ', to create perturbed evaluation datasets."
          },
          "result": true,
          "issue": null
        },
        {
          "qa": {
            "question": "The article claims the C-BOD methodology is \"dataset-agnostic.\" Based on the description, what aspects of the generative process allow it to be applied to benchmarks other than MMLU?",
            "answer": "The summary states that the C-BOD framework is dataset-agnostic, which facilitates its integration into training pipelines to promote more robust language understanding and allows for its application to various benchmarks."
          },
          "result": true,
          "issue": null
        },
        {
          "qa": {
            "question": "What is the role of McNemar's test within the C-BOD methodology after a generative model has created a perturbed dataset?",
            "answer": "McNemar's test is employed within the C-BOD methodology to statistically validate performance differences between the original and perturbed datasets, indicating overfitting."
          },
          "result": true,
          "issue": null
        },
        {
          "qa": {
            "question": "Explain the process and purpose of using the specific query prompt, \"Rephrase the following question without changing its context or the correct answer,\" to guide the generative model in creating the adversarial dataset.",
            "answer": "Not enough information in summary"
          },
          "result": false,
          "issue": "Not enough information"
        },
        {
          "qa": {
            "question": "According to the authors, how does using a generative model to create rephrased datasets fill a gap left by existing methods like n-gram overlap and embedding similarity for detecting overfitting?",
            "answer": "Not enough information in summary"
          },
          "result": false,
          "issue": "Not enough information"
        },
        {
          "qa": {
            "question": "ACU. What is the name of the framework introduced in the article for creating perturbed evaluation datasets?",
            "answer": "The framework introduced in the article for creating perturbed evaluation datasets is the Chameleon Benchmark Overfit Detector (C-BOD)."
          },
          "result": true,
          "issue": null
        },
        {
          "qa": {
            "question": "ACU. What specific generative model was used to create the transformed version of the MMLU dataset?",
            "answer": "Not enough information in summary"
          },
          "result": false,
          "issue": "Not enough information"
        },
        {
          "qa": {
            "question": "ACU. What is the name of the parameter that controls the extent of textual modification in the distortion operator?",
            "answer": "The name of the parameter that controls the extent of textual modification in the distortion operator is 'µ'."
          },
          "result": true,
          "issue": null
        },
        {
          "qa": {
            "question": "ACU. What specific benchmark dataset was used to generate a perturbed version for the experiments?",
            "answer": "The specific benchmark dataset used to generate a perturbed version for the experiments was MMLU."
          },
          "result": true,
          "issue": null
        }
      ],
      "qa_pairs": [
        {
          "question": "How does the C-BOD framework use a generative \"rephrasing tool\" and a distortion parameter µ to systematically create perturbed evaluation datasets?",
          "answer": "The C-BOD framework systematically perturbs benchmark prompts using a parametric transformation, controlled by a distortion parameter 'µ', to create perturbed evaluation datasets."
        },
        {
          "question": "The article claims the C-BOD methodology is \"dataset-agnostic.\" Based on the description, what aspects of the generative process allow it to be applied to benchmarks other than MMLU?",
          "answer": "The summary states that the C-BOD framework is dataset-agnostic, which facilitates its integration into training pipelines to promote more robust language understanding and allows for its application to various benchmarks."
        },
        {
          "question": "What is the role of McNemar's test within the C-BOD methodology after a generative model has created a perturbed dataset?",
          "answer": "McNemar's test is employed within the C-BOD methodology to statistically validate performance differences between the original and perturbed datasets, indicating overfitting."
        },
        {
          "question": "Explain the process and purpose of using the specific query prompt, \"Rephrase the following question without changing its context or the correct answer,\" to guide the generative model in creating the adversarial dataset.",
          "answer": "Not enough information in summary"
        },
        {
          "question": "According to the authors, how does using a generative model to create rephrased datasets fill a gap left by existing methods like n-gram overlap and embedding similarity for detecting overfitting?",
          "answer": "Not enough information in summary"
        },
        {
          "question": "ACU. What is the name of the framework introduced in the article for creating perturbed evaluation datasets?",
          "answer": "The framework introduced in the article for creating perturbed evaluation datasets is the Chameleon Benchmark Overfit Detector (C-BOD)."
        },
        {
          "question": "ACU. What specific generative model was used to create the transformed version of the MMLU dataset?",
          "answer": "Not enough information in summary"
        },
        {
          "question": "ACU. What is the name of the parameter that controls the extent of textual modification in the distortion operator?",
          "answer": "The name of the parameter that controls the extent of textual modification in the distortion operator is 'µ'."
        },
        {
          "question": "ACU. What specific benchmark dataset was used to generate a perturbed version for the experiments?",
          "answer": "The specific benchmark dataset used to generate a perturbed version for the experiments was MMLU."
        }
      ],
      "judge": {
        "evaluations": [
          {
            "qa": {
              "question": "How does the C-BOD framework use a generative \"rephrasing tool\" and a distortion parameter µ to systematically create perturbed evaluation datasets?",
              "answer": "The C-BOD framework systematically perturbs benchmark prompts using a parametric transformation, controlled by a distortion parameter 'µ', to create perturbed evaluation datasets."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "The article claims the C-BOD methodology is \"dataset-agnostic.\" Based on the description, what aspects of the generative process allow it to be applied to benchmarks other than MMLU?",
              "answer": "The summary states that the C-BOD framework is dataset-agnostic, which facilitates its integration into training pipelines to promote more robust language understanding and allows for its application to various benchmarks."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "What is the role of McNemar's test within the C-BOD methodology after a generative model has created a perturbed dataset?",
              "answer": "McNemar's test is employed within the C-BOD methodology to statistically validate performance differences between the original and perturbed datasets, indicating overfitting."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "Explain the process and purpose of using the specific query prompt, \"Rephrase the following question without changing its context or the correct answer,\" to guide the generative model in creating the adversarial dataset.",
              "answer": "Not enough information in summary"
            },
            "result": false,
            "issue": "The summary does not explain the process and purpose of the specific query prompt used to guide the generative model."
          },
          {
            "qa": {
              "question": "According to the authors, how does using a generative model to create rephrased datasets fill a gap left by existing methods like n-gram overlap and embedding similarity for detecting overfitting?",
              "answer": "Not enough information in summary"
            },
            "result": false,
            "issue": "The summary does not explain how the generative model addresses gaps left by existing methods."
          },
          {
            "qa": {
              "question": "ACU. What is the name of the framework introduced in the article for creating perturbed evaluation datasets?",
              "answer": "The framework introduced in the article for creating perturbed evaluation datasets is the Chameleon Benchmark Overfit Detector (C-BOD)."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "ACU. What specific generative model was used to create the transformed version of the MMLU dataset?",
              "answer": "Not enough information in summary"
            },
            "result": false,
            "issue": "The summary does not specify the generative model used for transforming the MMLU dataset."
          },
          {
            "qa": {
              "question": "ACU. What is the name of the parameter that controls the extent of textual modification in the distortion operator?",
              "answer": "The name of the parameter that controls the extent of textual modification in the distortion operator is 'µ'."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "ACU. What specific benchmark dataset was used to generate a perturbed version for the experiments?",
              "answer": "The specific benchmark dataset used to generate a perturbed version for the experiments was MMLU."
            },
            "result": true,
            "issue": ""
          }
        ],
        "judgment": false,
        "sections_to_highlight": [
          "C-BOD Framework",
          "Overfitting Detection",
          "Parametric Transformation (µ)",
          "McNemar's Test",
          "LLM Performance Degradation",
          "Dataset and Model Agnosticism",
          "Limitations of C-BOD"
        ]
      },
      "correct_count_all": 6,
      "correct_count_acu": 3,
      "num_of_questions": 5,
      "sections_to_highlight": [
        "C-BOD Framework",
        "Overfitting Detection",
        "Parametric Transformation (µ)",
        "McNemar's Test",
        "LLM Performance Degradation",
        "Dataset and Model Agnosticism",
        "Limitations of C-BOD"
      ],
      "sections_to_highlight_size": 7,
      "qa_failures_present": true
    },
    {
      "iteration_number": 10,
      "summary": "1. SUMMARY:\nThis paper introduces the Chameleon Benchmark Overfit Detector (C-BOD), a meta-evaluation framework designed to uncover whether Large Language Models (LLMs) overfit to specific surface cues in benchmark datasets rather than exhibiting true language understanding. C-BOD systematically distorts benchmark prompts using a parametric transformation (controlled by parameter µ) to generate perturbed datasets that preserve semantic content but alter wording and structure. By evaluating LLMs on both original and perturbed datasets and applying McNemar's test, C-BOD quantifies performance degradation. The study found that many leading LLMs, particularly larger ones and those with high baseline accuracy, exhibit significant performance drops under these perturbations, indicating a reliance on memorized patterns. This framework is dataset and model agnostic, allowing integration into training pipelines to promote more robust models. While effective for detecting surface-level overfitting, C-BOD's limitations include potential oversight of deeper forms of overfitting and increased computational cost for iterative mitigation.\n\n2. KEY HIGHLIGHTS:\n*   C-BOD uses parametric transformations (µ) to create perturbed evaluation datasets, preserving semantics but altering prompt phrasing to detect LLM overfitting to surface cues.\n*   McNemar's test is employed to statistically validate performance degradation between original and perturbed datasets, revealing significant drops in many LLMs.\n*   Larger LLMs and those with higher initial benchmark accuracy tend to show greater performance degradation, suggesting a stronger reliance on fixed prompt patterns.\n*   The C-BOD framework is dataset and model agnostic, offering a reusable method for evaluating and improving LLM robustness.\n*   Limitations include potential inability to detect deeper forms of overfitting and the computational cost of integrating transformations into training.",
      "qa_evaluations": [
        {
          "qa": {
            "question": "How does the C-BOD framework use a generative ",
            "answer": "The C-BOD framework uses a generative rephrasing tool and a distortion parameter µ to systematically create perturbed evaluation datasets by distorting benchmark prompts. This process preserves semantic content while altering wording and structure to detect LLM overfitting to surface cues."
          },
          "result": true,
          "issue": null
        },
        {
          "qa": {
            "question": "The article claims the C-BOD methodology is ",
            "answer": "The summary states that the C-BOD framework is dataset-agnostic, meaning it can be applied to benchmarks other than MMLU. However, it does not specify which aspects of the generative process allow for this broader applicability."
          },
          "result": false,
          "issue": "Not enough information in summary"
        },
        {
          "qa": {
            "question": "What is the role of McNemar's test within the C-BOD methodology after a generative model has created a perturbed dataset?",
            "answer": "McNemar's test is employed within the C-BOD methodology to statistically validate performance degradation between the original and perturbed datasets, thereby revealing significant performance drops in many LLMs."
          },
          "result": true,
          "issue": null
        },
        {
          "qa": {
            "question": "Explain the process and purpose of using the specific query prompt, ",
            "answer": "The summary does not contain information about the specific query prompt used to guide the generative model."
          },
          "result": false,
          "issue": "Not enough information in summary"
        },
        {
          "qa": {
            "question": "According to the authors, how does using a generative model to create rephrased datasets fill a gap left by existing methods like n-gram overlap and embedding similarity for detecting overfitting?",
            "answer": "The summary does not explain how using a generative model to create rephrased datasets fills a gap left by existing methods like n-gram overlap and embedding similarity for detecting overfitting."
          },
          "result": false,
          "issue": "Not enough information in summary"
        },
        {
          "qa": {
            "question": "ACU. What is the name of the framework introduced in the article for creating perturbed evaluation datasets?",
            "answer": "The name of the framework introduced in the article for creating perturbed evaluation datasets is the Chameleon Benchmark Overfit Detector (C-BOD)."
          },
          "result": true,
          "issue": null
        },
        {
          "qa": {
            "question": "ACU. What specific generative model was used to create the transformed version of the MMLU dataset?",
            "answer": "The summary does not specify which generative model was used to create the transformed version of the MMLU dataset."
          },
          "result": false,
          "issue": "Not enough information in summary"
        },
        {
          "qa": {
            "question": "ACU. What is the name of the parameter that controls the extent of textual modification in the distortion operator?",
            "answer": "The name of the parameter that controls the extent of textual modification in the distortion operator is µ."
          },
          "result": true,
          "issue": null
        },
        {
          "qa": {
            "question": "ACU. What specific benchmark dataset was used to generate a perturbed version for the experiments?",
            "answer": "The summary mentions that the C-BOD framework was used to create a perturbed version of the MMLU dataset for experiments, but it does not explicitly state that MMLU was the *only* benchmark dataset used."
          },
          "result": false,
          "issue": "partial information"
        }
      ],
      "qa_pairs": [
        {
          "question": "How does the C-BOD framework use a generative ",
          "answer": "The C-BOD framework uses a generative rephrasing tool and a distortion parameter µ to systematically create perturbed evaluation datasets by distorting benchmark prompts. This process preserves semantic content while altering wording and structure to detect LLM overfitting to surface cues."
        },
        {
          "question": "The article claims the C-BOD methodology is ",
          "answer": "The summary states that the C-BOD framework is dataset-agnostic, meaning it can be applied to benchmarks other than MMLU. However, it does not specify which aspects of the generative process allow for this broader applicability."
        },
        {
          "question": "What is the role of McNemar's test within the C-BOD methodology after a generative model has created a perturbed dataset?",
          "answer": "McNemar's test is employed within the C-BOD methodology to statistically validate performance degradation between the original and perturbed datasets, thereby revealing significant performance drops in many LLMs."
        },
        {
          "question": "Explain the process and purpose of using the specific query prompt, ",
          "answer": "The summary does not contain information about the specific query prompt used to guide the generative model."
        },
        {
          "question": "According to the authors, how does using a generative model to create rephrased datasets fill a gap left by existing methods like n-gram overlap and embedding similarity for detecting overfitting?",
          "answer": "The summary does not explain how using a generative model to create rephrased datasets fills a gap left by existing methods like n-gram overlap and embedding similarity for detecting overfitting."
        },
        {
          "question": "ACU. What is the name of the framework introduced in the article for creating perturbed evaluation datasets?",
          "answer": "The name of the framework introduced in the article for creating perturbed evaluation datasets is the Chameleon Benchmark Overfit Detector (C-BOD)."
        },
        {
          "question": "ACU. What specific generative model was used to create the transformed version of the MMLU dataset?",
          "answer": "The summary does not specify which generative model was used to create the transformed version of the MMLU dataset."
        },
        {
          "question": "ACU. What is the name of the parameter that controls the extent of textual modification in the distortion operator?",
          "answer": "The name of the parameter that controls the extent of textual modification in the distortion operator is µ."
        },
        {
          "question": "ACU. What specific benchmark dataset was used to generate a perturbed version for the experiments?",
          "answer": "The summary mentions that the C-BOD framework was used to create a perturbed version of the MMLU dataset for experiments, but it does not explicitly state that MMLU was the *only* benchmark dataset used."
        }
      ],
      "judge": {
        "evaluations": [
          {
            "qa": {
              "question": "How does the C-BOD framework use a generative",
              "answer": "The C-BOD framework uses a generative rephrasing tool and a distortion parameter µ to systematically create perturbed evaluation datasets by distorting benchmark prompts. This process preserves semantic content while altering wording and structure to detect LLM overfitting to surface cues."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "The article claims the C-BOD methodology is",
              "answer": "The summary states that the C-BOD framework is dataset-agnostic, meaning it can be applied to benchmarks other than MMLU. However, it does not specify which aspects of the generative process allow for this broader applicability."
            },
            "result": false,
            "issue": "The article states the methodology is dataset-agnostic, but the summary incorrectly claims it does not specify *which aspects* allow for this. The article explains that the framework can be applied to any benchmark by adapting the performance metric."
          },
          {
            "qa": {
              "question": "What is the role of McNemar's test within the C-BOD methodology after a generative model has created a perturbed dataset?",
              "answer": "McNemar's test is employed within the C-BOD methodology to statistically validate performance degradation between the original and perturbed datasets, thereby revealing significant performance drops in many LLMs."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "Explain the process and purpose of using the specific query prompt,",
              "answer": "The summary does not contain information about the specific query prompt used to guide the generative model."
            },
            "result": false,
            "issue": "The article details the specific query prompt used in section 4.3 Implementation Details: 'Select the best answer from the given options. Respond with only the letter corresponding to the correct choice. Question: {question}'"
          },
          {
            "qa": {
              "question": "According to the authors, how does using a generative model to create rephrased datasets fill a gap left by existing methods like n-gram overlap and embedding similarity for detecting overfitting?",
              "answer": "The summary does not explain how using a generative model to create rephrased datasets fills a gap left by existing methods like n-gram overlap and embedding similarity for detecting overfitting."
            },
            "result": false,
            "issue": "The article explains this in section 2.2 Gap in Current Work: 'Existing techniques such as n-gram overlap and embedding similarity fail to capture this subtle form of overfitting. In contrast, our approach explicitly quantifies the dependence of a model’s performance on the precise phrasing and structure of evaluation prompts, thereby filling this gap in current evaluation methodologies.'"
          },
          {
            "qa": {
              "question": "ACU. What is the name of the framework introduced in the article for creating perturbed evaluation datasets?",
              "answer": "The name of the framework introduced in the article for creating perturbed evaluation datasets is the Chameleon Benchmark Overfit Detector (C-BOD)."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "ACU. What specific generative model was used to create the transformed version of the MMLU dataset?",
              "answer": "The summary does not specify which generative model was used to create the transformed version of the MMLU dataset."
            },
            "result": false,
            "issue": "The article states in section 4.1 Dataset and Rephrasing Process: 'We used DeepSeek 3 to create the transformed version of each question.'"
          },
          {
            "qa": {
              "question": "ACU. What is the name of the parameter that controls the extent of textual modification in the distortion operator?",
              "answer": "The name of the parameter that controls the extent of textual modification in the distortion operator is µ."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "ACU. What specific benchmark dataset was used to generate a perturbed version for the experiments?",
              "answer": "The summary mentions that the C-BOD framework was used to create a perturbed version of the MMLU dataset for experiments, but it does not explicitly state that MMLU was the *only* benchmark dataset used."
            },
            "result": true,
            "issue": ""
          }
        ],
        "judgment": false,
        "sections_to_highlight": [
          "Introduction",
          "C-BOD rephrased dataset generation",
          "Evaluating the Impact of Distortion",
          "Statistical Validation",
          "Dataset and Rephrasing Process",
          "Relationship Between Model Size and Overfit Detection",
          "Relationship Between Model Accuracy and Overfit Detection"
        ]
      },
      "correct_count_all": 5,
      "correct_count_acu": 3,
      "num_of_questions": 5,
      "sections_to_highlight": [
        "Introduction",
        "C-BOD rephrased dataset generation",
        "Evaluating the Impact of Distortion",
        "Statistical Validation",
        "Dataset and Rephrasing Process",
        "Relationship Between Model Size and Overfit Detection",
        "Relationship Between Model Accuracy and Overfit Detection"
      ],
      "sections_to_highlight_size": 7,
      "qa_failures_present": true
    }
  ],
  "final_summary": "1. SUMMARY:\nThis paper introduces the Chameleon Benchmark Overfit Detector (C-BOD), a meta-evaluation framework designed to uncover whether Large Language Models (LLMs) overfit to specific surface cues in benchmark datasets rather than exhibiting true language understanding. C-BOD systematically distorts benchmark prompts using a parametric transformation (controlled by parameter µ) to generate perturbed datasets that preserve semantic content but alter wording and structure. By evaluating LLMs on both original and perturbed datasets and applying McNemar's test, C-BOD quantifies performance degradation. The study found that many leading LLMs, particularly larger ones and those with high baseline accuracy, exhibit significant performance drops under these perturbations, indicating a reliance on memorized patterns. This framework is dataset and model agnostic, allowing integration into training pipelines to promote more robust models. While effective for detecting surface-level overfitting, C-BOD's limitations include potential oversight of deeper forms of overfitting and increased computational cost for iterative mitigation.\n\n2. KEY HIGHLIGHTS:\n*   C-BOD uses parametric transformations (µ) to create perturbed evaluation datasets, preserving semantics but altering prompt phrasing to detect LLM overfitting to surface cues.\n*   McNemar's test is employed to statistically validate performance degradation between original and perturbed datasets, revealing significant drops in many LLMs.\n*   Larger LLMs and those with higher initial benchmark accuracy tend to show greater performance degradation, suggesting a stronger reliance on fixed prompt patterns.\n*   The C-BOD framework is dataset and model agnostic, offering a reusable method for evaluating and improving LLM robustness.\n*   Limitations include potential inability to detect deeper forms of overfitting and the computational cost of integrating transformations into training.",
  "total_iterations": 10,
  "status": "max_iterations_reached",
  "questions": [
    "How does the C-BOD framework use a generative \"rephrasing tool\" and a distortion parameter µ to systematically create perturbed evaluation datasets?",
    "The article claims the C-BOD methodology is \"dataset-agnostic.\" Based on the description, what aspects of the generative process allow it to be applied to benchmarks other than MMLU?",
    "What is the role of McNemar's test within the C-BOD methodology after a generative model has created a perturbed dataset?",
    "Explain the process and purpose of using the specific query prompt, \"Rephrase the following question without changing its context or the correct answer,\" to guide the generative model in creating the adversarial dataset.",
    "According to the authors, how does using a generative model to create rephrased datasets fill a gap left by existing methods like n-gram overlap and embedding similarity for detecting overfitting?"
  ],
  "acu_questions": [
    "ACU. What is the name of the framework introduced in the article for creating perturbed evaluation datasets?",
    "ACU. What specific generative model was used to create the transformed version of the MMLU dataset?",
    "ACU. What is the name of the parameter that controls the extent of textual modification in the distortion operator?",
    "ACU. What specific benchmark dataset was used to generate a perturbed version for the experiments?"
  ]
}