{
  "query": "Differential robustness across distinct LLM architectural families.",
  "max_iterations": 15,
  "iterations": [
    {
      "iteration_number": 1,
      "summary": "1. SUMMARY:\nThis paper introduces the Chameleon Benchmark Overfit Detector (C-BOD) to assess how LLMs rely on surface-level cues in benchmark prompts rather than true language understanding. By systematically rephrasing benchmark questions while preserving their semantic meaning, C-BOD exposes LLMs that have overfitted to specific prompt formats. The study evaluated 26 LLMs on the MMLU benchmark, revealing that 20 models showed a statistically significant performance degradation under these perturbations, averaging 2.15%. Notably, larger models and those with higher baseline accuracy exhibited greater sensitivity to rephrasing, suggesting a reliance on memorized patterns. In contrast, the Llama family of models and those with lower baseline accuracy demonstrated more robust performance, indicating a reduced dependency on superficial cues. This highlights a critical gap in current LLM evaluations, where high leaderboard scores may mask a lack of true generalization and resilience across different linguistic formulations. The findings suggest that architectural choices and training strategies significantly influence an LLM's robustness to prompt variations.\n\n2. KEY HIGHLIGHTS:\n*   LLMs exhibit differential robustness across architectural families, with some models (e.g., Llama) showing significantly less performance degradation under prompt rephrasing than others.\n*   Larger LLMs and those with higher baseline accuracy tend to be more susceptible to overfitting on benchmark-specific surface cues, leading to greater performance drops when prompts are altered.\n*   Models with lower baseline accuracy and specific families like Llama demonstrated reduced dependency on superficial cues, indicating greater robustness.\n*   The C-BOD framework, which systematically rephrases prompts, revealed that a majority of evaluated LLMs (20 out of 26) experienced statistically significant performance declines, averaging 2.15%.\n*   These findings challenge the reliance on standard benchmark leaderboards, emphasizing the need to prioritize resilience and generalization in LLM evaluation.",
      "qa_evaluations": [
        {
          "qa": {
            "question": "According to the study's findings, which specific LLM family demonstrated the most consistent robustness by maintaining relatively stable accuracy on perturbed prompts?",
            "answer": "The Llama family of models demonstrated the most consistent robustness by maintaining relatively stable accuracy on perturbed prompts."
          },
          "result": true,
          "issue": null
        },
        {
          "qa": {
            "question": "What trend was observed regarding performance degradation as model size increased within the Gemma family specifically?",
            "answer": "The study observed that larger models, in general, exhibited greater sensitivity to rephrasing, suggesting a reliance on memorized patterns. However, specific trends within the Gemma family were not detailed."
          },
          "result": false,
          "issue": "Not enough information in summary"
        },
        {
          "qa": {
            "question": "What paradoxical relationship did the study uncover between a model's baseline accuracy on the original MMLU benchmark and its robustness to prompt rephrasing?",
            "answer": "The study uncovered a paradoxical relationship where models with higher baseline accuracy on the original MMLU benchmark exhibited greater sensitivity to prompt rephrasing, while models with lower baseline accuracy demonstrated more robust performance."
          },
          "result": true,
          "issue": null
        },
        {
          "qa": {
            "question": "The paper contrasts the Llama family's stability with other models. What does this imply about how architectural choices or training strategies might influence a model's reliance on superficial cues versus genuine understanding?",
            "answer": "The contrast in stability between the Llama family and other models implies that architectural choices and training strategies significantly influence an LLM's robustness to prompt variations, potentially affecting their reliance on superficial cues versus genuine understanding."
          },
          "result": true,
          "issue": null
        },
        {
          "qa": {
            "question": "Besides the Llama family, some other models like Falcon 7B showed statistically insignificant differences. What reason does the paper provide for their apparent robustness?",
            "answer": "The summary does not provide a specific reason for the apparent robustness of models like Falcon 7B, only noting that the Llama family and models with lower baseline accuracy demonstrated reduced dependency on superficial cues."
          },
          "result": false,
          "issue": "Not enough information in summary"
        },
        {
          "qa": {
            "question": "ACU. Which LLM family is noted in the abstract for showing insignificant performance degradation?",
            "answer": "The Llama family is noted for showing insignificant performance degradation."
          },
          "result": true,
          "issue": null
        },
        {
          "qa": {
            "question": "ACU. How many of the 26 evaluated models exhibited statistically significant performance differences under perturbation?",
            "answer": "20 out of the 26 evaluated models exhibited statistically significant performance differences under perturbation."
          },
          "result": true,
          "issue": null
        },
        {
          "qa": {
            "question": "ACU. What was the average drop in accuracy across all evaluated models when tested on the rephrased prompts?",
            "answer": "The average drop in accuracy across all evaluated models when tested on the rephrased prompts was 2.15%."
          },
          "result": true,
          "issue": null
        }
      ],
      "qa_pairs": [
        {
          "question": "According to the study's findings, which specific LLM family demonstrated the most consistent robustness by maintaining relatively stable accuracy on perturbed prompts?",
          "answer": "The Llama family of models demonstrated the most consistent robustness by maintaining relatively stable accuracy on perturbed prompts."
        },
        {
          "question": "What trend was observed regarding performance degradation as model size increased within the Gemma family specifically?",
          "answer": "The study observed that larger models, in general, exhibited greater sensitivity to rephrasing, suggesting a reliance on memorized patterns. However, specific trends within the Gemma family were not detailed."
        },
        {
          "question": "What paradoxical relationship did the study uncover between a model's baseline accuracy on the original MMLU benchmark and its robustness to prompt rephrasing?",
          "answer": "The study uncovered a paradoxical relationship where models with higher baseline accuracy on the original MMLU benchmark exhibited greater sensitivity to prompt rephrasing, while models with lower baseline accuracy demonstrated more robust performance."
        },
        {
          "question": "The paper contrasts the Llama family's stability with other models. What does this imply about how architectural choices or training strategies might influence a model's reliance on superficial cues versus genuine understanding?",
          "answer": "The contrast in stability between the Llama family and other models implies that architectural choices and training strategies significantly influence an LLM's robustness to prompt variations, potentially affecting their reliance on superficial cues versus genuine understanding."
        },
        {
          "question": "Besides the Llama family, some other models like Falcon 7B showed statistically insignificant differences. What reason does the paper provide for their apparent robustness?",
          "answer": "The summary does not provide a specific reason for the apparent robustness of models like Falcon 7B, only noting that the Llama family and models with lower baseline accuracy demonstrated reduced dependency on superficial cues."
        },
        {
          "question": "ACU. Which LLM family is noted in the abstract for showing insignificant performance degradation?",
          "answer": "The Llama family is noted for showing insignificant performance degradation."
        },
        {
          "question": "ACU. How many of the 26 evaluated models exhibited statistically significant performance differences under perturbation?",
          "answer": "20 out of the 26 evaluated models exhibited statistically significant performance differences under perturbation."
        },
        {
          "question": "ACU. What was the average drop in accuracy across all evaluated models when tested on the rephrased prompts?",
          "answer": "The average drop in accuracy across all evaluated models when tested on the rephrased prompts was 2.15%."
        }
      ],
      "judge": {
        "evaluations": [
          {
            "qa": {
              "question": "According to the study's findings, which specific LLM family demonstrated the most consistent robustness by maintaining relatively stable accuracy on perturbed prompts?",
              "answer": "The Llama family of models demonstrated the most consistent robustness by maintaining relatively stable accuracy on perturbed prompts."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "What trend was observed regarding performance degradation as model size increased within the Gemma family specifically?",
              "answer": "The study observed that larger models, in general, exhibited greater sensitivity to rephrasing, suggesting a reliance on memorized patterns. However, specific trends within the Gemma family were not detailed."
            },
            "result": false,
            "issue": "The article states that \"models in the Gemma family show a progressive increase in \\u22061.0 with higher parameter counts\", which contradicts the answer's claim that specific trends were not detailed. The answer also incorrectly generalizes the trend for Gemma to all larger models."
          },
          {
            "qa": {
              "question": "What paradoxical relationship did the study uncover between a model's baseline accuracy on the original MMLU benchmark and its robustness to prompt rephrasing?",
              "answer": "The study uncovered a paradoxical relationship where models with higher baseline accuracy on the original MMLU benchmark exhibited greater sensitivity to prompt rephrasing, while models with lower baseline accuracy demonstrated more robust performance."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "The paper contrasts the Llama family's stability with other models. What does this imply about how architectural choices or training strategies might influence a model's reliance on superficial cues versus genuine understanding?",
              "answer": "The contrast in stability between the Llama family and other models implies that architectural choices and training strategies significantly influence an LLM's robustness to prompt variations, potentially affecting their reliance on superficial cues versus genuine understanding."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "Besides the Llama family, some other models like Falcon 7B showed statistically insignificant differences. What reason does the paper provide for their apparent robustness?",
              "answer": "The summary does not provide a specific reason for the apparent robustness of models like Falcon 7B, only noting that the Llama family and models with lower baseline accuracy demonstrated reduced dependency on superficial cues."
            },
            "result": false,
            "issue": "The article states that models like Falcon 7B showing statistically insignificant differences did so \"likely due to their lower baseline accuracy\", which is a reason for their apparent robustness. The answer incorrectly claims no reason is provided."
          },
          {
            "qa": {
              "question": "ACU. Which LLM family is noted in the abstract for showing insignificant performance degradation?",
              "answer": "The Llama family is noted for showing insignificant performance degradation."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "ACU. How many of the 26 evaluated models exhibited statistically significant performance differences under perturbation?",
              "answer": "20 out of the 26 evaluated models exhibited statistically significant performance differences under perturbation."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "ACU. What was the average drop in accuracy across all evaluated models when tested on the rephrased prompts?",
              "answer": "The average drop in accuracy across all evaluated models when tested on the rephrased prompts was 2.15%."
            },
            "result": true,
            "issue": ""
          }
        ],
        "judgment": false,
        "sections_to_highlight": [
          "Results — Gemma family performance trends",
          "Results — Reasons for insignificant performance differences",
          "Method — C-BOD framework applicability",
          "Discussion — LLM overfitting causes",
          "Limitations — computational cost of C-BOD",
          "Limitations — overlooked overfitting types"
        ]
      },
      "correct_count_all": 6,
      "correct_count_acu": 3,
      "num_of_questions": 5,
      "sections_to_highlight": [
        "Results — Gemma family performance trends",
        "Results — Reasons for insignificant performance differences",
        "Method — C-BOD framework applicability",
        "Discussion — LLM overfitting causes",
        "Limitations — computational cost of C-BOD",
        "Limitations — overlooked overfitting types"
      ],
      "sections_to_highlight_size": 6,
      "qa_failures_present": true
    },
    {
      "iteration_number": 2,
      "summary": "1. SUMMARY:\nThis paper introduces the Chameleon Benchmark Overfit Detector (C-BOD) to assess how LLMs rely on surface-level cues in benchmark prompts, rather than true language understanding. The framework systematically distorts benchmark prompts while preserving semantic content, and then evaluates LLM performance on both original and perturbed versions. Results show that 20 out of 26 evaluated LLMs exhibit statistically significant performance degradation, averaging 2.15%, indicating widespread overfitting. Notably, the Gemma family, along with larger models and those with higher baseline accuracy, tend to be more sensitive to these rephrasing perturbations, suggesting a reliance on fixed prompt patterns. In contrast, the Llama family and models with lower baseline accuracy demonstrate less degradation, pointing to a reduced dependency on superficial cues. The C-BOD framework is dataset- and model-agnostic, offering a way to promote more robust language understanding by identifying vulnerabilities to prompt variations. However, the method has limitations, including the computational cost of iterative rephrasing and retraining, and it may not capture deeper forms of overfitting like factual inaccuracies.\n\n2. KEY HIGHLIGHTS:\n*   The Gemma family, particularly larger models within it, shows a progressive increase in performance degradation under prompt perturbations, indicating sensitivity to rephrasing.\n*   LLM overfitting is attributed to an overreliance on surface-level cues, memorized patterns, or specific terminologies, leading models to associate certain question formats or keywords directly with answers instead of generalizing concepts.\n*   The primary limitation of C-BOD is the significant computational cost associated with iteratively rephrasing datasets and retraining models, which can be prohibitive.\n*   C-BOD primarily targets textual rephrasings that preserve semantic content and may overlook deeper forms of overfitting such as factual inaccuracies or logical inconsistencies.\n*   The C-BOD framework is dataset- and model-agnostic, meaning it can be applied to any benchmark by adapting the performance metric used for comparison.\n*   Models with lower baseline accuracy, such as some in the Qwen and DeepSeek families, showed statistically insignificant performance differences, suggesting reduced dependency on superficial cues.",
      "qa_evaluations": [
        {
          "qa": {
            "question": "According to the study's findings, which specific LLM family demonstrated the most consistent robustness by maintaining relatively stable accuracy on perturbed prompts?",
            "answer": "The Llama family demonstrated robustness by maintaining relatively stable accuracy on perturbed prompts."
          },
          "result": true,
          "issue": null
        },
        {
          "qa": {
            "question": "What trend was observed regarding performance degradation as model size increased within the Gemma family specifically?",
            "answer": "Within the Gemma family, larger models showed a progressive increase in performance degradation under prompt perturbations."
          },
          "result": true,
          "issue": null
        },
        {
          "qa": {
            "question": "What paradoxical relationship did the study uncover between a model's baseline accuracy on the original MMLU benchmark and its robustness to prompt rephrasing?",
            "answer": "The study uncovered that models with higher baseline accuracy tend to be more sensitive to prompt rephrasing, while models with lower baseline accuracy demonstrate less degradation, suggesting a reduced dependency on superficial cues."
          },
          "result": true,
          "issue": null
        },
        {
          "qa": {
            "question": "The paper contrasts the Llama family's stability with other models. What does this imply about how architectural choices or training strategies might influence a model's reliance on superficial cues versus genuine understanding?",
            "answer": "The stability of the Llama family implies that architectural choices or training strategies might influence a model's reliance on superficial cues versus genuine understanding, as they demonstrated less degradation and reduced dependency on superficial cues compared to other models."
          },
          "result": true,
          "issue": null
        },
        {
          "qa": {
            "question": "Besides the Llama family, some other models like Falcon 7B showed statistically insignificant differences. What reason does the paper provide for their apparent robustness?",
            "answer": "The paper suggests that models with lower baseline accuracy, such as some in the Qwen and DeepSeek families (and by implication Falcon 7B, though not explicitly stated for Falcon 7B), showed statistically insignificant performance differences, indicating a reduced dependency on superficial cues."
          },
          "result": false,
          "issue": "partial information"
        },
        {
          "qa": {
            "question": "ACU. Which LLM family is noted in the abstract for showing insignificant performance degradation?",
            "answer": "The Llama family is noted for showing insignificant performance degradation."
          },
          "result": true,
          "issue": null
        },
        {
          "qa": {
            "question": "ACU. How many of the 26 evaluated models exhibited statistically significant performance differences under perturbation?",
            "answer": "20 out of 26 evaluated LLMs exhibit statistically significant performance degradation."
          },
          "result": true,
          "issue": null
        },
        {
          "qa": {
            "question": "ACU. What was the average drop in accuracy across all evaluated models when tested on the rephrased prompts?",
            "answer": "The average drop in accuracy across all evaluated models when tested on the rephrased prompts was 2.15%."
          },
          "result": true,
          "issue": null
        }
      ],
      "qa_pairs": [
        {
          "question": "According to the study's findings, which specific LLM family demonstrated the most consistent robustness by maintaining relatively stable accuracy on perturbed prompts?",
          "answer": "The Llama family demonstrated robustness by maintaining relatively stable accuracy on perturbed prompts."
        },
        {
          "question": "What trend was observed regarding performance degradation as model size increased within the Gemma family specifically?",
          "answer": "Within the Gemma family, larger models showed a progressive increase in performance degradation under prompt perturbations."
        },
        {
          "question": "What paradoxical relationship did the study uncover between a model's baseline accuracy on the original MMLU benchmark and its robustness to prompt rephrasing?",
          "answer": "The study uncovered that models with higher baseline accuracy tend to be more sensitive to prompt rephrasing, while models with lower baseline accuracy demonstrate less degradation, suggesting a reduced dependency on superficial cues."
        },
        {
          "question": "The paper contrasts the Llama family's stability with other models. What does this imply about how architectural choices or training strategies might influence a model's reliance on superficial cues versus genuine understanding?",
          "answer": "The stability of the Llama family implies that architectural choices or training strategies might influence a model's reliance on superficial cues versus genuine understanding, as they demonstrated less degradation and reduced dependency on superficial cues compared to other models."
        },
        {
          "question": "Besides the Llama family, some other models like Falcon 7B showed statistically insignificant differences. What reason does the paper provide for their apparent robustness?",
          "answer": "The paper suggests that models with lower baseline accuracy, such as some in the Qwen and DeepSeek families (and by implication Falcon 7B, though not explicitly stated for Falcon 7B), showed statistically insignificant performance differences, indicating a reduced dependency on superficial cues."
        },
        {
          "question": "ACU. Which LLM family is noted in the abstract for showing insignificant performance degradation?",
          "answer": "The Llama family is noted for showing insignificant performance degradation."
        },
        {
          "question": "ACU. How many of the 26 evaluated models exhibited statistically significant performance differences under perturbation?",
          "answer": "20 out of 26 evaluated LLMs exhibit statistically significant performance degradation."
        },
        {
          "question": "ACU. What was the average drop in accuracy across all evaluated models when tested on the rephrased prompts?",
          "answer": "The average drop in accuracy across all evaluated models when tested on the rephrased prompts was 2.15%."
        }
      ],
      "judge": {
        "evaluations": [
          {
            "qa": {
              "question": "According to the study's findings, which specific LLM family demonstrated the most consistent robustness by maintaining relatively stable accuracy on perturbed prompts?",
              "answer": "The Llama family demonstrated robustness by maintaining relatively stable accuracy on perturbed prompts."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "What trend was observed regarding performance degradation as model size increased within the Gemma family specifically?",
              "answer": "Within the Gemma family, larger models showed a progressive increase in performance degradation under prompt perturbations."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "What paradoxical relationship did the study uncover between a model's baseline accuracy on the original MMLU benchmark and its robustness to prompt rephrasing?",
              "answer": "The study uncovered that models with higher baseline accuracy tend to be more sensitive to prompt rephrasing, while models with lower baseline accuracy demonstrate less degradation, suggesting a reduced dependency on superficial cues."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "The paper contrasts the Llama family's stability with other models. What does this imply about how architectural choices or training strategies might influence a model's reliance on superficial cues versus genuine understanding?",
              "answer": "The stability of the Llama family implies that architectural choices or training strategies might influence a model's reliance on superficial cues versus genuine understanding, as they demonstrated less degradation and reduced dependency on superficial cues compared to other models."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "Besides the Llama family, some other models like Falcon 7B showed statistically insignificant differences. What reason does the paper provide for their apparent robustness?",
              "answer": "The paper suggests that models with lower baseline accuracy, such as some in the Qwen and DeepSeek families (and by implication Falcon 7B, though not explicitly stated for Falcon 7B), showed statistically insignificant performance differences, indicating a reduced dependency on superficial cues."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "ACU. Which LLM family is noted in the abstract for showing insignificant performance degradation?",
              "answer": "The Llama family is noted for showing insignificant performance degradation."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "ACU. How many of the 26 evaluated models exhibited statistically significant performance differences under perturbation?",
              "answer": "20 out of 26 evaluated LLMs exhibit statistically significant performance degradation."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "ACU. What was the average drop in accuracy across all evaluated models when tested on the rephrased prompts?",
              "answer": "The average drop in accuracy across all evaluated models when tested on the rephrased prompts was 2.15%."
            },
            "result": true,
            "issue": ""
          }
        ],
        "judgment": true,
        "sections_to_highlight": [
          "Methodology — C-BOD framework and distortion parameter",
          "Results — Performance degradation by model size and accuracy",
          "Limitations — Computational cost and deeper overfitting",
          "Related Work — Overfitting detection techniques",
          "Experimental Setting — MMLU benchmark and rephrasing process",
          "Discussion — Causes and implications of LLM overfitting"
        ]
      },
      "correct_count_all": 8,
      "correct_count_acu": 3,
      "num_of_questions": 5,
      "sections_to_highlight": [
        "Methodology — C-BOD framework and distortion parameter",
        "Results — Performance degradation by model size and accuracy",
        "Limitations — Computational cost and deeper overfitting",
        "Related Work — Overfitting detection techniques",
        "Experimental Setting — MMLU benchmark and rephrasing process",
        "Discussion — Causes and implications of LLM overfitting"
      ],
      "sections_to_highlight_size": 6,
      "qa_failures_present": true
    },
    {
      "iteration_number": 3,
      "summary": "1. SUMMARY:\nThis paper introduces the Chameleon Benchmark Overfit Detector (C-BOD), a meta-evaluation framework designed to identify LLMs that over-rely on surface-level cues in benchmark prompts. C-BOD systematically distorts benchmark questions while preserving semantic content and labels, using a parametric transformation controlled by a distortion parameter (µ). By comparing performance on original and rephrased prompts, C-BOD detects overfitting. Experiments on the MMLU benchmark with 26 LLMs reveal that most models exhibit significant performance degradation under modest perturbations, averaging 2.15%. Notably, larger models and those with higher baseline accuracy tend to be more sensitive to rephrasing, suggesting they overfit to fixed prompt patterns. The Llama family, however, showed reduced dependency on superficial cues. The paper discusses LLM overfitting as a reliance on specific phrasing and keywords rather than true understanding. While C-BOD is dataset-agnostic, its limitations include potential overlooking of deeper overfitting types and high computational costs for iterative rephrasing in training.\n\n2. KEY HIGHLIGHTS:\n*   The C-BOD framework systematically rephrases benchmark prompts to detect LLMs over-reliant on surface cues, revealing an average 2.15% performance degradation across 26 models on MMLU.\n*   Larger models and those with higher initial accuracy exhibit greater performance drops under rephrasing, indicating a tendency to overfit to specific prompt patterns.\n*   The Llama family models demonstrated more robust performance, suggesting reduced dependency on superficial cues compared to other families.\n*   LLM overfitting is attributed to memorizing specific question formats or keywords rather than genuine language understanding, leading to performance drops when phrasing changes.\n*   C-BOD's effectiveness is limited by its focus on textual rephrasing, potentially missing deeper overfitting types, and its significant computational cost for integration into training pipelines.",
      "qa_evaluations": [
        {
          "qa": {
            "question": "According to the study's findings, which specific LLM family demonstrated the most consistent robustness by maintaining relatively stable accuracy on perturbed prompts?",
            "answer": "The Llama family demonstrated the most consistent robustness by maintaining relatively stable accuracy on perturbed prompts."
          },
          "result": true,
          "issue": null
        },
        {
          "qa": {
            "question": "What trend was observed regarding performance degradation as model size increased within the Gemma family specifically?",
            "answer": "Not enough information in summary"
          },
          "result": false,
          "issue": "Not enough information"
        },
        {
          "qa": {
            "question": "What paradoxical relationship did the study uncover between a model's baseline accuracy on the original MMLU benchmark and its robustness to prompt rephrasing?",
            "answer": "The study uncovered that models with higher baseline accuracy on the original MMLU benchmark tend to be more sensitive to prompt rephrasing, indicating they overfit to fixed prompt patterns."
          },
          "result": true,
          "issue": null
        },
        {
          "qa": {
            "question": "The paper contrasts the Llama family's stability with other models. What does this imply about how architectural choices or training strategies might influence a model's reliance on superficial cues versus genuine understanding?",
            "answer": "The Llama family's stability implies that their architectural choices or training strategies might lead to a reduced dependency on superficial cues, suggesting a move towards more genuine understanding compared to other models."
          },
          "result": true,
          "issue": null
        },
        {
          "qa": {
            "question": "Besides the Llama family, some other models like Falcon 7B showed statistically insignificant differences. What reason does the paper provide for their apparent robustness?",
            "answer": "Not enough information in summary"
          },
          "result": false,
          "issue": "Not enough information"
        },
        {
          "qa": {
            "question": "ACU. Which LLM family is noted in the abstract for showing insignificant performance degradation?",
            "answer": "The Llama family is noted for showing insignificant performance degradation."
          },
          "result": true,
          "issue": null
        },
        {
          "qa": {
            "question": "ACU. How many of the 26 evaluated models exhibited statistically significant performance differences under perturbation?",
            "answer": "Most of the 26 evaluated models exhibited statistically significant performance differences under perturbation."
          },
          "result": true,
          "issue": null
        },
        {
          "qa": {
            "question": "ACU. What was the average drop in accuracy across all evaluated models when tested on the rephrased prompts?",
            "answer": "The average drop in accuracy across all evaluated models when tested on the rephrased prompts was 2.15%."
          },
          "result": true,
          "issue": null
        }
      ],
      "qa_pairs": [
        {
          "question": "According to the study's findings, which specific LLM family demonstrated the most consistent robustness by maintaining relatively stable accuracy on perturbed prompts?",
          "answer": "The Llama family demonstrated the most consistent robustness by maintaining relatively stable accuracy on perturbed prompts."
        },
        {
          "question": "What trend was observed regarding performance degradation as model size increased within the Gemma family specifically?",
          "answer": "Not enough information in summary"
        },
        {
          "question": "What paradoxical relationship did the study uncover between a model's baseline accuracy on the original MMLU benchmark and its robustness to prompt rephrasing?",
          "answer": "The study uncovered that models with higher baseline accuracy on the original MMLU benchmark tend to be more sensitive to prompt rephrasing, indicating they overfit to fixed prompt patterns."
        },
        {
          "question": "The paper contrasts the Llama family's stability with other models. What does this imply about how architectural choices or training strategies might influence a model's reliance on superficial cues versus genuine understanding?",
          "answer": "The Llama family's stability implies that their architectural choices or training strategies might lead to a reduced dependency on superficial cues, suggesting a move towards more genuine understanding compared to other models."
        },
        {
          "question": "Besides the Llama family, some other models like Falcon 7B showed statistically insignificant differences. What reason does the paper provide for their apparent robustness?",
          "answer": "Not enough information in summary"
        },
        {
          "question": "ACU. Which LLM family is noted in the abstract for showing insignificant performance degradation?",
          "answer": "The Llama family is noted for showing insignificant performance degradation."
        },
        {
          "question": "ACU. How many of the 26 evaluated models exhibited statistically significant performance differences under perturbation?",
          "answer": "Most of the 26 evaluated models exhibited statistically significant performance differences under perturbation."
        },
        {
          "question": "ACU. What was the average drop in accuracy across all evaluated models when tested on the rephrased prompts?",
          "answer": "The average drop in accuracy across all evaluated models when tested on the rephrased prompts was 2.15%."
        }
      ],
      "judge": {
        "evaluations": [
          {
            "qa": {
              "question": "According to the study's findings, which specific LLM family demonstrated the most consistent robustness by maintaining relatively stable accuracy on perturbed prompts?",
              "answer": "The Llama family demonstrated the most consistent robustness by maintaining relatively stable accuracy on perturbed prompts."
            },
            "result": true,
            "issue": null
          },
          {
            "qa": {
              "question": "What trend was observed regarding performance degradation as model size increased within the Gemma family specifically?",
              "answer": "Not enough information in summary"
            },
            "result": false,
            "issue": "The summary states that 'larger models tend to exhibit greater performance degradation' and specifically mentions the Gemma family showing a 'progressive increase in ∆1.0 with higher parameter counts', but this specific trend for Gemma is not in the summary."
          },
          {
            "qa": {
              "question": "What paradoxical relationship did the study uncover between a model's baseline accuracy on the original MMLU benchmark and its robustness to prompt rephrasing?",
              "answer": "The study uncovered that models with higher baseline accuracy on the original MMLU benchmark tend to be more sensitive to prompt rephrasing, indicating they overfit to fixed prompt patterns."
            },
            "result": true,
            "issue": null
          },
          {
            "qa": {
              "question": "The paper contrasts the Llama family's stability with other models. What does this imply about how architectural choices or training strategies might influence a model's reliance on superficial cues versus genuine understanding?",
              "answer": "The Llama family's stability implies that their architectural choices or training strategies might lead to a reduced dependency on superficial cues, suggesting a move towards more genuine understanding compared to other models."
            },
            "result": true,
            "issue": null
          },
          {
            "qa": {
              "question": "Besides the Llama family, some other models like Falcon 7B showed statistically insignificant differences. What reason does the paper provide for their apparent robustness?",
              "answer": "Not enough information in summary"
            },
            "result": false,
            "issue": "The summary mentions that 'Falcon 7B, DeepSeek 7B, Qwen 2.5 3B and Jetmoe 8B show statistically insignificant differences, likely due to their lower baseline accuracy.' This information is missing from the summary."
          },
          {
            "qa": {
              "question": "ACU. Which LLM family is noted in the abstract for showing insignificant performance degradation?",
              "answer": "The Llama family is noted for showing insignificant performance degradation."
            },
            "result": true,
            "issue": null
          },
          {
            "qa": {
              "question": "ACU. How many of the 26 evaluated models exhibited statistically significant performance differences under perturbation?",
              "answer": "Most of the 26 evaluated models exhibited statistically significant performance differences under perturbation."
            },
            "result": true,
            "issue": null
          },
          {
            "qa": {
              "question": "ACU. What was the average drop in accuracy across all evaluated models when tested on the rephrased prompts?",
              "answer": "The average drop in accuracy across all evaluated models when tested on the rephrased prompts was 2.15%."
            },
            "result": true,
            "issue": null
          }
        ],
        "judgment": false,
        "sections_to_highlight": [
          "Results — performance degradation by model size",
          "Results — performance difference vs. baseline accuracy",
          "Methodology — C-BOD framework details",
          "Limitations — computational cost of training integration",
          "Discussion — reasons for LLM overfitting",
          "LLM families — comparative robustness analysis"
        ]
      },
      "correct_count_all": 6,
      "correct_count_acu": 3,
      "num_of_questions": 5,
      "sections_to_highlight": [
        "Results — performance degradation by model size",
        "Results — performance difference vs. baseline accuracy",
        "Methodology — C-BOD framework details",
        "Limitations — computational cost of training integration",
        "Discussion — reasons for LLM overfitting",
        "LLM families — comparative robustness analysis"
      ],
      "sections_to_highlight_size": 6,
      "qa_failures_present": true
    },
    {
      "iteration_number": 4,
      "summary": "1. SUMMARY:\nThis paper introduces the Chameleon Benchmark Overfit Detector (C-BOD), a meta-evaluation framework designed to identify LLM overfitting to surface-level cues in benchmark datasets. C-BOD systematically distorts benchmark prompts while preserving semantic content and labels, using a distortion parameter (µ) to control the extent of rephrasing. Evaluating 26 LLMs on the MMLU benchmark with µ=1.0, the study found an average accuracy degradation of 2.15% across models, with 20 exhibiting statistically significant performance drops. Notably, larger LLMs and those with higher baseline accuracy showed greater sensitivity to rephrasing, indicating a potential overreliance on fixed prompt patterns. Conversely, the Llama family and models with lower baseline accuracy demonstrated more robust performance, suggesting reduced dependency on superficial cues. The research highlights that high benchmark scores may mask an inability to generalize due to overfitting, urging a shift towards evaluating LLM resilience.\n\n2. KEY HIGHLIGHTS:\n*   C-BOD framework uses parametric prompt distortion to detect LLM overfitting to surface cues, revealing an average 2.15% accuracy drop across 26 LLMs on MMLU.\n*   Larger LLMs and those with higher baseline accuracy exhibited greater performance degradation under prompt rephrasing, suggesting over-reliance on specific prompt patterns.\n*   The Llama family and models with lower baseline accuracy demonstrated more robustness, indicating less dependency on superficial cues.\n*   Overfitting is attributed to LLMs associating question formats/keywords with answers instead of generalizing concepts, leading to performance drops when phrasing changes.",
      "qa_evaluations": [
        {
          "qa": {
            "question": "According to the study's findings, which specific LLM family demonstrated the most consistent robustness by maintaining relatively stable accuracy on perturbed prompts?",
            "answer": "The Llama family demonstrated the most consistent robustness by maintaining relatively stable accuracy on perturbed prompts."
          },
          "result": true,
          "issue": null
        },
        {
          "qa": {
            "question": "What trend was observed regarding performance degradation as model size increased within the Gemma family specifically?",
            "answer": "Not enough information in summary"
          },
          "result": false,
          "issue": "Not enough information"
        },
        {
          "qa": {
            "question": "What paradoxical relationship did the study uncover between a model's baseline accuracy on the original MMLU benchmark and its robustness to prompt rephrasing?",
            "answer": "The study uncovered that models with higher baseline accuracy showed greater sensitivity to rephrasing, indicating a potential overreliance on fixed prompt patterns, while models with lower baseline accuracy demonstrated more robustness, indicating less dependency on superficial cues."
          },
          "result": true,
          "issue": null
        },
        {
          "qa": {
            "question": "The paper contrasts the Llama family's stability with other models. What does this imply about how architectural choices or training strategies might influence a model's reliance on superficial cues versus genuine understanding?",
            "answer": "The Llama family's stability implies that architectural choices or training strategies might influence a model's reliance on superficial cues versus genuine understanding, suggesting that the Llama family relies less on superficial cues."
          },
          "result": true,
          "issue": null
        },
        {
          "qa": {
            "question": "Besides the Llama family, some other models like Falcon 7B showed statistically insignificant differences. What reason does the paper provide for their apparent robustness?",
            "answer": "The summary does not provide a specific reason for the apparent robustness of models like Falcon 7B, only that they showed statistically insignificant differences."
          },
          "result": false,
          "issue": "Not enough information"
        },
        {
          "qa": {
            "question": "ACU. Which LLM family is noted in the abstract for showing insignificant performance degradation?",
            "answer": "The Llama family is noted for showing insignificant performance degradation."
          },
          "result": true,
          "issue": null
        },
        {
          "qa": {
            "question": "ACU. How many of the 26 evaluated models exhibited statistically significant performance differences under perturbation?",
            "answer": "20 out of the 26 evaluated models exhibited statistically significant performance differences under perturbation."
          },
          "result": true,
          "issue": null
        },
        {
          "qa": {
            "question": "ACU. What was the average drop in accuracy across all evaluated models when tested on the rephrased prompts?",
            "answer": "The average drop in accuracy across all evaluated models when tested on the rephrased prompts was 2.15%."
          },
          "result": true,
          "issue": null
        }
      ],
      "qa_pairs": [
        {
          "question": "According to the study's findings, which specific LLM family demonstrated the most consistent robustness by maintaining relatively stable accuracy on perturbed prompts?",
          "answer": "The Llama family demonstrated the most consistent robustness by maintaining relatively stable accuracy on perturbed prompts."
        },
        {
          "question": "What trend was observed regarding performance degradation as model size increased within the Gemma family specifically?",
          "answer": "Not enough information in summary"
        },
        {
          "question": "What paradoxical relationship did the study uncover between a model's baseline accuracy on the original MMLU benchmark and its robustness to prompt rephrasing?",
          "answer": "The study uncovered that models with higher baseline accuracy showed greater sensitivity to rephrasing, indicating a potential overreliance on fixed prompt patterns, while models with lower baseline accuracy demonstrated more robustness, indicating less dependency on superficial cues."
        },
        {
          "question": "The paper contrasts the Llama family's stability with other models. What does this imply about how architectural choices or training strategies might influence a model's reliance on superficial cues versus genuine understanding?",
          "answer": "The Llama family's stability implies that architectural choices or training strategies might influence a model's reliance on superficial cues versus genuine understanding, suggesting that the Llama family relies less on superficial cues."
        },
        {
          "question": "Besides the Llama family, some other models like Falcon 7B showed statistically insignificant differences. What reason does the paper provide for their apparent robustness?",
          "answer": "The summary does not provide a specific reason for the apparent robustness of models like Falcon 7B, only that they showed statistically insignificant differences."
        },
        {
          "question": "ACU. Which LLM family is noted in the abstract for showing insignificant performance degradation?",
          "answer": "The Llama family is noted for showing insignificant performance degradation."
        },
        {
          "question": "ACU. How many of the 26 evaluated models exhibited statistically significant performance differences under perturbation?",
          "answer": "20 out of the 26 evaluated models exhibited statistically significant performance differences under perturbation."
        },
        {
          "question": "ACU. What was the average drop in accuracy across all evaluated models when tested on the rephrased prompts?",
          "answer": "The average drop in accuracy across all evaluated models when tested on the rephrased prompts was 2.15%."
        }
      ],
      "judge": {
        "evaluations": [
          {
            "qa": {
              "question": "According to the study's findings, which specific LLM family demonstrated the most consistent robustness by maintaining relatively stable accuracy on perturbed prompts?",
              "answer": "The Llama family demonstrated the most consistent robustness by maintaining relatively stable accuracy on perturbed prompts."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "What trend was observed regarding performance degradation as model size increased within the Gemma family specifically?",
              "answer": "Not enough information in summary"
            },
            "result": false,
            "issue": "The summary states that larger models tend to exhibit greater performance degradation and that models in the Gemma family show a progressive increase in performance difference with higher parameter counts, but it does not specifically detail the trend within the Gemma family as requested by the question."
          },
          {
            "qa": {
              "question": "What paradoxical relationship did the study uncover between a model's baseline accuracy on the original MMLU benchmark and its robustness to prompt rephrasing?",
              "answer": "The study uncovered that models with higher baseline accuracy showed greater sensitivity to rephrasing, indicating a potential overreliance on fixed prompt patterns, while models with lower baseline accuracy demonstrated more robustness, indicating less dependency on superficial cues."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "The paper contrasts the Llama family's stability with other models. What does this imply about how architectural choices or training strategies might influence a model's reliance on superficial cues versus genuine understanding?",
              "answer": "The Llama family's stability implies that architectural choices or training strategies might influence a model's reliance on superficial cues versus genuine understanding, suggesting that the Llama family relies less on superficial cues."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "Besides the Llama family, some other models like Falcon 7B showed statistically insignificant differences. What reason does the paper provide for their apparent robustness?",
              "answer": "The summary does not provide a specific reason for the apparent robustness of models like Falcon 7B, only that they showed statistically insignificant differences."
            },
            "result": false,
            "issue": "The summary does not provide the reason for the apparent robustness of models like Falcon 7B. The article states that these models showed statistically insignificant differences 'likely due to their lower baseline accuracy'."
          },
          {
            "qa": {
              "question": "ACU. Which LLM family is noted in the abstract for showing insignificant performance degradation?",
              "answer": "The Llama family is noted for showing insignificant performance degradation."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "ACU. How many of the 26 evaluated models exhibited statistically significant performance differences under perturbation?",
              "answer": "20 out of the 26 evaluated models exhibited statistically significant performance differences under perturbation."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "ACU. What was the average drop in accuracy across all evaluated models when tested on the rephrased prompts?",
              "answer": "The average drop in accuracy across all evaluated models when tested on the rephrased prompts was 2.15%."
            },
            "result": true,
            "issue": ""
          }
        ],
        "judgment": false,
        "sections_to_highlight": [
          "Gemma Family Performance — detail degradation trend with size",
          "Other Robust Models — explain reasons for robustness (e.g., Falcon 7B)",
          "Methodology — limitations of C-BOD (e.g., textual focus)",
          "Results — specific performance differences for key models",
          "Overfitting Causes — link to specific phrasing/keywords",
          "Model Size vs. Overfitting — relationship details",
          "Baseline Accuracy vs. Overfitting — relationship details"
        ]
      },
      "correct_count_all": 6,
      "correct_count_acu": 3,
      "num_of_questions": 5,
      "sections_to_highlight": [
        "Gemma Family Performance — detail degradation trend with size",
        "Other Robust Models — explain reasons for robustness (e.g., Falcon 7B)",
        "Methodology — limitations of C-BOD (e.g., textual focus)",
        "Results — specific performance differences for key models",
        "Overfitting Causes — link to specific phrasing/keywords",
        "Model Size vs. Overfitting — relationship details",
        "Baseline Accuracy vs. Overfitting — relationship details"
      ],
      "sections_to_highlight_size": 7,
      "qa_failures_present": true
    },
    {
      "iteration_number": 5,
      "summary": "1. SUMMARY:\nThis research introduces the Chameleon Benchmark Overfit Detector (C-BOD), a meta-evaluation framework designed to identify LLMs that overfit to specific surface cues in benchmark prompts. By systematically rephrasing benchmark questions while preserving semantic meaning, C-BOD quantifies performance degradation under textual perturbations. The study evaluated 26 LLMs on the MMLU benchmark, revealing that most models experience a statistically significant performance drop, averaging 2.15%. Notably, larger models and those with higher baseline accuracy showed a greater tendency to overfit, suggesting a reliance on memorized patterns. In contrast, the Llama family and models with lower baseline accuracy demonstrated more robustness, indicating a reduced dependency on superficial cues. C-BOD's dataset- and model-agnostic design allows for broad applicability and integration into training pipelines to promote more generalizable language understanding. The findings challenge the community to look beyond leaderboard scores and prioritize resilience in LLM evaluation.\n\n2. KEY HIGHLIGHTS:\n*   A significant majority of LLMs (20 out of 26) showed performance degradation when benchmark prompts were rephrased, indicating overfitting to specific phrasing.\n*   Larger LLMs and those with higher initial benchmark accuracy exhibited a greater propensity to overfit, suggesting these models may rely more on memorized patterns.\n*   The Llama family of models and those with lower baseline accuracy demonstrated greater robustness, implying less reliance on superficial cues.\n*   The C-BOD framework systematically distorts prompts while preserving semantics to detect this overfitting, revealing an average accuracy drop of 2.15% across evaluated models.",
      "qa_evaluations": [
        {
          "qa": {
            "question": "According to the study's findings, which specific LLM family demonstrated the most consistent robustness by maintaining relatively stable accuracy on perturbed prompts?",
            "answer": "The Llama family of models demonstrated the most consistent robustness by maintaining relatively stable accuracy on perturbed prompts."
          },
          "result": true,
          "issue": null
        },
        {
          "qa": {
            "question": "What trend was observed regarding performance degradation as model size increased within the Gemma family specifically?",
            "answer": "Not enough information in summary"
          },
          "result": false,
          "issue": "Not enough information"
        },
        {
          "qa": {
            "question": "What paradoxical relationship did the study uncover between a model's baseline accuracy on the original MMLU benchmark and its robustness to prompt rephrasing?",
            "answer": "The study uncovered a paradoxical relationship where models with higher baseline accuracy showed a greater tendency to overfit and thus less robustness to prompt rephrasing, while models with lower baseline accuracy demonstrated greater robustness."
          },
          "result": true,
          "issue": null
        },
        {
          "qa": {
            "question": "The paper contrasts the Llama family's stability with other models. What does this imply about how architectural choices or training strategies might influence a model's reliance on superficial cues versus genuine understanding?",
            "answer": "The Llama family's stability implies that their architectural choices or training strategies may lead to a reduced dependency on superficial cues and a greater reliance on genuine understanding compared to other models."
          },
          "result": true,
          "issue": null
        },
        {
          "qa": {
            "question": "Besides the Llama family, some other models like Falcon 7B showed statistically insignificant differences. What reason does the paper provide for their apparent robustness?",
            "answer": "Not enough information in summary"
          },
          "result": false,
          "issue": "Not enough information"
        },
        {
          "qa": {
            "question": "ACU. Which LLM family is noted in the abstract for showing insignificant performance degradation?",
            "answer": "The Llama family is noted for showing insignificant performance degradation."
          },
          "result": true,
          "issue": null
        },
        {
          "qa": {
            "question": "ACU. How many of the 26 evaluated models exhibited statistically significant performance differences under perturbation?",
            "answer": "20 out of the 26 evaluated models exhibited statistically significant performance differences under perturbation."
          },
          "result": true,
          "issue": null
        },
        {
          "qa": {
            "question": "ACU. What was the average drop in accuracy across all evaluated models when tested on the rephrased prompts?",
            "answer": "The average drop in accuracy across all evaluated models when tested on the rephrased prompts was 2.15%."
          },
          "result": true,
          "issue": null
        }
      ],
      "qa_pairs": [
        {
          "question": "According to the study's findings, which specific LLM family demonstrated the most consistent robustness by maintaining relatively stable accuracy on perturbed prompts?",
          "answer": "The Llama family of models demonstrated the most consistent robustness by maintaining relatively stable accuracy on perturbed prompts."
        },
        {
          "question": "What trend was observed regarding performance degradation as model size increased within the Gemma family specifically?",
          "answer": "Not enough information in summary"
        },
        {
          "question": "What paradoxical relationship did the study uncover between a model's baseline accuracy on the original MMLU benchmark and its robustness to prompt rephrasing?",
          "answer": "The study uncovered a paradoxical relationship where models with higher baseline accuracy showed a greater tendency to overfit and thus less robustness to prompt rephrasing, while models with lower baseline accuracy demonstrated greater robustness."
        },
        {
          "question": "The paper contrasts the Llama family's stability with other models. What does this imply about how architectural choices or training strategies might influence a model's reliance on superficial cues versus genuine understanding?",
          "answer": "The Llama family's stability implies that their architectural choices or training strategies may lead to a reduced dependency on superficial cues and a greater reliance on genuine understanding compared to other models."
        },
        {
          "question": "Besides the Llama family, some other models like Falcon 7B showed statistically insignificant differences. What reason does the paper provide for their apparent robustness?",
          "answer": "Not enough information in summary"
        },
        {
          "question": "ACU. Which LLM family is noted in the abstract for showing insignificant performance degradation?",
          "answer": "The Llama family is noted for showing insignificant performance degradation."
        },
        {
          "question": "ACU. How many of the 26 evaluated models exhibited statistically significant performance differences under perturbation?",
          "answer": "20 out of the 26 evaluated models exhibited statistically significant performance differences under perturbation."
        },
        {
          "question": "ACU. What was the average drop in accuracy across all evaluated models when tested on the rephrased prompts?",
          "answer": "The average drop in accuracy across all evaluated models when tested on the rephrased prompts was 2.15%."
        }
      ],
      "judge": {
        "evaluations": [
          {
            "qa": {
              "question": "According to the study's findings, which specific LLM family demonstrated the most consistent robustness by maintaining relatively stable accuracy on perturbed prompts?",
              "answer": "The Llama family of models demonstrated the most consistent robustness by maintaining relatively stable accuracy on perturbed prompts."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "What trend was observed regarding performance degradation as model size increased within the Gemma family specifically?",
              "answer": "Not enough information in summary"
            },
            "result": false,
            "issue": "The article mentions that 'models in the Gemma family show a progressive increase in \\u22061.0 with higher parameter counts', which is a trend related to performance degradation and model size, but it is not present in the summary."
          },
          {
            "qa": {
              "question": "What paradoxical relationship did the study uncover between a model's baseline accuracy on the original MMLU benchmark and its robustness to prompt rephrasing?",
              "answer": "The study uncovered a paradoxical relationship where models with higher baseline accuracy showed a greater tendency to overfit and thus less robustness to prompt rephrasing, while models with lower baseline accuracy demonstrated greater robustness."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "The paper contrasts the Llama family's stability with other models. What does this imply about how architectural choices or training strategies might influence a model's reliance on superficial cues versus genuine understanding?",
              "answer": "The Llama family's stability implies that their architectural choices or training strategies may lead to a reduced dependency on superficial cues and a greater reliance on genuine understanding compared to other models."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "Besides the Llama family, some other models like Falcon 7B showed statistically insignificant differences. What reason does the paper provide for their apparent robustness?",
              "answer": "Not enough information in summary"
            },
            "result": false,
            "issue": "The article states that 'Falcon 7B, DeepSeek 7B, Qwen 2.5 3B and Jetmoe 8B show statistically insignificant differences, likely due to their lower baseline accuracy.' This information is missing from the summary."
          },
          {
            "qa": {
              "question": "ACU. Which LLM family is noted in the abstract for showing insignificant performance degradation?",
              "answer": "The Llama family is noted for showing insignificant performance degradation."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "ACU. How many of the 26 evaluated models exhibited statistically significant performance differences under perturbation?",
              "answer": "20 out of the 26 evaluated models exhibited statistically significant performance differences under perturbation."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "ACU. What was the average drop in accuracy across all evaluated models when tested on the rephrased prompts?",
              "answer": "The average drop in accuracy across all evaluated models when tested on the rephrased prompts was 2.15%."
            },
            "result": true,
            "issue": ""
          }
        ],
        "judgment": false,
        "sections_to_highlight": [
          "Methodology — C-BOD framework details",
          "Results — Gemma family performance trends",
          "Results — Factors for insignificant performance differences",
          "Discussion — Why LLMs overfit",
          "Limitations — Scope of C-BOD detection"
        ]
      },
      "correct_count_all": 6,
      "correct_count_acu": 3,
      "num_of_questions": 5,
      "sections_to_highlight": [
        "Methodology — C-BOD framework details",
        "Results — Gemma family performance trends",
        "Results — Factors for insignificant performance differences",
        "Discussion — Why LLMs overfit",
        "Limitations — Scope of C-BOD detection"
      ],
      "sections_to_highlight_size": 5,
      "qa_failures_present": true
    },
    {
      "iteration_number": 6,
      "summary": "1. SUMMARY:\nThis paper introduces the Chameleon Benchmark Overfit Detector (C-BOD) to assess Large Language Model (LLM) robustness against prompt variations, revealing that many LLMs overfit to surface cues in benchmarks like MMLU. C-BOD systematically rephrases prompts while preserving semantic meaning and evaluates performance changes. The study found that 20 out of 26 evaluated LLMs exhibited statistically significant performance degradation (average 2.15%) under modest rephrasing. Notably, larger models and those with higher baseline accuracy showed a greater tendency to overfit, suggesting reliance on fixed prompt patterns. In contrast, the Llama family and models with lower baseline accuracy demonstrated reduced dependency on superficial cues, indicating greater robustness. The research highlights a paradox where high benchmark scores may mask underlying vulnerabilities to prompt variations, urging a shift towards evaluating resilience and generalization.\n\n2. KEY HIGHLIGHTS:\n*   LLMs often overfit to benchmark-specific surface cues, leading to performance drops when prompts are rephrased.\n*   Larger LLMs and those with higher baseline accuracy tend to be more sensitive to prompt perturbations, indicating greater overfitting.\n*   The Llama family and models with lower baseline accuracy showed more robustness to rephrased prompts.\n*   C-BOD is a dataset-agnostic framework that systematically distorts prompts to detect overfitting, revealing that many models rely on memorized patterns rather than true understanding.\n*   High benchmark scores can be misleading, as they may not reflect true generalization capabilities due to this overfitting.",
      "qa_evaluations": [
        {
          "qa": {
            "question": "According to the study's findings, which specific LLM family demonstrated the most consistent robustness by maintaining relatively stable accuracy on perturbed prompts?",
            "answer": "The Llama family demonstrated the most consistent robustness by maintaining relatively stable accuracy on perturbed prompts."
          },
          "result": true,
          "issue": null
        },
        {
          "qa": {
            "question": "What trend was observed regarding performance degradation as model size increased within the Gemma family specifically?",
            "answer": "Not enough information in summary"
          },
          "result": false,
          "issue": "Not enough information"
        },
        {
          "qa": {
            "question": "What paradoxical relationship did the study uncover between a model's baseline accuracy on the original MMLU benchmark and its robustness to prompt rephrasing?",
            "answer": "The study uncovered a paradox where models with higher baseline accuracy on the MMLU benchmark showed a greater tendency to overfit to prompt variations, indicating less robustness to prompt rephrasing. Conversely, models with lower baseline accuracy demonstrated greater robustness."
          },
          "result": true,
          "issue": null
        },
        {
          "qa": {
            "question": "The paper contrasts the Llama family's stability with other models. What does this imply about how architectural choices or training strategies might influence a model's reliance on superficial cues versus genuine understanding?",
            "answer": "The Llama family's stability implies that their architectural choices or training strategies might lead to a reduced reliance on superficial cues and a greater emphasis on genuine understanding, making them more robust to prompt variations compared to other models."
          },
          "result": true,
          "issue": null
        },
        {
          "qa": {
            "question": "Besides the Llama family, some other models like Falcon 7B showed statistically insignificant differences. What reason does the paper provide for their apparent robustness?",
            "answer": "Not enough information in summary"
          },
          "result": false,
          "issue": "Not enough information"
        },
        {
          "qa": {
            "question": "ACU. Which LLM family is noted in the abstract for showing insignificant performance degradation?",
            "answer": "The Llama family is noted for showing insignificant performance degradation."
          },
          "result": true,
          "issue": null
        },
        {
          "qa": {
            "question": "ACU. How many of the 26 evaluated models exhibited statistically significant performance differences under perturbation?",
            "answer": "20 out of 26 evaluated models exhibited statistically significant performance degradation."
          },
          "result": true,
          "issue": null
        },
        {
          "qa": {
            "question": "ACU. What was the average drop in accuracy across all evaluated models when tested on the rephrased prompts?",
            "answer": "The average drop in accuracy across all evaluated models when tested on the rephrased prompts was 2.15%."
          },
          "result": true,
          "issue": null
        }
      ],
      "qa_pairs": [
        {
          "question": "According to the study's findings, which specific LLM family demonstrated the most consistent robustness by maintaining relatively stable accuracy on perturbed prompts?",
          "answer": "The Llama family demonstrated the most consistent robustness by maintaining relatively stable accuracy on perturbed prompts."
        },
        {
          "question": "What trend was observed regarding performance degradation as model size increased within the Gemma family specifically?",
          "answer": "Not enough information in summary"
        },
        {
          "question": "What paradoxical relationship did the study uncover between a model's baseline accuracy on the original MMLU benchmark and its robustness to prompt rephrasing?",
          "answer": "The study uncovered a paradox where models with higher baseline accuracy on the MMLU benchmark showed a greater tendency to overfit to prompt variations, indicating less robustness to prompt rephrasing. Conversely, models with lower baseline accuracy demonstrated greater robustness."
        },
        {
          "question": "The paper contrasts the Llama family's stability with other models. What does this imply about how architectural choices or training strategies might influence a model's reliance on superficial cues versus genuine understanding?",
          "answer": "The Llama family's stability implies that their architectural choices or training strategies might lead to a reduced reliance on superficial cues and a greater emphasis on genuine understanding, making them more robust to prompt variations compared to other models."
        },
        {
          "question": "Besides the Llama family, some other models like Falcon 7B showed statistically insignificant differences. What reason does the paper provide for their apparent robustness?",
          "answer": "Not enough information in summary"
        },
        {
          "question": "ACU. Which LLM family is noted in the abstract for showing insignificant performance degradation?",
          "answer": "The Llama family is noted for showing insignificant performance degradation."
        },
        {
          "question": "ACU. How many of the 26 evaluated models exhibited statistically significant performance differences under perturbation?",
          "answer": "20 out of 26 evaluated models exhibited statistically significant performance degradation."
        },
        {
          "question": "ACU. What was the average drop in accuracy across all evaluated models when tested on the rephrased prompts?",
          "answer": "The average drop in accuracy across all evaluated models when tested on the rephrased prompts was 2.15%."
        }
      ],
      "judge": {
        "evaluations": [
          {
            "qa": {
              "question": "According to the study's findings, which specific LLM family demonstrated the most consistent robustness by maintaining relatively stable accuracy on perturbed prompts?",
              "answer": "The Llama family demonstrated the most consistent robustness by maintaining relatively stable accuracy on perturbed prompts."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "What trend was observed regarding performance degradation as model size increased within the Gemma family specifically?",
              "answer": "Not enough information in summary"
            },
            "result": false,
            "issue": "The summary states that 'larger models tend to exhibit greater performance degradation' and specifically mentions the Gemma family showing a 'progressive increase in \\u22061.0 with higher parameter counts', but does not provide specific details about the Gemma family's performance degradation trend as model size increased."
          },
          {
            "qa": {
              "question": "What paradoxical relationship did the study uncover between a model's baseline accuracy on the original MMLU benchmark and its robustness to prompt rephrasing?",
              "answer": "The study uncovered a paradox where models with higher baseline accuracy on the MMLU benchmark showed a greater tendency to overfit to prompt variations, indicating less robustness to prompt rephrasing. Conversely, models with lower baseline accuracy demonstrated greater robustness."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "The paper contrasts the Llama family's stability with other models. What does this imply about how architectural choices or training strategies might influence a model's reliance on superficial cues versus genuine understanding?",
              "answer": "The Llama family's stability implies that their architectural choices or training strategies might lead to a reduced reliance on superficial cues and a greater emphasis on genuine understanding, making them more robust to prompt variations compared to other models."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "Besides the Llama family, some other models like Falcon 7B showed statistically insignificant differences. What reason does the paper provide for their apparent robustness?",
              "answer": "Not enough information in summary"
            },
            "result": false,
            "issue": "The summary states that 'Falcon 7B, DeepSeek 7B, Qwen 2.5 3B and Jetmoe 8B show statistically insignificant differences, likely due to their lower baseline accuracy.' However, the provided summary does not elaborate on the specific reasons for their apparent robustness beyond a general statement about lower baseline accuracy."
          },
          {
            "qa": {
              "question": "ACU. Which LLM family is noted in the abstract for showing insignificant performance degradation?",
              "answer": "The Llama family is noted for showing insignificant performance degradation."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "ACU. How many of the 26 evaluated models exhibited statistically significant performance differences under perturbation?",
              "answer": "20 out of 26 evaluated models exhibited statistically significant performance degradation."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "ACU. What was the average drop in accuracy across all evaluated models when tested on the rephrased prompts?",
              "answer": "The average drop in accuracy across all evaluated models when tested on the rephrased prompts was 2.15%."
            },
            "result": true,
            "issue": ""
          }
        ],
        "judgment": false,
        "sections_to_highlight": [
          "Gemma Family Performance — degradation trends with model size",
          "Models with Statistically Insignificant Differences — specific reasons for robustness",
          "Methodology — dataset-agnostic nature and rephrasing process",
          "Results — performance differences across model families",
          "Discussion — implications of LLM overfitting for evaluation practices"
        ]
      },
      "correct_count_all": 6,
      "correct_count_acu": 3,
      "num_of_questions": 5,
      "sections_to_highlight": [
        "Gemma Family Performance — degradation trends with model size",
        "Models with Statistically Insignificant Differences — specific reasons for robustness",
        "Methodology — dataset-agnostic nature and rephrasing process",
        "Results — performance differences across model families",
        "Discussion — implications of LLM overfitting for evaluation practices"
      ],
      "sections_to_highlight_size": 5,
      "qa_failures_present": true
    },
    {
      "iteration_number": 7,
      "summary": "1. SUMMARY:\nThis paper introduces the Chameleon Benchmark Overfit Detector (C-BOD), a meta-evaluation framework designed to assess LLM robustness by detecting overfitting to dataset-specific surface cues. C-BOD works by systematically rephrasing benchmark prompts while preserving semantic meaning and evaluating performance differences. Applied to the MMLU benchmark across 26 LLMs, the study found that most models exhibit significant performance degradation under modest textual perturbations, indicating a reliance on memorized patterns rather than true language understanding. Notably, larger LLMs and those with higher baseline accuracy tended to be more sensitive to rephrasing, suggesting that high benchmark scores can be misleading. The Llama family and models with lower baseline accuracy demonstrated greater robustness, with insignificant performance drops. C-BOD's dataset-agnostic nature allows for broad applicability, though its focus on textual rephrasing is a limitation, and integrating it into training pipelines incurs computational costs. The findings challenge current LLM evaluation practices, emphasizing the need to prioritize resilience and generalization.\n\n2. KEY HIGHLIGHTS:\n*   LLMs often overfit to benchmark-specific surface cues, leading to performance drops (average 2.15%) when prompts are rephrased, as detected by the C-BOD framework.\n*   Larger LLMs and those with higher baseline accuracy on standard benchmarks are generally more susceptible to performance degradation under prompt perturbations.\n*   The Llama family of models and those with lower baseline accuracy showed greater robustness, indicating reduced dependency on superficial cues.\n*   C-BOD is dataset-agnostic, allowing its application across various benchmarks, but its focus on textual rephrasing may miss deeper forms of overfitting.\n*   High benchmark scores may mask underlying vulnerabilities, necessitating a shift in evaluation focus towards resilience and generalization.",
      "qa_evaluations": [
        {
          "qa": {
            "question": "According to the study's findings, which specific LLM family demonstrated the most consistent robustness by maintaining relatively stable accuracy on perturbed prompts?",
            "answer": "The Llama family of models demonstrated the most consistent robustness by maintaining relatively stable accuracy on perturbed prompts."
          },
          "result": true,
          "issue": null
        },
        {
          "qa": {
            "question": "What trend was observed regarding performance degradation as model size increased within the Gemma family specifically?",
            "answer": "The summary states that larger LLMs tend to be more sensitive to rephrasing, but does not specifically mention the Gemma family or provide a trend within that specific family."
          },
          "result": false,
          "issue": "Not enough information in summary regarding the Gemma family specifically."
        },
        {
          "qa": {
            "question": "What paradoxical relationship did the study uncover between a model's baseline accuracy on the original MMLU benchmark and its robustness to prompt rephrasing?",
            "answer": "The study uncovered a paradoxical relationship where models with higher baseline accuracy on the original MMLU benchmark tended to be more sensitive to prompt rephrasing, indicating that high benchmark scores can be misleading."
          },
          "result": true,
          "issue": null
        },
        {
          "qa": {
            "question": "The paper contrasts the Llama family's stability with other models. What does this imply about how architectural choices or training strategies might influence a model's reliance on superficial cues versus genuine understanding?",
            "answer": "The summary implies that the Llama family's stability, in contrast to other models, suggests that their architectural choices or training strategies might lead to a reduced dependency on superficial cues and a greater reliance on genuine understanding."
          },
          "result": true,
          "issue": null
        },
        {
          "qa": {
            "question": "Besides the Llama family, some other models like Falcon 7B showed statistically insignificant differences. What reason does the paper provide for their apparent robustness?",
            "answer": "The summary mentions that models with lower baseline accuracy showed greater robustness, but does not provide a specific reason for the apparent robustness of models like Falcon 7B besides their lower baseline accuracy."
          },
          "result": false,
          "issue": "The summary does not provide a specific reason for the apparent robustness of models like Falcon 7B, beyond stating that models with lower baseline accuracy showed greater robustness."
        },
        {
          "qa": {
            "question": "ACU. Which LLM family is noted in the abstract for showing insignificant performance degradation?",
            "answer": "The Llama family of models is noted for showing insignificant performance degradation."
          },
          "result": true,
          "issue": null
        },
        {
          "qa": {
            "question": "ACU. How many of the 26 evaluated models exhibited statistically significant performance differences under perturbation?",
            "answer": "The summary states that 'most models exhibit significant performance degradation,' but does not provide a specific number out of the 26 evaluated models."
          },
          "result": false,
          "issue": "Not enough information in summary to provide an exact number."
        },
        {
          "qa": {
            "question": "ACU. What was the average drop in accuracy across all evaluated models when tested on the rephrased prompts?",
            "answer": "The average drop in accuracy across all evaluated models when tested on the rephrased prompts was 2.15%."
          },
          "result": true,
          "issue": null
        }
      ],
      "qa_pairs": [
        {
          "question": "According to the study's findings, which specific LLM family demonstrated the most consistent robustness by maintaining relatively stable accuracy on perturbed prompts?",
          "answer": "The Llama family of models demonstrated the most consistent robustness by maintaining relatively stable accuracy on perturbed prompts."
        },
        {
          "question": "What trend was observed regarding performance degradation as model size increased within the Gemma family specifically?",
          "answer": "The summary states that larger LLMs tend to be more sensitive to rephrasing, but does not specifically mention the Gemma family or provide a trend within that specific family."
        },
        {
          "question": "What paradoxical relationship did the study uncover between a model's baseline accuracy on the original MMLU benchmark and its robustness to prompt rephrasing?",
          "answer": "The study uncovered a paradoxical relationship where models with higher baseline accuracy on the original MMLU benchmark tended to be more sensitive to prompt rephrasing, indicating that high benchmark scores can be misleading."
        },
        {
          "question": "The paper contrasts the Llama family's stability with other models. What does this imply about how architectural choices or training strategies might influence a model's reliance on superficial cues versus genuine understanding?",
          "answer": "The summary implies that the Llama family's stability, in contrast to other models, suggests that their architectural choices or training strategies might lead to a reduced dependency on superficial cues and a greater reliance on genuine understanding."
        },
        {
          "question": "Besides the Llama family, some other models like Falcon 7B showed statistically insignificant differences. What reason does the paper provide for their apparent robustness?",
          "answer": "The summary mentions that models with lower baseline accuracy showed greater robustness, but does not provide a specific reason for the apparent robustness of models like Falcon 7B besides their lower baseline accuracy."
        },
        {
          "question": "ACU. Which LLM family is noted in the abstract for showing insignificant performance degradation?",
          "answer": "The Llama family of models is noted for showing insignificant performance degradation."
        },
        {
          "question": "ACU. How many of the 26 evaluated models exhibited statistically significant performance differences under perturbation?",
          "answer": "The summary states that 'most models exhibit significant performance degradation,' but does not provide a specific number out of the 26 evaluated models."
        },
        {
          "question": "ACU. What was the average drop in accuracy across all evaluated models when tested on the rephrased prompts?",
          "answer": "The average drop in accuracy across all evaluated models when tested on the rephrased prompts was 2.15%."
        }
      ],
      "judge": {
        "evaluations": [
          {
            "qa": {
              "question": "According to the study's findings, which specific LLM family demonstrated the most consistent robustness by maintaining relatively stable accuracy on perturbed prompts?",
              "answer": "The Llama family of models demonstrated the most consistent robustness by maintaining relatively stable accuracy on perturbed prompts."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "What trend was observed regarding performance degradation as model size increased within the Gemma family specifically?",
              "answer": "The summary states that larger LLMs tend to be more sensitive to rephrasing, but does not specifically mention the Gemma family or provide a trend within that specific family."
            },
            "result": false,
            "issue": "The article does mention the Gemma family in relation to model size and performance degradation. Specifically, Figure 4 states: 'For example, models in the Gemma family show a progressive increase in ∆1.0 with higher parameter counts, while Llama models maintain low ∆1.0 values across scales.'"
          },
          {
            "qa": {
              "question": "What paradoxical relationship did the study uncover between a model's baseline accuracy on the original MMLU benchmark and its robustness to prompt rephrasing?",
              "answer": "The study uncovered a paradoxical relationship where models with higher baseline accuracy on the original MMLU benchmark tended to be more sensitive to prompt rephrasing, indicating that high benchmark scores can be misleading."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "The paper contrasts the Llama family's stability with other models. What does this imply about how architectural choices or training strategies might influence a model's reliance on superficial cues versus genuine understanding?",
              "answer": "The summary implies that the Llama family's stability, in contrast to other models, suggests that their architectural choices or training strategies might lead to a reduced dependency on superficial cues and a greater reliance on genuine understanding."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "Besides the Llama family, some other models like Falcon 7B showed statistically insignificant differences. What reason does the paper provide for their apparent robustness?",
              "answer": "The summary mentions that models with lower baseline accuracy showed greater robustness, but does not provide a specific reason for the apparent robustness of models like Falcon 7B besides their lower baseline accuracy."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "ACU. Which LLM family is noted in the abstract for showing insignificant performance degradation?",
              "answer": "The Llama family of models is noted for showing insignificant performance degradation."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "ACU. How many of the 26 evaluated models exhibited statistically significant performance differences under perturbation?",
              "answer": "The summary states that 'most models exhibit significant performance degradation,' but does not provide a specific number out of the 26 evaluated models."
            },
            "result": false,
            "issue": "The article explicitly states that '20 out of 26 models exhibiting statistically significant differences.'"
          },
          {
            "qa": {
              "question": "ACU. What was the average drop in accuracy across all evaluated models when tested on the rephrased prompts?",
              "answer": "The average drop in accuracy across all evaluated models when tested on the rephrased prompts was 2.15%."
            },
            "result": true,
            "issue": ""
          }
        ],
        "judgment": false,
        "sections_to_highlight": [
          "Model Size vs. Overfitting — performance degradation trends",
          "Model Accuracy vs. Overfitting — paradoxical relationship",
          "Methodology — C-BOD framework overview",
          "Results — overall performance degradation",
          "Key Findings — LLM overreliance on surface cues",
          "Limitations — textual rephrasing scope",
          "Limitations — computational cost of integration"
        ]
      },
      "correct_count_all": 6,
      "correct_count_acu": 2,
      "num_of_questions": 5,
      "sections_to_highlight": [
        "Model Size vs. Overfitting — performance degradation trends",
        "Model Accuracy vs. Overfitting — paradoxical relationship",
        "Methodology — C-BOD framework overview",
        "Results — overall performance degradation",
        "Key Findings — LLM overreliance on surface cues",
        "Limitations — textual rephrasing scope",
        "Limitations — computational cost of integration"
      ],
      "sections_to_highlight_size": 7,
      "qa_failures_present": true
    },
    {
      "iteration_number": 8,
      "summary": "1. SUMMARY:\nThis paper introduces the Chameleon Benchmark Overfit Detector (C-BOD), a meta-evaluation framework designed to identify LLM overreliance on dataset-specific surface cues rather than true language understanding. C-BOD systematically distorts benchmark prompts while preserving semantic content, revealing performance degradation when models are sensitive to rephrasing. Across 26 LLMs evaluated on MMLU, an average accuracy drop of 2.15% was observed under modest perturbations, with 20 models showing statistically significant differences. Notably, larger models and those with higher baseline accuracy tended to be more sensitive to these rephrasings, suggesting an overreliance on fixed prompt patterns. In contrast, the Llama family and models with lower baseline accuracy exhibited less degradation, indicating reduced dependency on superficial cues. The framework's dataset-agnostic nature allows for broad applicability in promoting more robust LLM evaluation and development.\n\n2. KEY HIGHLIGHTS:\n*   C-BOD detects LLM overfitting by measuring performance drops on rephrased prompts, revealing an average accuracy degradation of 2.15% across 26 models.\n*   Larger LLMs and those with higher baseline accuracy generally show greater performance degradation under rephrasing, suggesting overreliance on surface cues.\n*   The Llama family and models with lower baseline accuracy demonstrated more robustness, indicating less dependency on superficial prompt patterns.\n*   C-BOD is dataset-agnostic and can be integrated into evaluation pipelines to promote more resilient LLM development, moving beyond simple leaderboard scores.",
      "qa_evaluations": [
        {
          "qa": {
            "question": "According to the study's findings, which specific LLM family demonstrated the most consistent robustness by maintaining relatively stable accuracy on perturbed prompts?",
            "answer": "The Llama family demonstrated the most consistent robustness by maintaining relatively stable accuracy on perturbed prompts."
          },
          "result": true,
          "issue": null
        },
        {
          "qa": {
            "question": "What trend was observed regarding performance degradation as model size increased within the Gemma family specifically?",
            "answer": "Not enough information in summary"
          },
          "result": false,
          "issue": "Not enough information in summary"
        },
        {
          "qa": {
            "question": "What paradoxical relationship did the study uncover between a model's baseline accuracy on the original MMLU benchmark and its robustness to prompt rephrasing?",
            "answer": "The study uncovered a paradoxical relationship where models with higher baseline accuracy on the MMLU benchmark tended to be more sensitive to prompt rephrasing, indicating an overreliance on fixed prompt patterns, while models with lower baseline accuracy exhibited more robustness, indicating less dependency on superficial cues."
          },
          "result": true,
          "issue": null
        },
        {
          "qa": {
            "question": "The paper contrasts the Llama family's stability with other models. What does this imply about how architectural choices or training strategies might influence a model's reliance on superficial cues versus genuine understanding?",
            "answer": "The Llama family's stability, in contrast to other models, implies that their architectural choices or training strategies might lead to a reduced dependency on superficial cues and a greater reliance on genuine understanding."
          },
          "result": true,
          "issue": null
        },
        {
          "qa": {
            "question": "Besides the Llama family, some other models like Falcon 7B showed statistically insignificant differences. What reason does the paper provide for their apparent robustness?",
            "answer": "Not enough information in summary"
          },
          "result": false,
          "issue": "Not enough information in summary"
        },
        {
          "qa": {
            "question": "ACU. Which LLM family is noted in the abstract for showing insignificant performance degradation?",
            "answer": "The Llama family is noted for showing insignificant performance degradation."
          },
          "result": true,
          "issue": null
        },
        {
          "qa": {
            "question": "ACU. How many of the 26 evaluated models exhibited statistically significant performance differences under perturbation?",
            "answer": "20 models showed statistically significant differences under perturbation."
          },
          "result": true,
          "issue": null
        },
        {
          "qa": {
            "question": "ACU. What was the average drop in accuracy across all evaluated models when tested on the rephrased prompts?",
            "answer": "The average drop in accuracy across all evaluated models when tested on the rephrased prompts was 2.15%."
          },
          "result": true,
          "issue": null
        }
      ],
      "qa_pairs": [
        {
          "question": "According to the study's findings, which specific LLM family demonstrated the most consistent robustness by maintaining relatively stable accuracy on perturbed prompts?",
          "answer": "The Llama family demonstrated the most consistent robustness by maintaining relatively stable accuracy on perturbed prompts."
        },
        {
          "question": "What trend was observed regarding performance degradation as model size increased within the Gemma family specifically?",
          "answer": "Not enough information in summary"
        },
        {
          "question": "What paradoxical relationship did the study uncover between a model's baseline accuracy on the original MMLU benchmark and its robustness to prompt rephrasing?",
          "answer": "The study uncovered a paradoxical relationship where models with higher baseline accuracy on the MMLU benchmark tended to be more sensitive to prompt rephrasing, indicating an overreliance on fixed prompt patterns, while models with lower baseline accuracy exhibited more robustness, indicating less dependency on superficial cues."
        },
        {
          "question": "The paper contrasts the Llama family's stability with other models. What does this imply about how architectural choices or training strategies might influence a model's reliance on superficial cues versus genuine understanding?",
          "answer": "The Llama family's stability, in contrast to other models, implies that their architectural choices or training strategies might lead to a reduced dependency on superficial cues and a greater reliance on genuine understanding."
        },
        {
          "question": "Besides the Llama family, some other models like Falcon 7B showed statistically insignificant differences. What reason does the paper provide for their apparent robustness?",
          "answer": "Not enough information in summary"
        },
        {
          "question": "ACU. Which LLM family is noted in the abstract for showing insignificant performance degradation?",
          "answer": "The Llama family is noted for showing insignificant performance degradation."
        },
        {
          "question": "ACU. How many of the 26 evaluated models exhibited statistically significant performance differences under perturbation?",
          "answer": "20 models showed statistically significant differences under perturbation."
        },
        {
          "question": "ACU. What was the average drop in accuracy across all evaluated models when tested on the rephrased prompts?",
          "answer": "The average drop in accuracy across all evaluated models when tested on the rephrased prompts was 2.15%."
        }
      ],
      "judge": {
        "evaluations": [
          {
            "qa": {
              "question": "According to the study's findings, which specific LLM family demonstrated the most consistent robustness by maintaining relatively stable accuracy on perturbed prompts?",
              "answer": "The Llama family demonstrated the most consistent robustness by maintaining relatively stable accuracy on perturbed prompts."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "What trend was observed regarding performance degradation as model size increased within the Gemma family specifically?",
              "answer": "Not enough information in summary"
            },
            "result": false,
            "issue": "The summary states that larger models tend to exhibit greater performance degradation, and specifically mentions the Gemma family showing a progressive increase in degradation with higher parameter counts. This information is present in the article but missing from the summary."
          },
          {
            "qa": {
              "question": "What paradoxical relationship did the study uncover between a model's baseline accuracy on the original MMLU benchmark and its robustness to prompt rephrasing?",
              "answer": "The study uncovered a paradoxical relationship where models with higher baseline accuracy on the MMLU benchmark tended to be more sensitive to prompt rephrasing, indicating an overreliance on fixed prompt patterns, while models with lower baseline accuracy exhibited more robustness, indicating less dependency on superficial cues."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "The paper contrasts the Llama family's stability with other models. What does this imply about how architectural choices or training strategies might influence a model's reliance on superficial cues versus genuine understanding?",
              "answer": "The Llama family's stability, in contrast to other models, implies that their architectural choices or training strategies might lead to a reduced dependency on superficial cues and a greater reliance on genuine understanding."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "Besides the Llama family, some other models like Falcon 7B showed statistically insignificant differences. What reason does the paper provide for their apparent robustness?",
              "answer": "Not enough information in summary"
            },
            "result": false,
            "issue": "The summary omits the reason provided in the article for the robustness of models like Falcon 7B, DeepSeek 7B, Qwen 2.5 3B, and Jetmoe 8B, which is attributed to their lower baseline accuracy."
          },
          {
            "qa": {
              "question": "ACU. Which LLM family is noted in the abstract for showing insignificant performance degradation?",
              "answer": "The Llama family is noted for showing insignificant performance degradation."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "ACU. How many of the 26 evaluated models exhibited statistically significant performance differences under perturbation?",
              "answer": "20 models showed statistically significant differences under perturbation."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "ACU. What was the average drop in accuracy across all evaluated models when tested on the rephrased prompts?",
              "answer": "The average drop in accuracy across all evaluated models when tested on the rephrased prompts was 2.15%."
            },
            "result": true,
            "issue": ""
          }
        ],
        "judgment": false,
        "sections_to_highlight": [
          "Gemma family — performance degradation vs. size",
          "Falcon 7B, DeepSeek 7B, Qwen 2.5 3B, Jetmoe 8B — reason for robustness",
          "Model Size — impact on performance degradation",
          "Model Accuracy — relationship with overfitting",
          "C-BOD — dataset-agnostic nature and integration",
          "Overfitting Causes — surface cues and memorization",
          "LLM Evaluation — beyond leaderboard scores and prompt resilience"
        ]
      },
      "correct_count_all": 6,
      "correct_count_acu": 3,
      "num_of_questions": 5,
      "sections_to_highlight": [
        "Gemma family — performance degradation vs. size",
        "Falcon 7B, DeepSeek 7B, Qwen 2.5 3B, Jetmoe 8B — reason for robustness",
        "Model Size — impact on performance degradation",
        "Model Accuracy — relationship with overfitting",
        "C-BOD — dataset-agnostic nature and integration",
        "Overfitting Causes — surface cues and memorization",
        "LLM Evaluation — beyond leaderboard scores and prompt resilience"
      ],
      "sections_to_highlight_size": 7,
      "qa_failures_present": true
    },
    {
      "iteration_number": 9,
      "summary": "1. SUMMARY:\nThis research introduces the Chameleon Benchmark Overfit Detector (C-BOD), a meta-evaluation framework designed to identify LLM overreliance on benchmark-specific surface cues. C-BOD systematically distorts benchmark prompts while preserving semantic meaning, enabling the detection of overfitting through performance degradation. Evaluations on the MMLU benchmark with 26 LLMs revealed an average accuracy drop of 2.15% under modest rephrasing, with most models showing statistically significant declines. Notably, larger LLMs and those with higher baseline accuracy exhibited greater performance degradation, suggesting a dependency on fixed prompt patterns. In contrast, the Llama family and models with lower baseline accuracy demonstrated less sensitivity to rephrasing, indicating reduced reliance on superficial cues. This highlights a critical paradox where high leaderboard scores may mask a lack of true language understanding and generalization. The dataset-agnostic nature of C-BOD allows for broad applicability and integration into training pipelines to foster more robust LLMs.\n\n2. KEY HIGHLIGHTS:\n*   LLMs often overfit to benchmark-specific surface cues, leading to performance drops (average 2.15%) when prompts are rephrased while preserving semantic meaning.\n*   Larger LLMs and those with higher baseline accuracy tend to be more susceptible to performance degradation due to rephrasing, indicating a reliance on memorized patterns.\n*   The Llama family and models with lower baseline accuracy showed greater robustness to prompt perturbations, suggesting a reduced dependency on superficial cues.\n*   C-BOD is a dataset-agnostic framework that quantifies overfitting by comparing performance on original and rephrased prompts, revealing a gap between leaderboard scores and true generalization.",
      "qa_evaluations": [
        {
          "qa": {
            "question": "According to the study's findings, which specific LLM family demonstrated the most consistent robustness by maintaining relatively stable accuracy on perturbed prompts?",
            "answer": "The Llama family demonstrated the most consistent robustness by maintaining relatively stable accuracy on perturbed prompts."
          },
          "result": true,
          "issue": null
        },
        {
          "qa": {
            "question": "What trend was observed regarding performance degradation as model size increased within the Gemma family specifically?",
            "answer": "Larger LLMs and those with higher baseline accuracy exhibited greater performance degradation, suggesting a dependency on fixed prompt patterns."
          },
          "result": false,
          "issue": "The summary states that larger LLMs in general showed this trend, but does not specifically mention the Gemma family."
        },
        {
          "qa": {
            "question": "What paradoxical relationship did the study uncover between a model's baseline accuracy on the original MMLU benchmark and its robustness to prompt rephrasing?",
            "answer": "The study uncovered a paradox where models with higher baseline accuracy on the MMLU benchmark exhibited greater performance degradation when prompts were rephrased, indicating a dependency on fixed prompt patterns. Conversely, models with lower baseline accuracy demonstrated less sensitivity to rephrasing, suggesting a reduced reliance on superficial cues."
          },
          "result": true,
          "issue": null
        },
        {
          "qa": {
            "question": "The paper contrasts the Llama family's stability with other models. What does this imply about how architectural choices or training strategies might influence a model's reliance on superficial cues versus genuine understanding?",
            "answer": "The Llama family's stability implies that architectural choices or training strategies might influence a model's reliance on superficial cues versus genuine understanding, as they demonstrated less sensitivity to rephrasing compared to other models."
          },
          "result": true,
          "issue": null
        },
        {
          "qa": {
            "question": "Besides the Llama family, some other models like Falcon 7B showed statistically insignificant differences. What reason does the paper provide for their apparent robustness?",
            "answer": "Not enough information in summary"
          },
          "result": false,
          "issue": "Not enough information"
        },
        {
          "qa": {
            "question": "ACU. Which LLM family is noted in the abstract for showing insignificant performance degradation?",
            "answer": "The Llama family is noted for showing insignificant performance degradation."
          },
          "result": true,
          "issue": null
        },
        {
          "qa": {
            "question": "ACU. How many of the 26 evaluated models exhibited statistically significant performance differences under perturbation?",
            "answer": "Most models showed statistically significant declines."
          },
          "result": false,
          "issue": "The summary states 'most models' but does not provide a specific number out of the 26 evaluated models."
        },
        {
          "qa": {
            "question": "ACU. What was the average drop in accuracy across all evaluated models when tested on the rephrased prompts?",
            "answer": "The average accuracy drop across all evaluated models when tested on the rephrased prompts was 2.15%."
          },
          "result": true,
          "issue": null
        }
      ],
      "qa_pairs": [
        {
          "question": "According to the study's findings, which specific LLM family demonstrated the most consistent robustness by maintaining relatively stable accuracy on perturbed prompts?",
          "answer": "The Llama family demonstrated the most consistent robustness by maintaining relatively stable accuracy on perturbed prompts."
        },
        {
          "question": "What trend was observed regarding performance degradation as model size increased within the Gemma family specifically?",
          "answer": "Larger LLMs and those with higher baseline accuracy exhibited greater performance degradation, suggesting a dependency on fixed prompt patterns."
        },
        {
          "question": "What paradoxical relationship did the study uncover between a model's baseline accuracy on the original MMLU benchmark and its robustness to prompt rephrasing?",
          "answer": "The study uncovered a paradox where models with higher baseline accuracy on the MMLU benchmark exhibited greater performance degradation when prompts were rephrased, indicating a dependency on fixed prompt patterns. Conversely, models with lower baseline accuracy demonstrated less sensitivity to rephrasing, suggesting a reduced reliance on superficial cues."
        },
        {
          "question": "The paper contrasts the Llama family's stability with other models. What does this imply about how architectural choices or training strategies might influence a model's reliance on superficial cues versus genuine understanding?",
          "answer": "The Llama family's stability implies that architectural choices or training strategies might influence a model's reliance on superficial cues versus genuine understanding, as they demonstrated less sensitivity to rephrasing compared to other models."
        },
        {
          "question": "Besides the Llama family, some other models like Falcon 7B showed statistically insignificant differences. What reason does the paper provide for their apparent robustness?",
          "answer": "Not enough information in summary"
        },
        {
          "question": "ACU. Which LLM family is noted in the abstract for showing insignificant performance degradation?",
          "answer": "The Llama family is noted for showing insignificant performance degradation."
        },
        {
          "question": "ACU. How many of the 26 evaluated models exhibited statistically significant performance differences under perturbation?",
          "answer": "Most models showed statistically significant declines."
        },
        {
          "question": "ACU. What was the average drop in accuracy across all evaluated models when tested on the rephrased prompts?",
          "answer": "The average accuracy drop across all evaluated models when tested on the rephrased prompts was 2.15%."
        }
      ],
      "judge": {
        "evaluations": [
          {
            "qa": {
              "question": "According to the study's findings, which specific LLM family demonstrated the most consistent robustness by maintaining relatively stable accuracy on perturbed prompts?",
              "answer": "The Llama family demonstrated the most consistent robustness by maintaining relatively stable accuracy on perturbed prompts."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "What trend was observed regarding performance degradation as model size increased within the Gemma family specifically?",
              "answer": "Larger LLMs and those with higher baseline accuracy exhibited greater performance degradation, suggesting a dependency on fixed prompt patterns."
            },
            "result": false,
            "issue": "The answer incorrectly attributes the Gemma family trend to a general observation about larger LLMs and those with higher baseline accuracy. The article states that 'models in the Gemma family show a progressive increase in ∆1.0 with higher parameter counts', which is a specific observation for the Gemma family, not a general trend applied to them."
          },
          {
            "qa": {
              "question": "What paradoxical relationship did the study uncover between a model's baseline accuracy on the original MMLU benchmark and its robustness to prompt rephrasing?",
              "answer": "The study uncovered a paradox where models with higher baseline accuracy on the MMLU benchmark exhibited greater performance degradation when prompts were rephrased, indicating a dependency on fixed prompt patterns. Conversely, models with lower baseline accuracy demonstrated less sensitivity to rephrasing, suggesting a reduced reliance on superficial cues."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "The paper contrasts the Llama family's stability with other models. What does this imply about how architectural choices or training strategies might influence a model's reliance on superficial cues versus genuine understanding?",
              "answer": "The Llama family's stability implies that architectural choices or training strategies might influence a model's reliance on superficial cues versus genuine understanding, as they demonstrated less sensitivity to rephrasing compared to other models."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "Besides the Llama family, some other models like Falcon 7B showed statistically insignificant differences. What reason does the paper provide for their apparent robustness?",
              "answer": "Not enough information in summary"
            },
            "result": false,
            "issue": "The summary states 'Not enough information in summary' for a question that can be answered by the article. The article states: 'We also observed that Falcon 7B, DeepSeek 7B, Qwen 2.5 3B and Jetmoe 8B show statistically insignificant differences, likely due to their lower baseline accuracy.'"
          },
          {
            "qa": {
              "question": "ACU. Which LLM family is noted in the abstract for showing insignificant performance degradation?",
              "answer": "The Llama family is noted for showing insignificant performance degradation."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "ACU. How many of the 26 evaluated models exhibited statistically significant performance differences under perturbation?",
              "answer": "Most models showed statistically significant declines."
            },
            "result": false,
            "issue": "The answer is not specific. The article states: 'with 20 out of 26 models exhibiting statistically significant differences.'"
          },
          {
            "qa": {
              "question": "ACU. What was the average drop in accuracy across all evaluated models when tested on the rephrased prompts?",
              "answer": "The average accuracy drop across all evaluated models when tested on the rephrased prompts was 2.15%."
            },
            "result": true,
            "issue": ""
          }
        ],
        "judgment": false,
        "sections_to_highlight": [
          "Results — performance difference by model size",
          "Results — performance difference by baseline accuracy",
          "Methodology — C-BOD framework explanation",
          "Introduction — LLM overfitting problem",
          "Discussion — reasons for LLM overfitting",
          "Results — specific models with insignificant differences"
        ]
      },
      "correct_count_all": 5,
      "correct_count_acu": 2,
      "num_of_questions": 5,
      "sections_to_highlight": [
        "Results — performance difference by model size",
        "Results — performance difference by baseline accuracy",
        "Methodology — C-BOD framework explanation",
        "Introduction — LLM overfitting problem",
        "Discussion — reasons for LLM overfitting",
        "Results — specific models with insignificant differences"
      ],
      "sections_to_highlight_size": 6,
      "qa_failures_present": true
    },
    {
      "iteration_number": 10,
      "summary": "1. SUMMARY:\nThis paper introduces the Chameleon Benchmark Overfit Detector (C-BOD), a meta-evaluation framework designed to identify LLM overreliance on surface-level cues in benchmark datasets. C-BOD systematically distorts benchmark prompts while preserving semantic content, revealing performance degradation when LLMs depend on specific phrasing. Evaluations on the MMLU benchmark using 26 LLMs showed an average accuracy drop of 2.15% under modest rephrasing, with 20 models exhibiting statistically significant declines. Notably, larger LLMs and those with higher baseline accuracy tended to be more sensitive to these perturbations, suggesting an overreliance on fixed patterns. Conversely, the Llama family and models with lower baseline accuracy demonstrated reduced degradation, indicating greater robustness. C-BOD's dataset-agnostic nature allows for broad applicability and integration into training pipelines to foster more generalizable language understanding, challenging the community to look beyond leaderboard scores.\n\n2. KEY HIGHLIGHTS:\n*   C-BOD systematically distorts benchmark prompts to detect LLM overreliance on surface cues, showing an average 2.15% accuracy drop across 26 models on MMLU.\n*   Larger LLMs and those with higher baseline accuracy are more susceptible to performance degradation under prompt rephrasing, indicating a potential overfit to specific patterns.\n*   The Llama family of models and those with lower baseline accuracy exhibited greater robustness, suggesting reduced dependency on superficial cues.\n*   C-BOD's dataset-agnostic design allows for broad integration into evaluation and training processes to promote more resilient LLM development.",
      "qa_evaluations": [
        {
          "qa": {
            "question": "According to the study's findings, which specific LLM family demonstrated the most consistent robustness by maintaining relatively stable accuracy on perturbed prompts?",
            "answer": "The Llama family of models demonstrated the most consistent robustness by maintaining relatively stable accuracy on perturbed prompts."
          },
          "result": true,
          "issue": null
        },
        {
          "qa": {
            "question": "What trend was observed regarding performance degradation as model size increased within the Gemma family specifically?",
            "answer": "Not enough information in summary"
          },
          "result": false,
          "issue": "Not enough information"
        },
        {
          "qa": {
            "question": "What paradoxical relationship did the study uncover between a model's baseline accuracy on the original MMLU benchmark and its robustness to prompt rephrasing?",
            "answer": "The study uncovered that models with higher baseline accuracy on the original MMLU benchmark tended to be more sensitive to prompt rephrasing perturbations, indicating an overreliance on fixed patterns, while models with lower baseline accuracy exhibited greater robustness."
          },
          "result": true,
          "issue": null
        },
        {
          "qa": {
            "question": "The paper contrasts the Llama family's stability with other models. What does this imply about how architectural choices or training strategies might influence a model's reliance on superficial cues versus genuine understanding?",
            "answer": "The Llama family's stability suggests that their architectural choices or training strategies may lead to a reduced reliance on superficial cues and a greater capacity for genuine understanding compared to other models."
          },
          "result": true,
          "issue": null
        },
        {
          "qa": {
            "question": "Besides the Llama family, some other models like Falcon 7B showed statistically insignificant differences. What reason does the paper provide for their apparent robustness?",
            "answer": "Not enough information in summary"
          },
          "result": false,
          "issue": "Not enough information"
        },
        {
          "qa": {
            "question": "ACU. Which LLM family is noted in the abstract for showing insignificant performance degradation?",
            "answer": "The Llama family is noted for showing insignificant performance degradation."
          },
          "result": true,
          "issue": null
        },
        {
          "qa": {
            "question": "ACU. How many of the 26 evaluated models exhibited statistically significant performance differences under perturbation?",
            "answer": "20 models exhibited statistically significant performance differences under perturbation."
          },
          "result": true,
          "issue": null
        },
        {
          "qa": {
            "question": "ACU. What was the average drop in accuracy across all evaluated models when tested on the rephrased prompts?",
            "answer": "The average drop in accuracy across all evaluated models when tested on the rephrased prompts was 2.15%."
          },
          "result": true,
          "issue": null
        }
      ],
      "qa_pairs": [
        {
          "question": "According to the study's findings, which specific LLM family demonstrated the most consistent robustness by maintaining relatively stable accuracy on perturbed prompts?",
          "answer": "The Llama family of models demonstrated the most consistent robustness by maintaining relatively stable accuracy on perturbed prompts."
        },
        {
          "question": "What trend was observed regarding performance degradation as model size increased within the Gemma family specifically?",
          "answer": "Not enough information in summary"
        },
        {
          "question": "What paradoxical relationship did the study uncover between a model's baseline accuracy on the original MMLU benchmark and its robustness to prompt rephrasing?",
          "answer": "The study uncovered that models with higher baseline accuracy on the original MMLU benchmark tended to be more sensitive to prompt rephrasing perturbations, indicating an overreliance on fixed patterns, while models with lower baseline accuracy exhibited greater robustness."
        },
        {
          "question": "The paper contrasts the Llama family's stability with other models. What does this imply about how architectural choices or training strategies might influence a model's reliance on superficial cues versus genuine understanding?",
          "answer": "The Llama family's stability suggests that their architectural choices or training strategies may lead to a reduced reliance on superficial cues and a greater capacity for genuine understanding compared to other models."
        },
        {
          "question": "Besides the Llama family, some other models like Falcon 7B showed statistically insignificant differences. What reason does the paper provide for their apparent robustness?",
          "answer": "Not enough information in summary"
        },
        {
          "question": "ACU. Which LLM family is noted in the abstract for showing insignificant performance degradation?",
          "answer": "The Llama family is noted for showing insignificant performance degradation."
        },
        {
          "question": "ACU. How many of the 26 evaluated models exhibited statistically significant performance differences under perturbation?",
          "answer": "20 models exhibited statistically significant performance differences under perturbation."
        },
        {
          "question": "ACU. What was the average drop in accuracy across all evaluated models when tested on the rephrased prompts?",
          "answer": "The average drop in accuracy across all evaluated models when tested on the rephrased prompts was 2.15%."
        }
      ],
      "judge": {
        "evaluations": [
          {
            "qa": {
              "question": "According to the study's findings, which specific LLM family demonstrated the most consistent robustness by maintaining relatively stable accuracy on perturbed prompts?",
              "answer": "The Llama family of models demonstrated the most consistent robustness by maintaining relatively stable accuracy on perturbed prompts."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "What trend was observed regarding performance degradation as model size increased within the Gemma family specifically?",
              "answer": "Not enough information in summary"
            },
            "result": false,
            "issue": "The summary should have included information about the Gemma family's performance degradation trend with increasing model size, as this was present in the article."
          },
          {
            "qa": {
              "question": "What paradoxical relationship did the study uncover between a model's baseline accuracy on the original MMLU benchmark and its robustness to prompt rephrasing?",
              "answer": "The study uncovered that models with higher baseline accuracy on the original MMLU benchmark tended to be more sensitive to prompt rephrasing perturbations, indicating an overreliance on fixed patterns, while models with lower baseline accuracy exhibited greater robustness."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "The paper contrasts the Llama family's stability with other models. What does this imply about how architectural choices or training strategies might influence a model's reliance on superficial cues versus genuine understanding?",
              "answer": "The Llama family's stability suggests that their architectural choices or training strategies may lead to a reduced reliance on superficial cues and a greater capacity for genuine understanding compared to other models."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "Besides the Llama family, some other models like Falcon 7B showed statistically insignificant differences. What reason does the paper provide for their apparent robustness?",
              "answer": "Not enough information in summary"
            },
            "result": false,
            "issue": "The summary should have included the reason provided in the article for the robustness of models like Falcon 7B, which is their lower baseline accuracy."
          },
          {
            "qa": {
              "question": "ACU. Which LLM family is noted in the abstract for showing insignificant performance degradation?",
              "answer": "The Llama family is noted for showing insignificant performance degradation."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "ACU. How many of the 26 evaluated models exhibited statistically significant performance differences under perturbation?",
              "answer": "20 models exhibited statistically significant performance differences under perturbation."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "ACU. What was the average drop in accuracy across all evaluated models when tested on the rephrased prompts?",
              "answer": "The average drop in accuracy across all evaluated models when tested on the rephrased prompts was 2.15%."
            },
            "result": true,
            "issue": ""
          }
        ],
        "judgment": false,
        "sections_to_highlight": [
          "Results — Performance degradation vs. model size for Gemma family",
          "Results — Reason for robustness in models with lower baseline accuracy",
          "Method — C-BOD's dataset-agnostic nature",
          "Results — Correlation between baseline accuracy and performance difference",
          "Introduction — Problem statement on LLM evaluation",
          "Method — C-BOD's core mechanism",
          "Results — Comparison of Llama family stability with other models"
        ]
      },
      "correct_count_all": 6,
      "correct_count_acu": 3,
      "num_of_questions": 5,
      "sections_to_highlight": [
        "Results — Performance degradation vs. model size for Gemma family",
        "Results — Reason for robustness in models with lower baseline accuracy",
        "Method — C-BOD's dataset-agnostic nature",
        "Results — Correlation between baseline accuracy and performance difference",
        "Introduction — Problem statement on LLM evaluation",
        "Method — C-BOD's core mechanism",
        "Results — Comparison of Llama family stability with other models"
      ],
      "sections_to_highlight_size": 7,
      "qa_failures_present": true
    },
    {
      "iteration_number": 11,
      "summary": "1. SUMMARY:\nThis paper introduces the Chameleon Benchmark Overfit Detector (C-BOD), a meta-evaluation framework designed to assess LLM robustness by systematically distorting benchmark prompts while preserving semantic content. The study, conducted on the MMLU benchmark with 26 LLMs, reveals that many models over-rely on dataset-specific surface cues, leading to an average performance degradation of 2.15% under modest rephrasing. Notably, larger models and those with higher baseline accuracy tend to be more sensitive to these perturbations, indicating a reliance on fixed prompt patterns. In contrast, the Llama family and models with lower baseline accuracy demonstrated reduced dependency on superficial cues, showing insignificant performance degradation. C-BOD's dataset-agnostic nature allows its integration into training pipelines to foster more generalizable language understanding, urging a shift in evaluation beyond leaderboard scores to prioritize resilience.\n\n2. KEY HIGHLIGHTS:\n*   LLMs often exhibit a significant performance drop (average 2.15%) when prompts are rephrased, indicating over-reliance on surface-level cues rather than true language understanding.\n*   Larger LLMs and those with higher initial accuracy are more prone to performance degradation under prompt perturbations, suggesting a correlation between scale/performance and sensitivity to phrasing.\n*   The Llama family of models, along with those exhibiting lower baseline accuracy (e.g., Falcon 7B, DeepSeek 7B, Qwen 2.5 3B, Jetmoe 8B), demonstrated greater robustness and insignificant performance degradation, implying reduced dependency on superficial patterns.\n*   C-BOD is a dataset-agnostic framework that quantifies LLM overfitting by comparing performance on original versus semantically preserved, rephrased prompts, advocating for evaluation methods that prioritize prompt resilience.",
      "qa_evaluations": [
        {
          "qa": {
            "question": "According to the study's findings, which specific LLM family demonstrated the most consistent robustness by maintaining relatively stable accuracy on perturbed prompts?",
            "answer": "The Llama family of models demonstrated the most consistent robustness by maintaining relatively stable accuracy on perturbed prompts."
          },
          "result": true,
          "issue": null
        },
        {
          "qa": {
            "question": "What trend was observed regarding performance degradation as model size increased within the Gemma family specifically?",
            "answer": "Larger models and those with higher baseline accuracy tend to be more sensitive to these perturbations, indicating a reliance on fixed prompt patterns."
          },
          "result": false,
          "issue": "The summary states a general trend for larger models but does not specifically mention the Gemma family."
        },
        {
          "qa": {
            "question": "What paradoxical relationship did the study uncover between a model's baseline accuracy on the original MMLU benchmark and its robustness to prompt rephrasing?",
            "answer": "The study uncovered that larger models and those with higher baseline accuracy tend to be more sensitive to prompt rephrasing perturbations, indicating a reliance on fixed prompt patterns. In contrast, models with lower baseline accuracy demonstrated greater robustness and reduced dependency on superficial cues."
          },
          "result": true,
          "issue": null
        },
        {
          "qa": {
            "question": "The paper contrasts the Llama family's stability with other models. What does this imply about how architectural choices or training strategies might influence a model's reliance on superficial cues versus genuine understanding?",
            "answer": "The Llama family's stability, in contrast to other models, implies that their architectural choices or training strategies might influence a reduced dependency on superficial cues and a greater reliance on genuine understanding."
          },
          "result": true,
          "issue": null
        },
        {
          "qa": {
            "question": "Besides the Llama family, some other models like Falcon 7B showed statistically insignificant differences. What reason does the paper provide for their apparent robustness?",
            "answer": "The paper implies that models like Falcon 7B, along with others exhibiting lower baseline accuracy, demonstrated greater robustness and insignificant performance degradation due to a reduced dependency on superficial patterns."
          },
          "result": true,
          "issue": null
        },
        {
          "qa": {
            "question": "ACU. Which LLM family is noted in the abstract for showing insignificant performance degradation?",
            "answer": "The Llama family of models is noted for showing insignificant performance degradation."
          },
          "result": true,
          "issue": null
        },
        {
          "qa": {
            "question": "ACU. How many of the 26 evaluated models exhibited statistically significant performance differences under perturbation?",
            "answer": "Not enough information in summary"
          },
          "result": false,
          "issue": "Not enough information"
        },
        {
          "qa": {
            "question": "ACU. What was the average drop in accuracy across all evaluated models when tested on the rephrased prompts?",
            "answer": "The average performance degradation across all evaluated models was 2.15% when tested on rephrased prompts."
          },
          "result": true,
          "issue": null
        }
      ],
      "qa_pairs": [
        {
          "question": "According to the study's findings, which specific LLM family demonstrated the most consistent robustness by maintaining relatively stable accuracy on perturbed prompts?",
          "answer": "The Llama family of models demonstrated the most consistent robustness by maintaining relatively stable accuracy on perturbed prompts."
        },
        {
          "question": "What trend was observed regarding performance degradation as model size increased within the Gemma family specifically?",
          "answer": "Larger models and those with higher baseline accuracy tend to be more sensitive to these perturbations, indicating a reliance on fixed prompt patterns."
        },
        {
          "question": "What paradoxical relationship did the study uncover between a model's baseline accuracy on the original MMLU benchmark and its robustness to prompt rephrasing?",
          "answer": "The study uncovered that larger models and those with higher baseline accuracy tend to be more sensitive to prompt rephrasing perturbations, indicating a reliance on fixed prompt patterns. In contrast, models with lower baseline accuracy demonstrated greater robustness and reduced dependency on superficial cues."
        },
        {
          "question": "The paper contrasts the Llama family's stability with other models. What does this imply about how architectural choices or training strategies might influence a model's reliance on superficial cues versus genuine understanding?",
          "answer": "The Llama family's stability, in contrast to other models, implies that their architectural choices or training strategies might influence a reduced dependency on superficial cues and a greater reliance on genuine understanding."
        },
        {
          "question": "Besides the Llama family, some other models like Falcon 7B showed statistically insignificant differences. What reason does the paper provide for their apparent robustness?",
          "answer": "The paper implies that models like Falcon 7B, along with others exhibiting lower baseline accuracy, demonstrated greater robustness and insignificant performance degradation due to a reduced dependency on superficial patterns."
        },
        {
          "question": "ACU. Which LLM family is noted in the abstract for showing insignificant performance degradation?",
          "answer": "The Llama family of models is noted for showing insignificant performance degradation."
        },
        {
          "question": "ACU. How many of the 26 evaluated models exhibited statistically significant performance differences under perturbation?",
          "answer": "Not enough information in summary"
        },
        {
          "question": "ACU. What was the average drop in accuracy across all evaluated models when tested on the rephrased prompts?",
          "answer": "The average performance degradation across all evaluated models was 2.15% when tested on rephrased prompts."
        }
      ],
      "judge": {
        "evaluations": [
          {
            "qa": {
              "question": "According to the study's findings, which specific LLM family demonstrated the most consistent robustness by maintaining relatively stable accuracy on perturbed prompts?",
              "answer": "The Llama family of models demonstrated the most consistent robustness by maintaining relatively stable accuracy on perturbed prompts."
            },
            "result": true,
            "issue": null
          },
          {
            "qa": {
              "question": "What trend was observed regarding performance degradation as model size increased within the Gemma family specifically?",
              "answer": "Larger models and those with higher baseline accuracy tend to be more sensitive to these perturbations, indicating a reliance on fixed prompt patterns."
            },
            "result": false,
            "issue": "The answer is too general and does not specifically address the Gemma family as requested by the question. The article states that 'models in the Gemma family show a progressive increase in \\u22061.0 with higher parameter counts', which is more specific."
          },
          {
            "qa": {
              "question": "What paradoxical relationship did the study uncover between a model's baseline accuracy on the original MMLU benchmark and its robustness to prompt rephrasing?",
              "answer": "The study uncovered that larger models and those with higher baseline accuracy tend to be more sensitive to prompt rephrasing perturbations, indicating a reliance on fixed prompt patterns. In contrast, models with lower baseline accuracy demonstrated greater robustness and reduced dependency on superficial cues."
            },
            "result": true,
            "issue": null
          },
          {
            "qa": {
              "question": "The paper contrasts the Llama family's stability with other models. What does this imply about how architectural choices or training strategies might influence a model's reliance on superficial cues versus genuine understanding?",
              "answer": "The Llama family's stability, in contrast to other models, implies that their architectural choices or training strategies might influence a reduced dependency on superficial cues and a greater reliance on genuine understanding."
            },
            "result": true,
            "issue": null
          },
          {
            "qa": {
              "question": "Besides the Llama family, some other models like Falcon 7B showed statistically insignificant differences. What reason does the paper provide for their apparent robustness?",
              "answer": "The paper implies that models like Falcon 7B, along with others exhibiting lower baseline accuracy, demonstrated greater robustness and insignificant performance degradation due to a reduced dependency on superficial patterns."
            },
            "result": true,
            "issue": null
          },
          {
            "qa": {
              "question": "ACU. Which LLM family is noted in the abstract for showing insignificant performance degradation?",
              "answer": "The Llama family of models is noted for showing insignificant performance degradation."
            },
            "result": true,
            "issue": null
          },
          {
            "qa": {
              "question": "ACU. How many of the 26 evaluated models exhibited statistically significant performance differences under perturbation?",
              "answer": "Not enough information in summary"
            },
            "result": false,
            "issue": "The summary states '20 out of 26 models exhibiting statistically significant differences', which directly answers the question. The article also confirms this in the abstract and results section."
          },
          {
            "qa": {
              "question": "ACU. What was the average drop in accuracy across all evaluated models when tested on the rephrased prompts?",
              "answer": "The average performance degradation across all evaluated models was 2.15% when tested on rephrased prompts."
            },
            "result": true,
            "issue": null
          }
        ],
        "judgment": false,
        "sections_to_highlight": [
          "Methodology — C-BOD framework overview",
          "Results — performance difference vs. model size",
          "Results — performance difference vs. baseline accuracy",
          "Discussion — LLM overfitting causes",
          "Limitations — scope of C-BOD detection",
          "Limitations — computational cost of C-BOD integration"
        ]
      },
      "correct_count_all": 6,
      "correct_count_acu": 2,
      "num_of_questions": 5,
      "sections_to_highlight": [
        "Methodology — C-BOD framework overview",
        "Results — performance difference vs. model size",
        "Results — performance difference vs. baseline accuracy",
        "Discussion — LLM overfitting causes",
        "Limitations — scope of C-BOD detection",
        "Limitations — computational cost of C-BOD integration"
      ],
      "sections_to_highlight_size": 6,
      "qa_failures_present": true
    },
    {
      "iteration_number": 12,
      "summary": "1. SUMMARY:\nThis paper introduces the Chameleon Benchmark Overfit Detector (C-BOD), a meta-evaluation framework designed to assess LLM robustness by detecting overfitting to benchmark-specific surface cues. C-BOD systematically distorts benchmark prompts while preserving their semantic content, thereby exposing LLMs that rely on memorized patterns rather than true language understanding. The framework is dataset-agnostic and can be integrated into training pipelines to foster more generalizable models. Evaluations on the MMLU benchmark using 26 LLMs revealed an average performance degradation of 2.15% under modest rephrasing. Notably, larger LLMs and those with higher baseline accuracy exhibited greater performance drops, suggesting an overreliance on fixed prompt patterns. Conversely, the Llama family and models with lower baseline accuracy demonstrated reduced dependency on superficial cues, showing insignificant degradation. These findings challenge the reliance on leaderboard scores, emphasizing the need for resilience and generalization in LLM evaluation.\n\n2. KEY HIGHLIGHTS:\n*   C-BOD, a dataset-agnostic framework, systematically rephrases prompts to detect LLM overfitting to surface cues, revealing an average 2.15% performance drop across 26 LLMs on MMLU.\n*   Larger LLMs and those with higher baseline accuracy tend to be more sensitive to prompt rephrasing, indicating a greater reliance on specific phrasing.\n*   The Llama family and models with lower baseline accuracy showed more robustness, with insignificant performance degradation under rephrasing, suggesting reduced dependency on superficial cues.\n*   The findings highlight a paradoxical relationship where high benchmark scores may mask underlying vulnerabilities to prompt variations, urging a shift in evaluation focus towards resilience and generalization.",
      "qa_evaluations": [
        {
          "qa": {
            "question": "According to the study's findings, which specific LLM family demonstrated the most consistent robustness by maintaining relatively stable accuracy on perturbed prompts?",
            "answer": "The Llama family demonstrated the most consistent robustness by maintaining relatively stable accuracy on perturbed prompts."
          },
          "result": true,
          "issue": null
        },
        {
          "qa": {
            "question": "What trend was observed regarding performance degradation as model size increased within the Gemma family specifically?",
            "answer": "Not enough information in summary"
          },
          "result": false,
          "issue": "Not enough information"
        },
        {
          "qa": {
            "question": "What paradoxical relationship did the study uncover between a model's baseline accuracy on the original MMLU benchmark and its robustness to prompt rephrasing?",
            "answer": "The study uncovered a paradoxical relationship where models with higher baseline accuracy on the MMLU benchmark exhibited greater performance drops when subjected to prompt rephrasing, suggesting an overreliance on fixed prompt patterns. Conversely, models with lower baseline accuracy demonstrated reduced dependency on superficial cues and showed more robustness."
          },
          "result": true,
          "issue": null
        },
        {
          "qa": {
            "question": "The paper contrasts the Llama family's stability with other models. What does this imply about how architectural choices or training strategies might influence a model's reliance on superficial cues versus genuine understanding?",
            "answer": "The Llama family's stability implies that architectural choices or training strategies that reduce dependency on superficial cues can lead to more genuine understanding and better robustness to prompt variations."
          },
          "result": true,
          "issue": null
        },
        {
          "qa": {
            "question": "Besides the Llama family, some other models like Falcon 7B showed statistically insignificant differences. What reason does the paper provide for their apparent robustness?",
            "answer": "Not enough information in summary"
          },
          "result": false,
          "issue": "Not enough information"
        },
        {
          "qa": {
            "question": "ACU. Which LLM family is noted in the abstract for showing insignificant performance degradation?",
            "answer": "The Llama family is noted for showing insignificant performance degradation."
          },
          "result": true,
          "issue": null
        },
        {
          "qa": {
            "question": "ACU. How many of the 26 evaluated models exhibited statistically significant performance differences under perturbation?",
            "answer": "The summary states that evaluations on the MMLU benchmark using 26 LLMs revealed an average performance degradation of 2.15% under modest rephrasing, and that larger LLMs and those with higher baseline accuracy exhibited greater performance drops. However, it does not specify how many of the 26 models showed statistically significant performance differences."
          },
          "result": false,
          "issue": "Not enough information"
        },
        {
          "qa": {
            "question": "ACU. What was the average drop in accuracy across all evaluated models when tested on the rephrased prompts?",
            "answer": "The average performance degradation across all evaluated models when tested on rephrased prompts was 2.15%."
          },
          "result": true,
          "issue": null
        }
      ],
      "qa_pairs": [
        {
          "question": "According to the study's findings, which specific LLM family demonstrated the most consistent robustness by maintaining relatively stable accuracy on perturbed prompts?",
          "answer": "The Llama family demonstrated the most consistent robustness by maintaining relatively stable accuracy on perturbed prompts."
        },
        {
          "question": "What trend was observed regarding performance degradation as model size increased within the Gemma family specifically?",
          "answer": "Not enough information in summary"
        },
        {
          "question": "What paradoxical relationship did the study uncover between a model's baseline accuracy on the original MMLU benchmark and its robustness to prompt rephrasing?",
          "answer": "The study uncovered a paradoxical relationship where models with higher baseline accuracy on the MMLU benchmark exhibited greater performance drops when subjected to prompt rephrasing, suggesting an overreliance on fixed prompt patterns. Conversely, models with lower baseline accuracy demonstrated reduced dependency on superficial cues and showed more robustness."
        },
        {
          "question": "The paper contrasts the Llama family's stability with other models. What does this imply about how architectural choices or training strategies might influence a model's reliance on superficial cues versus genuine understanding?",
          "answer": "The Llama family's stability implies that architectural choices or training strategies that reduce dependency on superficial cues can lead to more genuine understanding and better robustness to prompt variations."
        },
        {
          "question": "Besides the Llama family, some other models like Falcon 7B showed statistically insignificant differences. What reason does the paper provide for their apparent robustness?",
          "answer": "Not enough information in summary"
        },
        {
          "question": "ACU. Which LLM family is noted in the abstract for showing insignificant performance degradation?",
          "answer": "The Llama family is noted for showing insignificant performance degradation."
        },
        {
          "question": "ACU. How many of the 26 evaluated models exhibited statistically significant performance differences under perturbation?",
          "answer": "The summary states that evaluations on the MMLU benchmark using 26 LLMs revealed an average performance degradation of 2.15% under modest rephrasing, and that larger LLMs and those with higher baseline accuracy exhibited greater performance drops. However, it does not specify how many of the 26 models showed statistically significant performance differences."
        },
        {
          "question": "ACU. What was the average drop in accuracy across all evaluated models when tested on the rephrased prompts?",
          "answer": "The average performance degradation across all evaluated models when tested on rephrased prompts was 2.15%."
        }
      ],
      "judge": {
        "evaluations": [
          {
            "qa": {
              "question": "According to the study's findings, which specific LLM family demonstrated the most consistent robustness by maintaining relatively stable accuracy on perturbed prompts?",
              "answer": "The Llama family demonstrated the most consistent robustness by maintaining relatively stable accuracy on perturbed prompts."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "What trend was observed regarding performance degradation as model size increased within the Gemma family specifically?",
              "answer": "Not enough information in summary"
            },
            "result": false,
            "issue": "The article mentions that \"models in the Gemma family show a progressive increase in \\u22061.0 with higher parameter counts\", but this specific detail is missing from the summary."
          },
          {
            "qa": {
              "question": "What paradoxical relationship did the study uncover between a model's baseline accuracy on the original MMLU benchmark and its robustness to prompt rephrasing?",
              "answer": "The study uncovered a paradoxical relationship where models with higher baseline accuracy on the MMLU benchmark exhibited greater performance drops when subjected to prompt rephrasing, suggesting an overreliance on fixed prompt patterns. Conversely, models with lower baseline accuracy demonstrated reduced dependency on superficial cues and showed more robustness."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "The paper contrasts the Llama family's stability with other models. What does this imply about how architectural choices or training strategies might influence a model's reliance on superficial cues versus genuine understanding?",
              "answer": "The Llama family's stability implies that architectural choices or training strategies that reduce dependency on superficial cues can lead to more genuine understanding and better robustness to prompt variations."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "Besides the Llama family, some other models like Falcon 7B showed statistically insignificant differences. What reason does the paper provide for their apparent robustness?",
              "answer": "Not enough information in summary"
            },
            "result": false,
            "issue": "The article states that Falcon 7B, DeepSeek 7B, Qwen 2.5 3B and Jetmoe 8B show statistically insignificant differences, \"likely due to their lower baseline accuracy\". This explanation is missing from the summary."
          },
          {
            "qa": {
              "question": "ACU. Which LLM family is noted in the abstract for showing insignificant performance degradation?",
              "answer": "The Llama family is noted for showing insignificant performance degradation."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "ACU. How many of the 26 evaluated models exhibited statistically significant performance differences under perturbation?",
              "answer": "The summary states that evaluations on the MMLU benchmark using 26 LLMs revealed an average performance degradation of 2.15% under modest rephrasing, and that larger LLMs and those with higher baseline accuracy exhibited greater performance drops. However, it does not specify how many of the 26 models showed statistically significant performance differences."
            },
            "result": false,
            "issue": "The article explicitly states: \"20 out of 26 models exhibiting statistically significant differences.\" This information is missing from the summary."
          },
          {
            "qa": {
              "question": "ACU. What was the average drop in accuracy across all evaluated models when tested on the rephrased prompts?",
              "answer": "The average performance degradation across all evaluated models when tested on rephrased prompts was 2.15%."
            },
            "result": true,
            "issue": ""
          }
        ],
        "judgment": false,
        "sections_to_highlight": [
          "Results — Gemma family performance vs. parameter count",
          "Results — Models with statistically insignificant differences",
          "Results — Number of models with significant performance differences",
          "Methodology — Dataset-agnostic nature",
          "Key Highlights — Paradoxical relationship in accuracy vs. robustness"
        ]
      },
      "correct_count_all": 5,
      "correct_count_acu": 2,
      "num_of_questions": 5,
      "sections_to_highlight": [
        "Results — Gemma family performance vs. parameter count",
        "Results — Models with statistically insignificant differences",
        "Results — Number of models with significant performance differences",
        "Methodology — Dataset-agnostic nature",
        "Key Highlights — Paradoxical relationship in accuracy vs. robustness"
      ],
      "sections_to_highlight_size": 5,
      "qa_failures_present": true
    },
    {
      "iteration_number": 13,
      "summary": "1. SUMMARY:\nThis research introduces the Chameleon Benchmark Overfit Detector (C-BOD), a meta-evaluation framework designed to assess the robustness of Large Language Models (LLMs) by detecting their overreliance on dataset-specific surface cues. C-BOD systematically distorts benchmark prompts while preserving semantic content, revealing whether a model's performance is due to genuine understanding or memorized patterns. Evaluated on the MMLU benchmark with 26 LLMs, C-BOD found that 20 models exhibited statistically significant performance degradation under modest rephrasing, averaging a 2.15% drop. Notably, larger LLMs and those with higher baseline accuracy showed greater sensitivity to these perturbations, indicating a potential overreliance on fixed prompt patterns. In contrast, the Llama family and models with lower baseline accuracy demonstrated reduced degradation, suggesting greater robustness. C-BOD's dataset-agnostic nature allows for broad application and integration into training pipelines to promote more generalized language understanding, challenging the community to move beyond leaderboard scores and prioritize resilience.\n\n2. KEY HIGHLIGHTS:\n*   C-BOD, a dataset-agnostic framework, systematically distorts prompts to detect LLM overfitting to surface cues, revealing that 20 out of 26 evaluated LLMs show statistically significant performance degradation.\n*   Larger LLMs and those with higher baseline accuracy tend to be more sensitive to prompt rephrasing, indicating a potential overreliance on fixed patterns, while the Llama family shows greater stability.\n*   The study highlights a paradoxical relationship where high benchmark accuracy can mask underlying vulnerabilities to prompt variations, suggesting leaderboard scores may not fully reflect true generalization.\n*   The findings advocate for evaluating LLMs beyond leaderboards, emphasizing prompt resilience and generalization as crucial metrics for robust language understanding.",
      "qa_evaluations": [
        {
          "qa": {
            "question": "According to the study's findings, which specific LLM family demonstrated the most consistent robustness by maintaining relatively stable accuracy on perturbed prompts?",
            "answer": "The Llama family demonstrated the most consistent robustness by maintaining relatively stable accuracy on perturbed prompts."
          },
          "result": true,
          "issue": null
        },
        {
          "qa": {
            "question": "What trend was observed regarding performance degradation as model size increased within the Gemma family specifically?",
            "answer": "Larger LLMs tended to be more sensitive to these perturbations, indicating a potential overreliance on fixed prompt patterns."
          },
          "result": false,
          "issue": "The summary mentions that larger LLMs in general showed this trend, but does not specifically mention the Gemma family."
        },
        {
          "qa": {
            "question": "What paradoxical relationship did the study uncover between a model's baseline accuracy on the original MMLU benchmark and its robustness to prompt rephrasing?",
            "answer": "The study uncovered a paradoxical relationship where models with higher baseline accuracy showed greater sensitivity to prompt rephrasing, indicating a potential overreliance on fixed prompt patterns."
          },
          "result": true,
          "issue": null
        },
        {
          "qa": {
            "question": "The paper contrasts the Llama family's stability with other models. What does this imply about how architectural choices or training strategies might influence a model's reliance on superficial cues versus genuine understanding?",
            "answer": "The Llama family's stability suggests that architectural choices or training strategies might influence a model's reliance on superficial cues versus genuine understanding, implying that these factors contribute to greater robustness."
          },
          "result": false,
          "issue": "The summary states that the Llama family showed greater stability and implies a connection to architectural choices or training strategies, but does not explicitly explain how these influence reliance on superficial cues versus genuine understanding."
        },
        {
          "qa": {
            "question": "Besides the Llama family, some other models like Falcon 7B showed statistically insignificant differences. What reason does the paper provide for their apparent robustness?",
            "answer": "Not enough information in summary"
          },
          "result": false,
          "issue": "Not enough information"
        },
        {
          "qa": {
            "question": "ACU. Which LLM family is noted in the abstract for showing insignificant performance degradation?",
            "answer": "The Llama family is noted for showing insignificant performance degradation."
          },
          "result": true,
          "issue": null
        },
        {
          "qa": {
            "question": "ACU. How many of the 26 evaluated models exhibited statistically significant performance differences under perturbation?",
            "answer": "20 out of 26 evaluated models exhibited statistically significant performance differences under perturbation."
          },
          "result": true,
          "issue": null
        },
        {
          "qa": {
            "question": "ACU. What was the average drop in accuracy across all evaluated models when tested on the rephrased prompts?",
            "answer": "The average drop in accuracy across all evaluated models when tested on the rephrased prompts was 2.15%."
          },
          "result": true,
          "issue": null
        }
      ],
      "qa_pairs": [
        {
          "question": "According to the study's findings, which specific LLM family demonstrated the most consistent robustness by maintaining relatively stable accuracy on perturbed prompts?",
          "answer": "The Llama family demonstrated the most consistent robustness by maintaining relatively stable accuracy on perturbed prompts."
        },
        {
          "question": "What trend was observed regarding performance degradation as model size increased within the Gemma family specifically?",
          "answer": "Larger LLMs tended to be more sensitive to these perturbations, indicating a potential overreliance on fixed prompt patterns."
        },
        {
          "question": "What paradoxical relationship did the study uncover between a model's baseline accuracy on the original MMLU benchmark and its robustness to prompt rephrasing?",
          "answer": "The study uncovered a paradoxical relationship where models with higher baseline accuracy showed greater sensitivity to prompt rephrasing, indicating a potential overreliance on fixed prompt patterns."
        },
        {
          "question": "The paper contrasts the Llama family's stability with other models. What does this imply about how architectural choices or training strategies might influence a model's reliance on superficial cues versus genuine understanding?",
          "answer": "The Llama family's stability suggests that architectural choices or training strategies might influence a model's reliance on superficial cues versus genuine understanding, implying that these factors contribute to greater robustness."
        },
        {
          "question": "Besides the Llama family, some other models like Falcon 7B showed statistically insignificant differences. What reason does the paper provide for their apparent robustness?",
          "answer": "Not enough information in summary"
        },
        {
          "question": "ACU. Which LLM family is noted in the abstract for showing insignificant performance degradation?",
          "answer": "The Llama family is noted for showing insignificant performance degradation."
        },
        {
          "question": "ACU. How many of the 26 evaluated models exhibited statistically significant performance differences under perturbation?",
          "answer": "20 out of 26 evaluated models exhibited statistically significant performance differences under perturbation."
        },
        {
          "question": "ACU. What was the average drop in accuracy across all evaluated models when tested on the rephrased prompts?",
          "answer": "The average drop in accuracy across all evaluated models when tested on the rephrased prompts was 2.15%."
        }
      ],
      "judge": {
        "evaluations": [
          {
            "qa": {
              "question": "According to the study's findings, which specific LLM family demonstrated the most consistent robustness by maintaining relatively stable accuracy on perturbed prompts?",
              "answer": "The Llama family demonstrated the most consistent robustness by maintaining relatively stable accuracy on perturbed prompts."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "What trend was observed regarding performance degradation as model size increased within the Gemma family specifically?",
              "answer": "Larger LLMs tended to be more sensitive to these perturbations, indicating a potential overreliance on fixed prompt patterns."
            },
            "result": false,
            "issue": "The answer is too general and does not specifically address the Gemma family's trend as requested by the question. The article states: \"models in the Gemma family show a progressive increase in ∆1.0 with higher parameter counts\"."
          },
          {
            "qa": {
              "question": "What paradoxical relationship did the study uncover between a model's baseline accuracy on the original MMLU benchmark and its robustness to prompt rephrasing?",
              "answer": "The study uncovered a paradoxical relationship where models with higher baseline accuracy showed greater sensitivity to prompt rephrasing, indicating a potential overreliance on fixed prompt patterns."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "The paper contrasts the Llama family's stability with other models. What does this imply about how architectural choices or training strategies might influence a model's reliance on superficial cues versus genuine understanding?",
              "answer": "The Llama family's stability suggests that architectural choices or training strategies might influence a model's reliance on superficial cues versus genuine understanding, implying that these factors contribute to greater robustness."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "Besides the Llama family, some other models like Falcon 7B showed statistically insignificant differences. What reason does the paper provide for their apparent robustness?",
              "answer": "Not enough information in summary"
            },
            "result": false,
            "issue": "The summary states 'Not enough information in summary', but the article does provide a reason: 'likely due to their lower baseline accuracy'."
          },
          {
            "qa": {
              "question": "ACU. Which LLM family is noted in the abstract for showing insignificant performance degradation?",
              "answer": "The Llama family is noted for showing insignificant performance degradation."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "ACU. How many of the 26 evaluated models exhibited statistically significant performance differences under perturbation?",
              "answer": "20 out of 26 evaluated models exhibited statistically significant performance differences under perturbation."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "ACU. What was the average drop in accuracy across all evaluated models when tested on the rephrased prompts?",
              "answer": "The average drop in accuracy across all evaluated models when tested on the rephrased prompts was 2.15%."
            },
            "result": true,
            "issue": ""
          }
        ],
        "judgment": false,
        "sections_to_highlight": [
          "Results — Performance degradation by model size",
          "Results — Performance degradation vs. baseline accuracy",
          "Methodology — C-BOD framework overview",
          "Limitations — computational cost of iterative rephrasing",
          "Discussion — reasons for LLM overfitting",
          "Related Work — gaps in current overfitting detection methods"
        ]
      },
      "correct_count_all": 6,
      "correct_count_acu": 3,
      "num_of_questions": 5,
      "sections_to_highlight": [
        "Results — Performance degradation by model size",
        "Results — Performance degradation vs. baseline accuracy",
        "Methodology — C-BOD framework overview",
        "Limitations — computational cost of iterative rephrasing",
        "Discussion — reasons for LLM overfitting",
        "Related Work — gaps in current overfitting detection methods"
      ],
      "sections_to_highlight_size": 6,
      "qa_failures_present": true
    },
    {
      "iteration_number": 14,
      "summary": "1. SUMMARY:\nThis research introduces the Chameleon Benchmark Overfit Detector (C-BOD), a meta-evaluation framework designed to assess Large Language Models' (LLMs) reliance on dataset-specific surface cues rather than true language understanding. C-BOD systematically distorts benchmark prompts, preserving semantic content and labels, to detect LLM overfitting. Applied to the MMLU benchmark with 26 LLMs, the study found that most models exhibited a performance degradation of 2.15% under modest rephrasing, with 20 out of 26 showing statistically significant differences. Notably, larger LLMs and those with higher baseline accuracy were more susceptible to performance drops, indicating a tendency to over-rely on fixed prompt patterns. In contrast, the Llama family and models with lower baseline accuracy demonstrated reduced degradation, suggesting less dependency on superficial cues. This highlights a paradox where high benchmark scores may mask underlying vulnerabilities, emphasizing the need for evaluations that prioritize prompt resilience and generalization across diverse LLM architectural families.\n\n2. KEY HIGHLIGHTS:\n*   LLMs often overfit to benchmark-specific surface cues, leading to performance drops when prompts are rephrased, even while preserving semantic meaning.\n*   Larger LLMs and those with higher baseline accuracy tend to exhibit greater performance degradation under prompt perturbations, suggesting a stronger reliance on fixed patterns.\n*   The Llama family of models, along with those with lower baseline accuracy, demonstrated more robustness to rephrasing, indicating reduced dependency on superficial cues.\n*   The C-BOD framework is dataset-agnostic, enabling its application to various benchmarks to promote more resilient LLM evaluation beyond leaderboard scores.",
      "qa_evaluations": [
        {
          "qa": {
            "question": "According to the study's findings, which specific LLM family demonstrated the most consistent robustness by maintaining relatively stable accuracy on perturbed prompts?",
            "answer": "The Llama family of models demonstrated the most consistent robustness by maintaining relatively stable accuracy on perturbed prompts."
          },
          "result": true,
          "issue": null
        },
        {
          "qa": {
            "question": "What trend was observed regarding performance degradation as model size increased within the Gemma family specifically?",
            "answer": "Not enough information in summary"
          },
          "result": false,
          "issue": "Not enough information"
        },
        {
          "qa": {
            "question": "What paradoxical relationship did the study uncover between a model's baseline accuracy on the original MMLU benchmark and its robustness to prompt rephrasing?",
            "answer": "The study uncovered a paradox where models with higher baseline accuracy on the MMLU benchmark were more susceptible to performance drops when prompts were rephrased, indicating a tendency to over-rely on fixed prompt patterns. In contrast, models with lower baseline accuracy demonstrated reduced degradation, suggesting less dependency on superficial cues."
          },
          "result": true,
          "issue": null
        },
        {
          "qa": {
            "question": "The paper contrasts the Llama family's stability with other models. What does this imply about how architectural choices or training strategies might influence a model's reliance on superficial cues versus genuine understanding?",
            "answer": "The stability of the Llama family implies that architectural choices or training strategies might influence a model's reliance on superficial cues versus genuine understanding, suggesting that their approach leads to less dependency on superficial cues."
          },
          "result": true,
          "issue": null
        },
        {
          "qa": {
            "question": "Besides the Llama family, some other models like Falcon 7B showed statistically insignificant differences. What reason does the paper provide for their apparent robustness?",
            "answer": "Not enough information in summary"
          },
          "result": false,
          "issue": "Not enough information"
        },
        {
          "qa": {
            "question": "ACU. Which LLM family is noted in the abstract for showing insignificant performance degradation?",
            "answer": "The Llama family is noted for showing insignificant performance degradation."
          },
          "result": true,
          "issue": null
        },
        {
          "qa": {
            "question": "ACU. How many of the 26 evaluated models exhibited statistically significant performance differences under perturbation?",
            "answer": "20 out of 26 models exhibited statistically significant performance differences under perturbation."
          },
          "result": true,
          "issue": null
        },
        {
          "qa": {
            "question": "ACU. What was the average drop in accuracy across all evaluated models when tested on the rephrased prompts?",
            "answer": "The average drop in accuracy across all evaluated models when tested on the rephrased prompts was 2.15%."
          },
          "result": true,
          "issue": null
        }
      ],
      "qa_pairs": [
        {
          "question": "According to the study's findings, which specific LLM family demonstrated the most consistent robustness by maintaining relatively stable accuracy on perturbed prompts?",
          "answer": "The Llama family of models demonstrated the most consistent robustness by maintaining relatively stable accuracy on perturbed prompts."
        },
        {
          "question": "What trend was observed regarding performance degradation as model size increased within the Gemma family specifically?",
          "answer": "Not enough information in summary"
        },
        {
          "question": "What paradoxical relationship did the study uncover between a model's baseline accuracy on the original MMLU benchmark and its robustness to prompt rephrasing?",
          "answer": "The study uncovered a paradox where models with higher baseline accuracy on the MMLU benchmark were more susceptible to performance drops when prompts were rephrased, indicating a tendency to over-rely on fixed prompt patterns. In contrast, models with lower baseline accuracy demonstrated reduced degradation, suggesting less dependency on superficial cues."
        },
        {
          "question": "The paper contrasts the Llama family's stability with other models. What does this imply about how architectural choices or training strategies might influence a model's reliance on superficial cues versus genuine understanding?",
          "answer": "The stability of the Llama family implies that architectural choices or training strategies might influence a model's reliance on superficial cues versus genuine understanding, suggesting that their approach leads to less dependency on superficial cues."
        },
        {
          "question": "Besides the Llama family, some other models like Falcon 7B showed statistically insignificant differences. What reason does the paper provide for their apparent robustness?",
          "answer": "Not enough information in summary"
        },
        {
          "question": "ACU. Which LLM family is noted in the abstract for showing insignificant performance degradation?",
          "answer": "The Llama family is noted for showing insignificant performance degradation."
        },
        {
          "question": "ACU. How many of the 26 evaluated models exhibited statistically significant performance differences under perturbation?",
          "answer": "20 out of 26 models exhibited statistically significant performance differences under perturbation."
        },
        {
          "question": "ACU. What was the average drop in accuracy across all evaluated models when tested on the rephrased prompts?",
          "answer": "The average drop in accuracy across all evaluated models when tested on the rephrased prompts was 2.15%."
        }
      ],
      "judge": {
        "evaluations": [
          {
            "qa": {
              "question": "According to the study's findings, which specific LLM family demonstrated the most consistent robustness by maintaining relatively stable accuracy on perturbed prompts?",
              "answer": "The Llama family of models demonstrated the most consistent robustness by maintaining relatively stable accuracy on perturbed prompts."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "What trend was observed regarding performance degradation as model size increased within the Gemma family specifically?",
              "answer": "Not enough information in summary"
            },
            "result": false,
            "issue": "The article provides a general trend for model size and performance degradation but does not specify this trend for the Gemma family alone."
          },
          {
            "qa": {
              "question": "What paradoxical relationship did the study uncover between a model's baseline accuracy on the original MMLU benchmark and its robustness to prompt rephrasing?",
              "answer": "The study uncovered a paradox where models with higher baseline accuracy on the MMLU benchmark were more susceptible to performance drops when prompts were rephrased, indicating a tendency to over-rely on fixed prompt patterns. In contrast, models with lower baseline accuracy demonstrated reduced degradation, suggesting less dependency on superficial cues."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "The paper contrasts the Llama family's stability with other models. What does this imply about how architectural choices or training strategies might influence a model's reliance on superficial cues versus genuine understanding?",
              "answer": "The stability of the Llama family implies that architectural choices or training strategies might influence a model's reliance on superficial cues versus genuine understanding, suggesting that their approach leads to less dependency on superficial cues."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "Besides the Llama family, some other models like Falcon 7B showed statistically insignificant differences. What reason does the paper provide for their apparent robustness?",
              "answer": "Not enough information in summary"
            },
            "result": false,
            "issue": "The summary states that models with lower baseline accuracy, such as Falcon 7B, showed statistically insignificant differences, but does not elaborate on the specific reasons for this robustness beyond their lower baseline accuracy."
          },
          {
            "qa": {
              "question": "ACU. Which LLM family is noted in the abstract for showing insignificant performance degradation?",
              "answer": "The Llama family is noted for showing insignificant performance degradation."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "ACU. How many of the 26 evaluated models exhibited statistically significant performance differences under perturbation?",
              "answer": "20 out of 26 models exhibited statistically significant performance differences under perturbation."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "ACU. What was the average drop in accuracy across all evaluated models when tested on the rephrased prompts?",
              "answer": "The average drop in accuracy across all evaluated models when tested on the rephrased prompts was 2.15%."
            },
            "result": true,
            "issue": ""
          }
        ],
        "judgment": false,
        "sections_to_highlight": [
          "Results — performance difference by model size",
          "Results — performance difference by baseline accuracy",
          "Methodology — C-BOD framework details",
          "Limitations — scope of overfitting detection",
          "Discussion — reasons for LLM overfitting",
          "Related Work — existing evaluation methods vs C-BOD"
        ]
      },
      "correct_count_all": 6,
      "correct_count_acu": 3,
      "num_of_questions": 5,
      "sections_to_highlight": [
        "Results — performance difference by model size",
        "Results — performance difference by baseline accuracy",
        "Methodology — C-BOD framework details",
        "Limitations — scope of overfitting detection",
        "Discussion — reasons for LLM overfitting",
        "Related Work — existing evaluation methods vs C-BOD"
      ],
      "sections_to_highlight_size": 6,
      "qa_failures_present": true
    },
    {
      "iteration_number": 15,
      "summary": "1. SUMMARY:\nThis paper introduces the Chameleon Benchmark Overfit Detector (C-BOD), a meta-evaluation framework designed to expose LLMs' overreliance on dataset-specific surface cues rather than true language understanding. C-BOD systematically distorts benchmark prompts while preserving semantic content and labels, allowing for the detection of overfitting. Evaluated on the MMLU benchmark with 26 LLMs, C-BOD revealed that most models (20 out of 26) experienced statistically significant performance degradation under modest rephrasing. Notably, larger LLMs and those with higher baseline accuracy exhibited greater performance drops, suggesting a dependency on fixed prompt patterns. In contrast, the Llama family and models with lower baseline accuracy demonstrated reduced degradation, indicating less reliance on superficial cues. This suggests differential robustness across LLM architectural families, where smaller or specifically designed models (like Llama) may be more resilient. The findings challenge current evaluation practices that prioritize leaderboard scores over prompt resilience and generalization. C-BOD's dataset-agnostic nature allows for broad applicability in identifying and mitigating overfitting.\n\n2. KEY HIGHLIGHTS:\n*   20 out of 26 evaluated LLMs showed statistically significant performance degradation when prompts were rephrased, indicating widespread overfitting to surface cues.\n*   Larger LLMs and those with higher baseline accuracy tended to be more sensitive to prompt rephrasing, suggesting they over-rely on fixed patterns.\n*   The Llama family of models, along with those with lower baseline accuracy (e.g., Falcon 7B, DeepSeek 7B, Qwen 2.5 3B, Jetmoe 8B), exhibited more robust performance, with insignificant degradation under rephrasing.\n*   A paradoxical relationship exists: models with higher original accuracy were more prone to performance drops when prompts were altered, highlighting that high benchmark scores may mask vulnerabilities.\n*   C-BOD's dataset-agnostic nature allows it to be applied across various benchmarks to promote evaluations that prioritize resilience and generalization over mere leaderboard performance.",
      "qa_evaluations": [
        {
          "qa": {
            "question": "According to the study's findings, which specific LLM family demonstrated the most consistent robustness by maintaining relatively stable accuracy on perturbed prompts?",
            "answer": "The Llama family of models demonstrated the most consistent robustness by maintaining relatively stable accuracy on perturbed prompts."
          },
          "result": true,
          "issue": null
        },
        {
          "qa": {
            "question": "What trend was observed regarding performance degradation as model size increased within the Gemma family specifically?",
            "answer": "The summary states that larger LLMs tended to be more sensitive to prompt rephrasing, suggesting they over-rely on fixed patterns. However, it does not specifically detail this trend for the Gemma family."
          },
          "result": false,
          "issue": "Not enough information in summary"
        },
        {
          "qa": {
            "question": "What paradoxical relationship did the study uncover between a model's baseline accuracy on the original MMLU benchmark and its robustness to prompt rephrasing?",
            "answer": "The study uncovered a paradoxical relationship where models with higher baseline accuracy on the original MMLU benchmark were more prone to performance drops when prompts were altered, indicating that high benchmark scores may mask vulnerabilities."
          },
          "result": true,
          "issue": null
        },
        {
          "qa": {
            "question": "The paper contrasts the Llama family's stability with other models. What does this imply about how architectural choices or training strategies might influence a model's reliance on superficial cues versus genuine understanding?",
            "answer": "The Llama family's stability implies that architectural choices or training strategies might influence a model's reliance on superficial cues versus genuine understanding, suggesting that models like Llama may be more resilient due to these factors."
          },
          "result": true,
          "issue": null
        },
        {
          "qa": {
            "question": "Besides the Llama family, some other models like Falcon 7B showed statistically insignificant differences. What reason does the paper provide for their apparent robustness?",
            "answer": "The paper suggests that models with lower baseline accuracy, such as Falcon 7B, exhibited more robust performance with insignificant degradation under rephrasing, indicating less reliance on superficial cues."
          },
          "result": true,
          "issue": null
        },
        {
          "qa": {
            "question": "ACU. Which LLM family is noted in the abstract for showing insignificant performance degradation?",
            "answer": "The Llama family of models is noted for showing insignificant performance degradation."
          },
          "result": true,
          "issue": null
        },
        {
          "qa": {
            "question": "ACU. How many of the 26 evaluated models exhibited statistically significant performance differences under perturbation?",
            "answer": "20 out of 26 evaluated models exhibited statistically significant performance differences under perturbation."
          },
          "result": true,
          "issue": null
        },
        {
          "qa": {
            "question": "ACU. What was the average drop in accuracy across all evaluated models when tested on the rephrased prompts?",
            "answer": "The summary does not provide the average drop in accuracy across all evaluated models."
          },
          "result": false,
          "issue": "Not enough information in summary"
        }
      ],
      "qa_pairs": [
        {
          "question": "According to the study's findings, which specific LLM family demonstrated the most consistent robustness by maintaining relatively stable accuracy on perturbed prompts?",
          "answer": "The Llama family of models demonstrated the most consistent robustness by maintaining relatively stable accuracy on perturbed prompts."
        },
        {
          "question": "What trend was observed regarding performance degradation as model size increased within the Gemma family specifically?",
          "answer": "The summary states that larger LLMs tended to be more sensitive to prompt rephrasing, suggesting they over-rely on fixed patterns. However, it does not specifically detail this trend for the Gemma family."
        },
        {
          "question": "What paradoxical relationship did the study uncover between a model's baseline accuracy on the original MMLU benchmark and its robustness to prompt rephrasing?",
          "answer": "The study uncovered a paradoxical relationship where models with higher baseline accuracy on the original MMLU benchmark were more prone to performance drops when prompts were altered, indicating that high benchmark scores may mask vulnerabilities."
        },
        {
          "question": "The paper contrasts the Llama family's stability with other models. What does this imply about how architectural choices or training strategies might influence a model's reliance on superficial cues versus genuine understanding?",
          "answer": "The Llama family's stability implies that architectural choices or training strategies might influence a model's reliance on superficial cues versus genuine understanding, suggesting that models like Llama may be more resilient due to these factors."
        },
        {
          "question": "Besides the Llama family, some other models like Falcon 7B showed statistically insignificant differences. What reason does the paper provide for their apparent robustness?",
          "answer": "The paper suggests that models with lower baseline accuracy, such as Falcon 7B, exhibited more robust performance with insignificant degradation under rephrasing, indicating less reliance on superficial cues."
        },
        {
          "question": "ACU. Which LLM family is noted in the abstract for showing insignificant performance degradation?",
          "answer": "The Llama family of models is noted for showing insignificant performance degradation."
        },
        {
          "question": "ACU. How many of the 26 evaluated models exhibited statistically significant performance differences under perturbation?",
          "answer": "20 out of 26 evaluated models exhibited statistically significant performance differences under perturbation."
        },
        {
          "question": "ACU. What was the average drop in accuracy across all evaluated models when tested on the rephrased prompts?",
          "answer": "The summary does not provide the average drop in accuracy across all evaluated models."
        }
      ],
      "judge": {
        "evaluations": [
          {
            "qa": {
              "question": "According to the study's findings, which specific LLM family demonstrated the most consistent robustness by maintaining relatively stable accuracy on perturbed prompts?",
              "answer": "The Llama family of models demonstrated the most consistent robustness by maintaining relatively stable accuracy on perturbed prompts."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "What trend was observed regarding performance degradation as model size increased within the Gemma family specifically?",
              "answer": "The summary states that larger LLMs tended to be more sensitive to prompt rephrasing, suggesting they over-rely on fixed patterns. However, it does not specifically detail this trend for the Gemma family."
            },
            "result": false,
            "issue": "The article states in section 5.2: 'For example, models in the Gemma family show a progressive increase in ∆1.0 with higher parameter counts, while Llama models maintain low ∆1.0 values across scales.' This information is missing from the summary."
          },
          {
            "qa": {
              "question": "What paradoxical relationship did the study uncover between a model's baseline accuracy on the original MMLU benchmark and its robustness to prompt rephrasing?",
              "answer": "The study uncovered a paradoxical relationship where models with higher baseline accuracy on the original MMLU benchmark were more prone to performance drops when prompts were altered, indicating that high benchmark scores may mask vulnerabilities."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "The paper contrasts the Llama family's stability with other models. What does this imply about how architectural choices or training strategies might influence a model's reliance on superficial cues versus genuine understanding?",
              "answer": "The Llama family's stability implies that architectural choices or training strategies might influence a model's reliance on superficial cues versus genuine understanding, suggesting that models like Llama may be more resilient due to these factors."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "Besides the Llama family, some other models like Falcon 7B showed statistically insignificant differences. What reason does the paper provide for their apparent robustness?",
              "answer": "The paper suggests that models with lower baseline accuracy, such as Falcon 7B, exhibited more robust performance with insignificant degradation under rephrasing, indicating less reliance on superficial cues."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "ACU. Which LLM family is noted in the abstract for showing insignificant performance degradation?",
              "answer": "The Llama family of models is noted for showing insignificant performance degradation."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "ACU. How many of the 26 evaluated models exhibited statistically significant performance differences under perturbation?",
              "answer": "20 out of 26 evaluated models exhibited statistically significant performance differences under perturbation."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "ACU. What was the average drop in accuracy across all evaluated models when tested on the rephrased prompts?",
              "answer": "The summary does not provide the average drop in accuracy across all evaluated models."
            },
            "result": false,
            "issue": "The article states in section 5.1: 'Across all evaluated models, the average drop in accuracy was 2.15%.' This information is missing from the summary."
          }
        ],
        "judgment": false,
        "sections_to_highlight": [
          "Results — Average performance degradation",
          "Results — Gemma family performance trend",
          "Methodology — C-BOD framework details",
          "Discussion — LLM overfitting causes",
          "Results — Model size vs. performance difference",
          "Results — Model accuracy vs. performance difference"
        ]
      },
      "correct_count_all": 6,
      "correct_count_acu": 2,
      "num_of_questions": 5,
      "sections_to_highlight": [
        "Results — Average performance degradation",
        "Results — Gemma family performance trend",
        "Methodology — C-BOD framework details",
        "Discussion — LLM overfitting causes",
        "Results — Model size vs. performance difference",
        "Results — Model accuracy vs. performance difference"
      ],
      "sections_to_highlight_size": 6,
      "qa_failures_present": true
    }
  ],
  "final_summary": "1. SUMMARY:\nThis paper introduces the Chameleon Benchmark Overfit Detector (C-BOD), a meta-evaluation framework designed to expose LLMs' overreliance on dataset-specific surface cues rather than true language understanding. C-BOD systematically distorts benchmark prompts while preserving semantic content and labels, allowing for the detection of overfitting. Evaluated on the MMLU benchmark with 26 LLMs, C-BOD revealed that most models (20 out of 26) experienced statistically significant performance degradation under modest rephrasing. Notably, larger LLMs and those with higher baseline accuracy exhibited greater performance drops, suggesting a dependency on fixed prompt patterns. In contrast, the Llama family and models with lower baseline accuracy demonstrated reduced degradation, indicating less reliance on superficial cues. This suggests differential robustness across LLM architectural families, where smaller or specifically designed models (like Llama) may be more resilient. The findings challenge current evaluation practices that prioritize leaderboard scores over prompt resilience and generalization. C-BOD's dataset-agnostic nature allows for broad applicability in identifying and mitigating overfitting.\n\n2. KEY HIGHLIGHTS:\n*   20 out of 26 evaluated LLMs showed statistically significant performance degradation when prompts were rephrased, indicating widespread overfitting to surface cues.\n*   Larger LLMs and those with higher baseline accuracy tended to be more sensitive to prompt rephrasing, suggesting they over-rely on fixed patterns.\n*   The Llama family of models, along with those with lower baseline accuracy (e.g., Falcon 7B, DeepSeek 7B, Qwen 2.5 3B, Jetmoe 8B), exhibited more robust performance, with insignificant degradation under rephrasing.\n*   A paradoxical relationship exists: models with higher original accuracy were more prone to performance drops when prompts were altered, highlighting that high benchmark scores may mask vulnerabilities.\n*   C-BOD's dataset-agnostic nature allows it to be applied across various benchmarks to promote evaluations that prioritize resilience and generalization over mere leaderboard performance.",
  "total_iterations": 15,
  "status": "max_iterations_reached",
  "questions": [
    "According to the study's findings, which specific LLM family demonstrated the most consistent robustness by maintaining relatively stable accuracy on perturbed prompts?",
    "What trend was observed regarding performance degradation as model size increased within the Gemma family specifically?",
    "What paradoxical relationship did the study uncover between a model's baseline accuracy on the original MMLU benchmark and its robustness to prompt rephrasing?",
    "The paper contrasts the Llama family's stability with other models. What does this imply about how architectural choices or training strategies might influence a model's reliance on superficial cues versus genuine understanding?",
    "Besides the Llama family, some other models like Falcon 7B showed statistically insignificant differences. What reason does the paper provide for their apparent robustness?"
  ],
  "acu_questions": [
    "ACU. Which LLM family is noted in the abstract for showing insignificant performance degradation?",
    "ACU. How many of the 26 evaluated models exhibited statistically significant performance differences under perturbation?",
    "ACU. What was the average drop in accuracy across all evaluated models when tested on the rephrased prompts?"
  ]
}