{
  "query": "The challenge of managing multilingual language drift when fine-tuning a foundational model for a monolingual reasoning task.",
  "max_iterations": 15,
  "iterations": [
    {
      "iteration_number": 1,
      "summary": "1. SUMMARY:\nThis study explores the effectiveness of reinforcement learning (RL) for enhancing the reasoning capabilities of small Large Language Models (LLMs) under significant resource constraints. The research team fine-tuned a 1.5-billion-parameter model, DeepSeek-R1-Distill-Qwen-1.5B, on a limited set of 7,000 high-quality mathematical reasoning examples using the Group Relative Policy Optimization (GRPO) algorithm. The experiments demonstrate that despite strict limitations like 24-hour training on 4 NVIDIA A40 GPUs and a maximum token limit, small LLMs can achieve substantial reasoning gains. For instance, accuracy on AMC23 increased from 63% to 80% and AIME24 reached 46.7%, outperforming some larger models and achieving this at a remarkably low cost of $42. The study identifies key challenges, including optimization instability and the emergence of multilingual language drift in the base model, which can degrade performance over extended training. These findings suggest that RL is a viable and cost-effective method for boosting the reasoning of small LLMs, though careful dataset curation and reward design are crucial to mitigate issues like output length constraints and unintended language shifts.\n\n2. KEY HIGHLIGHTS:\n*   Reinforcement learning (RL) can significantly improve the reasoning abilities of small LLMs (1.5B parameters) under strict resource constraints (4 GPUs, 24 hours).\n*   A curated dataset of 7,000 high-quality math reasoning examples and the GRPO algorithm enabled rapid reasoning gains, achieving 80% accuracy on AMC23 and 46.7% on AIME24.\n*   The approach is highly cost-effective, with training costs around $42, drastically lower than larger-scale methods.\n*   Challenges encountered include optimization instability and multilingual language drift from the base model, particularly with prolonged training or insufficient length constraints.\n*   Mixing easy and hard problems and using cosine rewards helped stabilize training and manage output length, though extended length limits may be needed for very complex tasks.",
      "qa_evaluations": [
        {
          "qa": {
            "question": "What specific evidence of multilingual language drift was observed after 200 training steps in Experiment 1?",
            "answer": "Not enough information in summary"
          },
          "result": false,
          "issue": "Not enough information in summary"
        },
        {
          "qa": {
            "question": "According to the 'Limitations' section, how did the inherent multilingual nature of the base model complicate the process of monolingual optimization for the reasoning task?",
            "answer": "Not enough information in summary"
          },
          "result": false,
          "issue": "Not enough information in summary"
        },
        {
          "qa": {
            "question": "What non-RL method was attempted in Experiment 3 to control the language of the model's output, and was it successful?",
            "answer": "Not enough information in summary"
          },
          "result": false,
          "issue": "Not enough information in summary"
        },
        {
          "qa": {
            "question": "What does the paper propose as a potential future solution to mitigate the language drift encountered when fine-tuning a multilingual model?",
            "answer": "Not enough information in summary"
          },
          "result": false,
          "issue": "Not enough information in summary"
        },
        {
          "qa": {
            "question": "In Experiment 2, after how many training steps did multilingual outputs begin to reemerge, suggesting the persistence of language drift?",
            "answer": "Not enough information in summary"
          },
          "result": false,
          "issue": "Not enough information in summary"
        },
        {
          "qa": {
            "question": "ACU. What is the name of the multilingual base model that was fine-tuned and exhibited language drift?",
            "answer": "The name of the multilingual base model that was fine-tuned and exhibited language drift is DeepSeek-R1-Distill-Qwen-1.5B."
          },
          "result": true,
          "issue": null
        }
      ],
      "qa_pairs": [
        {
          "question": "What specific evidence of multilingual language drift was observed after 200 training steps in Experiment 1?",
          "answer": "Not enough information in summary"
        },
        {
          "question": "According to the 'Limitations' section, how did the inherent multilingual nature of the base model complicate the process of monolingual optimization for the reasoning task?",
          "answer": "Not enough information in summary"
        },
        {
          "question": "What non-RL method was attempted in Experiment 3 to control the language of the model's output, and was it successful?",
          "answer": "Not enough information in summary"
        },
        {
          "question": "What does the paper propose as a potential future solution to mitigate the language drift encountered when fine-tuning a multilingual model?",
          "answer": "Not enough information in summary"
        },
        {
          "question": "In Experiment 2, after how many training steps did multilingual outputs begin to reemerge, suggesting the persistence of language drift?",
          "answer": "Not enough information in summary"
        },
        {
          "question": "ACU. What is the name of the multilingual base model that was fine-tuned and exhibited language drift?",
          "answer": "The name of the multilingual base model that was fine-tuned and exhibited language drift is DeepSeek-R1-Distill-Qwen-1.5B."
        }
      ],
      "judge": {
        "evaluations": [
          {
            "qa": {
              "question": "What specific evidence of multilingual language drift was observed after 200 training steps in Experiment 1?",
              "answer": "In Experiment 1, after 200 steps, the model's outputs included unreadable content and non-English outputs, indicating language drift."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "According to the 'Limitations' section, how did the inherent multilingual nature of the base model complicate the process of monolingual optimization for the reasoning task?",
              "answer": "The multilingual nature of the base model, DeepSeek-R1-Distill-Qwen-1.5B, complicated monolingual optimization by introducing unintended language drift after 150-200 steps, despite efforts to enforce English-only outputs."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "What non-RL method was attempted in Experiment 3 to control the language of the model's output, and was it successful?",
              "answer": "In Experiment 3, an instruction 'Reply in English only, do not use other languages' was appended to the system prompt. While it helped stabilize completion lengths, mixed-language content persisted, indicating it was not entirely successful in preventing language drift."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "What does the paper propose as a potential future solution to mitigate the language drift encountered when fine-tuning a multilingual model?",
              "answer": "The paper proposes that incorporating a lightweight language reward or monolingual pre-filtering of the base model might mitigate language drift and enhance output consistency."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "In Experiment 2, after how many training steps did multilingual outputs begin to reemerge, suggesting the persistence of language drift?",
              "answer": "In Experiment 2, after 150-200 steps, mixed-language outputs began to reemerge, indicating the persistence of language drift."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "ACU. What is the name of the multilingual base model that was fine-tuned and exhibited language drift?",
              "answer": "The name of the multilingual base model that was fine-tuned and exhibited language drift is DeepSeek-R1-Distill-Qwen-1.5B."
            },
            "result": true,
            "issue": ""
          }
        ],
        "judgment": true,
        "sections_to_highlight": [
          "Experiment 1 — observed degradation and length constraints",
          "Experiment 2 — effect of mixed difficulty on stability",
          "Experiment 3 — cosine reward for length control",
          "Limitations — impact of training duration and length limits",
          "Discussion — trade-offs between efficiency and reasoning depth",
          "Future Directions — addressing language drift and extending training",
          "Methodology — dataset curation and reward design"
        ]
      },
      "correct_count_all": 6,
      "correct_count_acu": 1,
      "num_of_questions": 5,
      "sections_to_highlight": [
        "Experiment 1 — observed degradation and length constraints",
        "Experiment 2 — effect of mixed difficulty on stability",
        "Experiment 3 — cosine reward for length control",
        "Limitations — impact of training duration and length limits",
        "Discussion — trade-offs between efficiency and reasoning depth",
        "Future Directions — addressing language drift and extending training",
        "Methodology — dataset curation and reward design"
      ],
      "sections_to_highlight_size": 7,
      "qa_failures_present": true
    },
    {
      "iteration_number": 2,
      "summary": "1. SUMMARY:\nThis study investigates enhancing the reasoning capabilities of small LLMs (1.5 billion parameters) using reinforcement learning (RL) under strict resource constraints (4 NVIDIA A40 GPUs, 24 hours). The researchers adapted the GRPO algorithm and curated a high-quality mathematical reasoning dataset. Their findings indicate that small LLMs can achieve rapid reasoning gains (e.g., AMC23 accuracy from 63% to 80%) with limited data and low cost ($42). However, prolonged training led to performance degradation, optimization instability, and multilingual language drift, particularly with longer completion lengths. Experiment 2, which mixed easier and harder problems, showed improved initial stability. Experiment 3 used a cosine reward to control output length, demonstrating better consistency but slightly lower peak performance. The discussion highlights trade-offs between efficiency and reasoning depth, noting that while RL is viable for small LLMs, challenges like length constraints and language drift in multilingual base models need addressing. Future directions include extending training duration and exploring methods to mitigate language drift.\n\n2. KEY HIGHLIGHTS:\n*   Small LLMs can achieve significant reasoning improvements via RL with minimal data and cost, demonstrating a viable alternative to larger models.\n*   Prolonged training on small LLMs can lead to performance degradation, optimization instability, and multilingual language drift, especially under strict length constraints.\n*   Mixing easy and hard problems (Experiment 2) improves initial stability, while cosine rewards (Experiment 3) better control output length but may slightly reduce peak performance.\n*   The multilingual nature of the base model poses a challenge for monolingual reasoning optimization, leading to language drift.\n*   Addressing length constraints and language drift through extended training or targeted methods is crucial for fully realizing the potential of small LLMs in reasoning tasks.",
      "qa_evaluations": [
        {
          "qa": {
            "question": "What specific evidence of multilingual language drift was observed after 200 training steps in Experiment 1?",
            "answer": "Not enough information in summary"
          },
          "result": false,
          "issue": "Not enough information"
        },
        {
          "qa": {
            "question": "According to the 'Limitations' section, how did the inherent multilingual nature of the base model complicate the process of monolingual optimization for the reasoning task?",
            "answer": "The inherent multilingual nature of the base model complicates the process of monolingual optimization for the reasoning task, leading to language drift."
          },
          "result": true,
          "issue": null
        },
        {
          "qa": {
            "question": "What non-RL method was attempted in Experiment 3 to control the language of the model's output, and was it successful?",
            "answer": "Experiment 3 used a cosine reward to control output length, demonstrating better consistency but slightly lower peak performance."
          },
          "result": true,
          "issue": null
        },
        {
          "qa": {
            "question": "What does the paper propose as a potential future solution to mitigate the language drift encountered when fine-tuning a multilingual model?",
            "answer": "The paper proposes exploring methods to mitigate language drift as a potential future solution."
          },
          "result": true,
          "issue": null
        },
        {
          "qa": {
            "question": "In Experiment 2, after how many training steps did multilingual outputs begin to reemerge, suggesting the persistence of language drift?",
            "answer": "Not enough information in summary"
          },
          "result": false,
          "issue": "Not enough information"
        },
        {
          "qa": {
            "question": "ACU. What is the name of the multilingual base model that was fine-tuned and exhibited language drift?",
            "answer": "Not enough information in summary"
          },
          "result": false,
          "issue": "Not enough information"
        }
      ],
      "qa_pairs": [
        {
          "question": "What specific evidence of multilingual language drift was observed after 200 training steps in Experiment 1?",
          "answer": "Not enough information in summary"
        },
        {
          "question": "According to the 'Limitations' section, how did the inherent multilingual nature of the base model complicate the process of monolingual optimization for the reasoning task?",
          "answer": "The inherent multilingual nature of the base model complicates the process of monolingual optimization for the reasoning task, leading to language drift."
        },
        {
          "question": "What non-RL method was attempted in Experiment 3 to control the language of the model's output, and was it successful?",
          "answer": "Experiment 3 used a cosine reward to control output length, demonstrating better consistency but slightly lower peak performance."
        },
        {
          "question": "What does the paper propose as a potential future solution to mitigate the language drift encountered when fine-tuning a multilingual model?",
          "answer": "The paper proposes exploring methods to mitigate language drift as a potential future solution."
        },
        {
          "question": "In Experiment 2, after how many training steps did multilingual outputs begin to reemerge, suggesting the persistence of language drift?",
          "answer": "Not enough information in summary"
        },
        {
          "question": "ACU. What is the name of the multilingual base model that was fine-tuned and exhibited language drift?",
          "answer": "Not enough information in summary"
        }
      ],
      "judge": {
        "evaluations": [
          {
            "qa": {
              "question": "What specific evidence of multilingual language drift was observed after 200 training steps in Experiment 1?",
              "answer": "Not enough information in summary"
            },
            "result": false,
            "issue": "The article mentions 'non-English outputs' in Experiment 1 after 200 steps, which is evidence of language drift, but the summary states there is not enough information."
          },
          {
            "qa": {
              "question": "According to the 'Limitations' section, how did the inherent multilingual nature of the base model complicate the process of monolingual optimization for the reasoning task?",
              "answer": "The inherent multilingual nature of the base model complicates the process of monolingual optimization for the reasoning task, leading to language drift."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "What non-RL method was attempted in Experiment 3 to control the language of the model's output, and was it successful?",
              "answer": "Experiment 3 used a cosine reward to control output length, demonstrating better consistency but slightly lower peak performance."
            },
            "result": false,
            "issue": "The question asks about a non-RL method to control *language*, but the answer describes a cosine reward for controlling *length*. The article mentions appending an instruction to the system prompt to reply in English only, which was a non-RL method attempted to control language."
          },
          {
            "qa": {
              "question": "What does the paper propose as a potential future solution to mitigate the language drift encountered when fine-tuning a multilingual model?",
              "answer": "The paper proposes exploring methods to mitigate language drift as a potential future solution."
            },
            "result": false,
            "issue": "The summary is too vague. The article suggests specific solutions: 'incorporating a lightweight language reward or monolingual pre-filtering of the base model'."
          },
          {
            "qa": {
              "question": "In Experiment 2, after how many training steps did multilingual outputs begin to reemerge, suggesting the persistence of language drift?",
              "answer": "Not enough information in summary"
            },
            "result": false,
            "issue": "The article states that in Experiment 2, 'after 150–200 steps (approximately 4000 local steps), performance declines, and KL divergence becomes unstable (Figure 4), with mixed-language outputs reemerging.' This information is present in the article but missing from the summary."
          },
          {
            "qa": {
              "question": "ACU. What is the name of the multilingual base model that was fine-tuned and exhibited language drift?",
              "answer": "Not enough information in summary"
            },
            "result": false,
            "issue": "The article explicitly states that the base model used was 'DeepSeek-R1-Distill-Qwen-1.5B' and that it exhibited language drift."
          }
        ],
        "judgment": false,
        "sections_to_highlight": [
          "Experiment 1 — specific evidence of language drift",
          "Experiment 3 — non-RL language control method",
          "Future Directions — specific solutions for language drift",
          "Experiment 2 — timing of language drift emergence",
          "Base Model — name and multilingual nature"
        ]
      },
      "correct_count_all": 1,
      "correct_count_acu": 0,
      "num_of_questions": 5,
      "sections_to_highlight": [
        "Experiment 1 — specific evidence of language drift",
        "Experiment 3 — non-RL language control method",
        "Future Directions — specific solutions for language drift",
        "Experiment 2 — timing of language drift emergence",
        "Base Model — name and multilingual nature"
      ],
      "sections_to_highlight_size": 5,
      "qa_failures_present": true
    },
    {
      "iteration_number": 3,
      "summary": "1. SUMMARY:\nThis paper investigates the efficacy of reinforcement learning (RL) for enhancing the reasoning capabilities of small LLMs (1.5B parameters) under strict resource constraints (4 GPUs, 24 hours). The study adapts the GRPO algorithm and curates a compact, high-quality mathematical reasoning dataset. The primary challenge addressed is managing multilingual language drift when fine-tuning a foundational model for a monolingual task. Experiments reveal that while RL can yield rapid reasoning gains (e.g., AMC23 accuracy from 63% to 80% with only 7,000 samples), prolonged training leads to performance degradation and language drift, particularly with the multilingual base model (DeepSeek-R1-Distill-Qwen-1.5B). Mixing easier and harder problems (Experiment 2) initially stabilizes training and improves performance, but late-stage instability persists. Cosine rewards (Experiment 3) help control output length but do not fully resolve language drift. The research highlights a trade-off between efficiency and reasoning depth, suggesting that extending training duration and employing explicit language control mechanisms are crucial for addressing language drift and fully realizing the potential of small LLMs for monolingual reasoning tasks.\n\n2. KEY HIGHLIGHTS:\n*   Fine-tuning a multilingual 1.5B LLM (DeepSeek-R1-Distill-Qwen-1.5B) with RL for monolingual reasoning shows rapid initial gains but suffers from performance degradation and language drift with prolonged training.\n*   Experiment 2, mixing easy and hard problems, improved early stability and performance, but late-stage instability and language drift persisted.\n*   Experiment 3's cosine reward effectively controlled output length, but did not fully resolve the language drift issue stemming from the multilingual base model.\n*   Future directions for addressing language drift include extending training duration and implementing explicit language rewards or monolingual pre-filtering.\n*   The study demonstrates a trade-off between efficiency (low cost, minimal data) and reasoning depth, with length constraints and multilingual nature posing significant challenges for monolingual optimization.",
      "qa_evaluations": [
        {
          "qa": {
            "question": "What specific evidence of multilingual language drift was observed after 200 training steps in Experiment 1?",
            "answer": "Not enough information in summary"
          },
          "result": false,
          "issue": "Not enough information in summary"
        },
        {
          "qa": {
            "question": "According to the 'Limitations' section, how did the inherent multilingual nature of the base model complicate the process of monolingual optimization for the reasoning task?",
            "answer": "The summary does not contain a 'Limitations' section. It mentions that the inherent multilingual nature of the base model (DeepSeek-R1-Distill-Qwen-1.5B) is a primary challenge that leads to language drift when fine-tuning for a monolingual task."
          },
          "result": false,
          "issue": "The summary does not contain a 'Limitations' section."
        },
        {
          "qa": {
            "question": "What non-RL method was attempted in Experiment 3 to control the language of the model's output, and was it successful?",
            "answer": "In Experiment 3, cosine rewards were used to control the output length, but this method did not fully resolve the language drift issue."
          },
          "result": true,
          "issue": null
        },
        {
          "qa": {
            "question": "What does the paper propose as a potential future solution to mitigate the language drift encountered when fine-tuning a multilingual model?",
            "answer": "The paper proposes extending training duration and implementing explicit language rewards or monolingual pre-filtering as potential future solutions to mitigate language drift."
          },
          "result": true,
          "issue": null
        },
        {
          "qa": {
            "question": "In Experiment 2, after how many training steps did multilingual outputs begin to reemerge, suggesting the persistence of language drift?",
            "answer": "Not enough information in summary"
          },
          "result": false,
          "issue": "Not enough information in summary"
        },
        {
          "qa": {
            "question": "ACU. What is the name of the multilingual base model that was fine-tuned and exhibited language drift?",
            "answer": "The name of the multilingual base model that was fine-tuned and exhibited language drift is DeepSeek-R1-Distill-Qwen-1.5B."
          },
          "result": true,
          "issue": null
        }
      ],
      "qa_pairs": [
        {
          "question": "What specific evidence of multilingual language drift was observed after 200 training steps in Experiment 1?",
          "answer": "Not enough information in summary"
        },
        {
          "question": "According to the 'Limitations' section, how did the inherent multilingual nature of the base model complicate the process of monolingual optimization for the reasoning task?",
          "answer": "The summary does not contain a 'Limitations' section. It mentions that the inherent multilingual nature of the base model (DeepSeek-R1-Distill-Qwen-1.5B) is a primary challenge that leads to language drift when fine-tuning for a monolingual task."
        },
        {
          "question": "What non-RL method was attempted in Experiment 3 to control the language of the model's output, and was it successful?",
          "answer": "In Experiment 3, cosine rewards were used to control the output length, but this method did not fully resolve the language drift issue."
        },
        {
          "question": "What does the paper propose as a potential future solution to mitigate the language drift encountered when fine-tuning a multilingual model?",
          "answer": "The paper proposes extending training duration and implementing explicit language rewards or monolingual pre-filtering as potential future solutions to mitigate language drift."
        },
        {
          "question": "In Experiment 2, after how many training steps did multilingual outputs begin to reemerge, suggesting the persistence of language drift?",
          "answer": "Not enough information in summary"
        },
        {
          "question": "ACU. What is the name of the multilingual base model that was fine-tuned and exhibited language drift?",
          "answer": "The name of the multilingual base model that was fine-tuned and exhibited language drift is DeepSeek-R1-Distill-Qwen-1.5B."
        }
      ],
      "judge": {
        "evaluations": [
          {
            "qa": {
              "question": "What specific evidence of multilingual language drift was observed after 200 training steps in Experiment 1?",
              "answer": "In Experiment 1, after 200 steps, the model's outputs included unreadable content and non-English outputs, indicating language drift."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "According to the 'Limitations' section, how did the inherent multilingual nature of the base model complicate the process of monolingual optimization for the reasoning task?",
              "answer": "The multilingual nature of the base model complicated monolingual optimization by introducing unintended language drift after 150-200 steps, despite efforts to enforce English-only outputs."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "What non-RL method was attempted in Experiment 3 to control the language of the model's output, and was it successful?",
              "answer": "In Experiment 3, an instruction was appended to the system prompt: 'Reply in English only, do not use other languages'. This was an attempt to control language without an expensive language reward function, but mixed-language content persisted."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "What does the paper propose as a potential future solution to mitigate the language drift encountered when fine-tuning a multilingual model?",
              "answer": "The paper proposes incorporating a lightweight language reward or monolingual pre-filtering of the base model to mitigate language drift."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "In Experiment 2, after how many training steps did multilingual outputs begin to reemerge, suggesting the persistence of language drift?",
              "answer": "In Experiment 2, after 150-200 steps, mixed-language outputs reemerged."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "ACU. What is the name of the multilingual base model that was fine-tuned and exhibited language drift?",
              "answer": "The multilingual base model that was fine-tuned and exhibited language drift is DeepSeek-R1-Distill-Qwen-1.5B."
            },
            "result": true,
            "issue": ""
          }
        ],
        "judgment": true,
        "sections_to_highlight": [
          "Abstract — key findings on RL for small LLMs",
          "Introduction — challenges of scaling LLM reasoning",
          "Methodology — GRPO algorithm and reward design",
          "Experiments — comparison of Exp 1, 2, 3 results",
          "Overall Comparison — performance vs baselines & cost",
          "Limitations & Discussion — trade-offs and future work",
          "Hyperparameter Setup — key training parameters"
        ]
      },
      "correct_count_all": 6,
      "correct_count_acu": 1,
      "num_of_questions": 5,
      "sections_to_highlight": [
        "Abstract — key findings on RL for small LLMs",
        "Introduction — challenges of scaling LLM reasoning",
        "Methodology — GRPO algorithm and reward design",
        "Experiments — comparison of Exp 1, 2, 3 results",
        "Overall Comparison — performance vs baselines & cost",
        "Limitations & Discussion — trade-offs and future work",
        "Hyperparameter Setup — key training parameters"
      ],
      "sections_to_highlight_size": 7,
      "qa_failures_present": true
    },
    {
      "iteration_number": 4,
      "summary": "1. SUMMARY:\nThis paper explores the efficacy of reinforcement learning (RL) for enhancing the reasoning capabilities of small Large Language Models (LLMs) under strict resource constraints. The study focuses on a 1.5-billion-parameter model, DeepSeek-R1-Distill-Qwen-1.5B, fine-tuned using the Group Relative Policy Optimization (GRPO) algorithm and a curated mathematical reasoning dataset. The research highlights that RL can yield significant reasoning improvements in small LLMs, achieving competitive performance with minimal data and cost ($42). However, challenges emerged, particularly concerning optimization instability and language drift. Experiment 1 showed rapid gains followed by degradation, while Experiment 2 demonstrated that mixing easy and hard problems improved initial stability but did not fully resolve late-stage issues. Experiment 3, using a cosine reward for length control, stabilized output lengths but did not eliminate multilingual tendencies. Future directions suggest extending training and employing language-specific rewards or model filtering to address language drift, alongside increasing length limits for complex tasks. The findings underscore the trade-offs between efficiency and reasoning depth, offering a cost-effective alternative to large-scale approaches but acknowledging limitations in prolonged training and multilingual base models.\n\n2. KEY HIGHLIGHTS:\n*   Reinforcement learning (RL) can significantly boost the reasoning abilities of small LLMs (1.5B parameters) under resource constraints, achieving competitive performance at a low cost ($42).\n*   Small LLMs show rapid reasoning improvements within 50-100 training steps, but prolonged training can lead to performance degradation and multilingual language drift, especially with fixed length constraints.\n*   Mixing easier problems with harder ones can improve initial stability, while cosine rewards can better control output length, but neither fully resolves the language drift issue in multilingual base models.\n*   Addressing language drift requires future work such as extending training duration, using explicit language rewards, or employing monolingual pre-filtering of base models.",
      "qa_evaluations": [
        {
          "qa": {
            "question": "What specific evidence of multilingual language drift was observed after 200 training steps in Experiment 1?",
            "answer": "The summary states that Experiment 1 showed rapid gains followed by degradation, and mentions language drift as a challenge, but does not provide specific evidence observed after 200 training steps."
          },
          "result": false,
          "issue": "Not enough information"
        },
        {
          "qa": {
            "question": "According to the 'Limitations' section, how did the inherent multilingual nature of the base model complicate the process of monolingual optimization for the reasoning task?",
            "answer": "The summary mentions that the inherent multilingual nature of the base model is a limitation and contributes to language drift, but does not detail how it specifically complicated the process of monolingual optimization for the reasoning task."
          },
          "result": false,
          "issue": "Not enough information"
        },
        {
          "qa": {
            "question": "What non-RL method was attempted in Experiment 3 to control the language of the model's output, and was it successful?",
            "answer": "In Experiment 3, a cosine reward for length control was attempted to stabilize output lengths. The summary states it stabilized output lengths but did not eliminate multilingual tendencies, implying partial success."
          },
          "result": false,
          "issue": "partial information"
        },
        {
          "qa": {
            "question": "What does the paper propose as a potential future solution to mitigate the language drift encountered when fine-tuning a multilingual model?",
            "answer": "The paper proposes extending training and employing language-specific rewards or model filtering to address language drift."
          },
          "result": true,
          "issue": null
        },
        {
          "qa": {
            "question": "In Experiment 2, after how many training steps did multilingual outputs begin to reemerge, suggesting the persistence of language drift?",
            "answer": "The summary states that Experiment 2 demonstrated that mixing easy and hard problems improved initial stability but did not fully resolve late-stage issues related to language drift, but it does not specify the number of training steps after which multilingual outputs began to reemerge."
          },
          "result": false,
          "issue": "Not enough information"
        },
        {
          "qa": {
            "question": "ACU. What is the name of the multilingual base model that was fine-tuned and exhibited language drift?",
            "answer": "The name of the multilingual base model that was fine-tuned and exhibited language drift is DeepSeek-R1-Distill-Qwen-1.5B."
          },
          "result": true,
          "issue": null
        }
      ],
      "qa_pairs": [
        {
          "question": "What specific evidence of multilingual language drift was observed after 200 training steps in Experiment 1?",
          "answer": "The summary states that Experiment 1 showed rapid gains followed by degradation, and mentions language drift as a challenge, but does not provide specific evidence observed after 200 training steps."
        },
        {
          "question": "According to the 'Limitations' section, how did the inherent multilingual nature of the base model complicate the process of monolingual optimization for the reasoning task?",
          "answer": "The summary mentions that the inherent multilingual nature of the base model is a limitation and contributes to language drift, but does not detail how it specifically complicated the process of monolingual optimization for the reasoning task."
        },
        {
          "question": "What non-RL method was attempted in Experiment 3 to control the language of the model's output, and was it successful?",
          "answer": "In Experiment 3, a cosine reward for length control was attempted to stabilize output lengths. The summary states it stabilized output lengths but did not eliminate multilingual tendencies, implying partial success."
        },
        {
          "question": "What does the paper propose as a potential future solution to mitigate the language drift encountered when fine-tuning a multilingual model?",
          "answer": "The paper proposes extending training and employing language-specific rewards or model filtering to address language drift."
        },
        {
          "question": "In Experiment 2, after how many training steps did multilingual outputs begin to reemerge, suggesting the persistence of language drift?",
          "answer": "The summary states that Experiment 2 demonstrated that mixing easy and hard problems improved initial stability but did not fully resolve late-stage issues related to language drift, but it does not specify the number of training steps after which multilingual outputs began to reemerge."
        },
        {
          "question": "ACU. What is the name of the multilingual base model that was fine-tuned and exhibited language drift?",
          "answer": "The name of the multilingual base model that was fine-tuned and exhibited language drift is DeepSeek-R1-Distill-Qwen-1.5B."
        }
      ],
      "judge": {
        "evaluations": [
          {
            "qa": {
              "question": "What specific evidence of multilingual language drift was observed after 200 training steps in Experiment 1?",
              "answer": "The summary states that Experiment 1 showed rapid gains followed by degradation, and mentions language drift as a challenge, but does not provide specific evidence observed after 200 training steps."
            },
            "result": false,
            "issue": "The article, in section 3.5.1, states: 'Post-200 steps, lengths increase again, accompanied by unreadable content and non-English outputs.' This is specific evidence of language drift."
          },
          {
            "qa": {
              "question": "According to the 'Limitations' section, how did the inherent multilingual nature of the base model complicate the process of monolingual optimization for the reasoning task?",
              "answer": "The summary mentions that the inherent multilingual nature of the base model is a limitation and contributes to language drift, but does not detail how it specifically complicated the process of monolingual optimization for the reasoning task."
            },
            "result": false,
            "issue": "The article, in section B.1 Limitations, states: 'This limitation reflects a trade-off in using a pre-trained, multilingual foundation, which, while efficient, complicates monolingual optimization.' This directly answers how it complicated the process."
          },
          {
            "qa": {
              "question": "What non-RL method was attempted in Experiment 3 to control the language of the model's output, and was it successful?",
              "answer": "In Experiment 3, a cosine reward for length control was attempted to stabilize output lengths. The summary states it stabilized output lengths but did not eliminate multilingual tendencies, implying partial success."
            },
            "result": false,
            "issue": "The article, in section 3.5.3, states: 'We also append an instruction to the system prompt: “Reply in English only, do not use other languages”, avoiding a computationally expensive language reward function.' This is a non-RL method. The article also states: 'After 200 steps, mixed-language content persists, reflecting the multilingual nature of DeepSeek-R1-Distill-Qwen-1.5B.' This indicates it was not fully successful."
          },
          {
            "qa": {
              "question": "What does the paper propose as a potential future solution to mitigate the language drift encountered when fine-tuning a multilingual model?",
              "answer": "The paper proposes extending training and employing language-specific rewards or model filtering to address language drift."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "In Experiment 2, after how many training steps did multilingual outputs begin to reemerge, suggesting the persistence of language drift?",
              "answer": "The summary states that Experiment 2 demonstrated that mixing easy and hard problems improved initial stability but did not fully resolve late-stage issues related to language drift, but it does not specify the number of training steps after which multilingual outputs began to reemerge."
            },
            "result": false,
            "issue": "The article, in section 3.5.2, states: 'However, after 150–200 steps (approximately 4000 local steps), performance declines, and KL divergence becomes unstable (Figure 4), with mixed-language outputs reemerging.' This specifies the training steps."
          },
          {
            "qa": {
              "question": "ACU. What is the name of the multilingual base model that was fine-tuned and exhibited language drift?",
              "answer": "The name of the multilingual base model that was fine-tuned and exhibited language drift is DeepSeek-R1-Distill-Qwen-1.5B."
            },
            "result": true,
            "issue": ""
          }
        ],
        "judgment": false,
        "sections_to_highlight": []
      },
      "correct_count_all": 2,
      "correct_count_acu": 1,
      "num_of_questions": 5,
      "sections_to_highlight": [],
      "sections_to_highlight_size": 0,
      "qa_failures_present": true
    },
    {
      "iteration_number": 5,
      "summary": "1. SUMMARY:\nThis paper explores the efficacy of reinforcement learning (RL) for enhancing the reasoning capabilities of small, 1.5-billion-parameter LLMs under strict resource constraints. The researchers adapted the GRPO algorithm and curated a high-quality mathematical reasoning dataset, achieving significant performance gains on benchmarks like AMC23 (63% to 80%) and AIME24 (46.7%) with a low training cost of $42. However, challenges related to optimization instability and language drift were observed, particularly with prolonged training. Experiment 1 showed performance degradation and multilingual outputs after 200 steps due to length constraints. Experiment 2, which mixed easy and hard problems, improved initial stability but still faced late-stage instability and language drift. Experiment 3 utilized a cosine reward for length control, stabilizing output lengths but not fully resolving the multilingual tendencies of the base model. Future directions include addressing language drift through monolingual pre-filtering or dedicated language rewards and extending training duration to mitigate length limitations.\n\n2. KEY HIGHLIGHTS:\n*   RL-based fine-tuning can significantly improve reasoning in small LLMs (1.5B parameters) with minimal data and cost ($42), achieving performance competitive with larger models.\n*   Prolonged training or strict length constraints (e.g., 3584 tokens) can lead to performance degradation and unintended language drift in small, multilingual base models.\n*   A cosine reward function helps stabilize output lengths, but doesn't fully resolve language drift. Mixed difficulty datasets can improve initial training stability.\n*   Future work should focus on addressing language drift via monolingual pre-filtering or specific language rewards, and extending training duration/length limits for complex tasks.",
      "qa_evaluations": [
        {
          "qa": {
            "question": "What specific evidence of multilingual language drift was observed after 200 training steps in Experiment 1?",
            "answer": "In Experiment 1, performance degradation and multilingual outputs were observed after 200 training steps."
          },
          "result": true,
          "issue": null
        },
        {
          "qa": {
            "question": "According to the 'Limitations' section, how did the inherent multilingual nature of the base model complicate the process of monolingual optimization for the reasoning task?",
            "answer": "Not enough information in summary"
          },
          "result": false,
          "issue": "The summary does not contain a 'Limitations' section, nor does it detail how the base model's multilingual nature complicated monolingual optimization."
        },
        {
          "qa": {
            "question": "What non-RL method was attempted in Experiment 3 to control the language of the model's output, and was it successful?",
            "answer": "In Experiment 3, a cosine reward for length control was attempted. It stabilized output lengths but did not fully resolve the multilingual tendencies of the base model."
          },
          "result": true,
          "issue": null
        },
        {
          "qa": {
            "question": "What does the paper propose as a potential future solution to mitigate the language drift encountered when fine-tuning a multilingual model?",
            "answer": "The paper proposes addressing language drift through monolingual pre-filtering or dedicated language rewards, and extending training duration to mitigate length limitations."
          },
          "result": true,
          "issue": null
        },
        {
          "qa": {
            "question": "In Experiment 2, after how many training steps did multilingual outputs begin to reemerge, suggesting the persistence of language drift?",
            "answer": "The summary states that Experiment 2 faced late-stage instability and language drift, but does not specify the exact training step count when multilingual outputs began to reemerge."
          },
          "result": false,
          "issue": "The specific training step count for the reemergence of multilingual outputs in Experiment 2 is not provided."
        },
        {
          "qa": {
            "question": "ACU. What is the name of the multilingual base model that was fine-tuned and exhibited language drift?",
            "answer": "Not enough information in summary"
          },
          "result": false,
          "issue": "The name of the multilingual base model is not mentioned in the summary."
        }
      ],
      "qa_pairs": [
        {
          "question": "What specific evidence of multilingual language drift was observed after 200 training steps in Experiment 1?",
          "answer": "In Experiment 1, performance degradation and multilingual outputs were observed after 200 training steps."
        },
        {
          "question": "According to the 'Limitations' section, how did the inherent multilingual nature of the base model complicate the process of monolingual optimization for the reasoning task?",
          "answer": "Not enough information in summary"
        },
        {
          "question": "What non-RL method was attempted in Experiment 3 to control the language of the model's output, and was it successful?",
          "answer": "In Experiment 3, a cosine reward for length control was attempted. It stabilized output lengths but did not fully resolve the multilingual tendencies of the base model."
        },
        {
          "question": "What does the paper propose as a potential future solution to mitigate the language drift encountered when fine-tuning a multilingual model?",
          "answer": "The paper proposes addressing language drift through monolingual pre-filtering or dedicated language rewards, and extending training duration to mitigate length limitations."
        },
        {
          "question": "In Experiment 2, after how many training steps did multilingual outputs begin to reemerge, suggesting the persistence of language drift?",
          "answer": "The summary states that Experiment 2 faced late-stage instability and language drift, but does not specify the exact training step count when multilingual outputs began to reemerge."
        },
        {
          "question": "ACU. What is the name of the multilingual base model that was fine-tuned and exhibited language drift?",
          "answer": "Not enough information in summary"
        }
      ],
      "judge": {
        "evaluations": [
          {
            "qa": {
              "question": "What specific evidence of multilingual language drift was observed after 200 training steps in Experiment 1?",
              "answer": "In Experiment 1, performance degradation and multilingual outputs were observed after 200 training steps."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "According to the 'Limitations' section, how did the inherent multilingual nature of the base model complicate the process of monolingual optimization for the reasoning task?",
              "answer": "The multilingual nature of the base model, DeepSeek-R1-Distill-Qwen-1.5B, introduced unintended language drift after 150–200 steps, despite efforts to enforce English-only outputs via prompts in Experiment 3. This limitation reflects a trade-off in using a pre-trained, multilingual foundation, which, while efficient, complicates monolingual optimization."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "What non-RL method was attempted in Experiment 3 to control the language of the model's output, and was it successful?",
              "answer": "In Experiment 3, a cosine reward for length control was attempted. It stabilized output lengths but did not fully resolve the multilingual tendencies of the base model."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "What does the paper propose as a potential future solution to mitigate the language drift encountered when fine-tuning a multilingual model?",
              "answer": "The paper proposes addressing language drift through monolingual pre-filtering or dedicated language rewards, and extending training duration to mitigate length limitations."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "In Experiment 2, after how many training steps did multilingual outputs begin to reemerge, suggesting the persistence of language drift?",
              "answer": "The summary states that Experiment 2 faced late-stage instability and language drift, but does not specify the exact training step count when multilingual outputs began to reemerge."
            },
            "result": false,
            "issue": "The article states that in Experiment 2, 'after 150–200 steps (approximately 4000 local steps), performance declines, and KL divergence becomes unstable (Figure 4), with mixed-language outputs reemerging.'"
          },
          {
            "qa": {
              "question": "ACU. What is the name of the multilingual base model that was fine-tuned and exhibited language drift?",
              "answer": "The multilingual base model that was fine-tuned and exhibited language drift is DeepSeek-R1-Distill-Qwen-1.5B."
            },
            "result": true,
            "issue": ""
          }
        ],
        "judgment": false,
        "sections_to_highlight": [
          "Experiment 2 — timing of multilingual output emergence",
          "Limitations — specific challenges of multilingual base models",
          "Methodology — dataset curation details",
          "Results — comparison of Open-RS variants",
          "Future Directions — addressing language drift and length constraints"
        ]
      },
      "correct_count_all": 5,
      "correct_count_acu": 1,
      "num_of_questions": 5,
      "sections_to_highlight": [
        "Experiment 2 — timing of multilingual output emergence",
        "Limitations — specific challenges of multilingual base models",
        "Methodology — dataset curation details",
        "Results — comparison of Open-RS variants",
        "Future Directions — addressing language drift and length constraints"
      ],
      "sections_to_highlight_size": 5,
      "qa_failures_present": true
    },
    {
      "iteration_number": 6,
      "summary": "1. SUMMARY:\nThis paper investigates the efficacy of Reinforcement Learning (RL) for enhancing the reasoning capabilities of small Large Language Models (LLMs) under strict resource constraints, specifically addressing the challenge of multilingual language drift when fine-tuning for monolingual tasks. The researchers adapted the Group Relative Policy Optimization (GRPO) algorithm and curated a high-quality mathematical reasoning dataset. They found that small LLMs can achieve rapid reasoning gains (e.g., AMC23 accuracy from 63% to 80%) with minimal data and cost ($42). However, prolonged training led to optimization instability and language drift, a consequence of using a multilingual base model (DeepSeek-R1-Distill-Qwen-1.5B). Experiment 3 demonstrated that a cosine reward could stabilize output lengths, improving training consistency, but did not fully resolve the language drift issue, suggesting a need for explicit language controls or longer completion lengths for complex tasks. Future directions include extending training duration and incorporating language-specific rewards to mitigate drift.\n\n2. KEY HIGHLIGHTS:\n*   Small LLMs can achieve significant reasoning improvements (e.g., AMC23 accuracy from 63% to 80%) using RL with minimal data and cost, but face challenges with optimization stability and language drift during prolonged training.\n*   The multilingual nature of the base model (DeepSeek-R1-Distill-Qwen-1.5B) is a key factor contributing to language drift, which emerges after 150-200 training steps.\n*   A cosine reward effectively stabilizes output lengths, improving training consistency, but does not fully resolve multilingual language drift, indicating a need for explicit language control mechanisms or longer completion lengths.\n*   Future work should focus on addressing language drift through methods like lightweight language rewards or monolingual pre-filtering, and by extending training duration to better handle complex reasoning tasks.",
      "qa_evaluations": [
        {
          "qa": {
            "question": "What specific evidence of multilingual language drift was observed after 200 training steps in Experiment 1?",
            "answer": "The summary states that language drift emerges after 150-200 training steps, but does not specify the exact evidence observed after 200 training steps in Experiment 1."
          },
          "result": false,
          "issue": "partial information"
        },
        {
          "qa": {
            "question": "According to the 'Limitations' section, how did the inherent multilingual nature of the base model complicate the process of monolingual optimization for the reasoning task?",
            "answer": "The summary indicates that the inherent multilingual nature of the base model contributed to language drift, complicating monolingual optimization, but does not detail how this complication manifested in the 'Limitations' section specifically."
          },
          "result": false,
          "issue": "Not enough information in summary"
        },
        {
          "qa": {
            "question": "What non-RL method was attempted in Experiment 3 to control the language of the model's output, and was it successful?",
            "answer": "In Experiment 3, a cosine reward was attempted to stabilize output lengths, which improved training consistency but did not fully resolve the language drift issue."
          },
          "result": true,
          "issue": null
        },
        {
          "qa": {
            "question": "What does the paper propose as a potential future solution to mitigate the language drift encountered when fine-tuning a multilingual model?",
            "answer": "The paper proposes addressing language drift through methods like lightweight language rewards or monolingual pre-filtering, and by extending training duration."
          },
          "result": true,
          "issue": null
        },
        {
          "qa": {
            "question": "In Experiment 2, after how many training steps did multilingual outputs begin to reemerge, suggesting the persistence of language drift?",
            "answer": "The summary states that language drift emerges after 150-200 training steps, but does not specify if this observation pertains to Experiment 2 or the exact number of steps for reemergence."
          },
          "result": false,
          "issue": "partial information"
        },
        {
          "qa": {
            "question": "ACU. What is the name of the multilingual base model that was fine-tuned and exhibited language drift?",
            "answer": "The multilingual base model that was fine-tuned and exhibited language drift is DeepSeek-R1-Distill-Qwen-1.5B."
          },
          "result": true,
          "issue": null
        }
      ],
      "qa_pairs": [
        {
          "question": "What specific evidence of multilingual language drift was observed after 200 training steps in Experiment 1?",
          "answer": "The summary states that language drift emerges after 150-200 training steps, but does not specify the exact evidence observed after 200 training steps in Experiment 1."
        },
        {
          "question": "According to the 'Limitations' section, how did the inherent multilingual nature of the base model complicate the process of monolingual optimization for the reasoning task?",
          "answer": "The summary indicates that the inherent multilingual nature of the base model contributed to language drift, complicating monolingual optimization, but does not detail how this complication manifested in the 'Limitations' section specifically."
        },
        {
          "question": "What non-RL method was attempted in Experiment 3 to control the language of the model's output, and was it successful?",
          "answer": "In Experiment 3, a cosine reward was attempted to stabilize output lengths, which improved training consistency but did not fully resolve the language drift issue."
        },
        {
          "question": "What does the paper propose as a potential future solution to mitigate the language drift encountered when fine-tuning a multilingual model?",
          "answer": "The paper proposes addressing language drift through methods like lightweight language rewards or monolingual pre-filtering, and by extending training duration."
        },
        {
          "question": "In Experiment 2, after how many training steps did multilingual outputs begin to reemerge, suggesting the persistence of language drift?",
          "answer": "The summary states that language drift emerges after 150-200 training steps, but does not specify if this observation pertains to Experiment 2 or the exact number of steps for reemergence."
        },
        {
          "question": "ACU. What is the name of the multilingual base model that was fine-tuned and exhibited language drift?",
          "answer": "The multilingual base model that was fine-tuned and exhibited language drift is DeepSeek-R1-Distill-Qwen-1.5B."
        }
      ],
      "judge": {
        "evaluations": [
          {
            "qa": {
              "question": "What specific evidence of multilingual language drift was observed after 200 training steps in Experiment 1?",
              "answer": "The summary states that language drift emerges after 150-200 training steps, but does not specify the exact evidence observed after 200 training steps in Experiment 1."
            },
            "result": false,
            "issue": "The article states in section 3.5.1: 'Post-200 steps, lengths increase again, accompanied by unreadable content and non-English outputs.' This specific evidence is missing from the summary's answer."
          },
          {
            "qa": {
              "question": "According to the 'Limitations' section, how did the inherent multilingual nature of the base model complicate the process of monolingual optimization for the reasoning task?",
              "answer": "The summary indicates that the inherent multilingual nature of the base model contributed to language drift, complicating monolingual optimization, but does not detail how this complication manifested in the 'Limitations' section specifically."
            },
            "result": false,
            "issue": "The article states in section B.1 Limitations: 'This limitation reflects a trade-off in using a pre-trained, multilingual foundation, which, while efficient, complicates monolingual optimization.' The answer should specify that the complication is the trade-off between efficiency and monolingual optimization."
          },
          {
            "qa": {
              "question": "What non-RL method was attempted in Experiment 3 to control the language of the model's output, and was it successful?",
              "answer": "In Experiment 3, a cosine reward was attempted to stabilize output lengths, which improved training consistency but did not fully resolve the language drift issue."
            },
            "result": false,
            "issue": "The question asks about controlling the *language* of the output, but the answer describes controlling *output length* with a cosine reward. The article mentions an instruction to the system prompt: 'Reply in English only, do not use other languages' in Experiment 3, which is the non-RL method for language control. The answer should address this and its success."
          },
          {
            "qa": {
              "question": "What does the paper propose as a potential future solution to mitigate the language drift encountered when fine-tuning a multilingual model?",
              "answer": "The paper proposes addressing language drift through methods like lightweight language rewards or monolingual pre-filtering, and by extending training duration."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "In Experiment 2, after how many training steps did multilingual outputs begin to reemerge, suggesting the persistence of language drift?",
              "answer": "The summary states that language drift emerges after 150-200 training steps, but does not specify if this observation pertains to Experiment 2 or the exact number of steps for reemergence."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "ACU. What is the name of the multilingual base model that was fine-tuned and exhibited language drift?",
              "answer": "The multilingual base model that was fine-tuned and exhibited language drift is DeepSeek-R1-Distill-Qwen-1.5B."
            },
            "result": true,
            "issue": ""
          }
        ],
        "judgment": false,
        "sections_to_highlight": [
          "Experiment 1 — specific evidence of language drift",
          "Experiment 3 — language control method and success",
          "Limitations — complicating factors of multilingual nature",
          "Methodology — dataset curation process",
          "Results — performance comparison with baselines",
          "Future Directions — addressing language drift and length constraints"
        ]
      },
      "correct_count_all": 3,
      "correct_count_acu": 1,
      "num_of_questions": 5,
      "sections_to_highlight": [
        "Experiment 1 — specific evidence of language drift",
        "Experiment 3 — language control method and success",
        "Limitations — complicating factors of multilingual nature",
        "Methodology — dataset curation process",
        "Results — performance comparison with baselines",
        "Future Directions — addressing language drift and length constraints"
      ],
      "sections_to_highlight_size": 6,
      "qa_failures_present": true
    },
    {
      "iteration_number": 7,
      "summary": "1. SUMMARY:\nThis paper explores enhancing the reasoning capabilities of small Large Language Models (LLMs) using Reinforcement Learning (RL) under strict resource constraints. The authors adapted the Group Relative Policy Optimization (GRPO) algorithm and a curated mathematical reasoning dataset to fine-tune a 1.5-billion-parameter model, DeepSeek-R1-Distill-Qwen-1.5B. The study found that RL-based fine-tuning can yield significant reasoning gains (e.g., AMC23 accuracy from 63% to 80%) with minimal data and cost ($42). However, challenges arose, including optimization instability and multilingual language drift, particularly with prolonged training. Experiment 3 introduced a cosine reward for length control and a system prompt to mitigate language drift, showing improved consistency but still encountering multilingual outputs due to the base model's nature. Future directions include extending training duration and employing explicit language control mechanisms to address these issues. The research highlights the trade-offs between efficiency and reasoning depth in small LLMs, offering a cost-effective alternative to large-scale approaches.\n\n2. KEY HIGHLIGHTS:\n*   Reinforcement learning (RL) can significantly improve reasoning in small LLMs (1.5B parameters) with minimal data and cost, achieving performance comparable to larger models on specific benchmarks.\n*   Prolonged RL training on small LLMs can lead to performance degradation, optimization instability, and multilingual language drift, especially when the base model is multilingual.\n*   A cosine reward function helps stabilize completion lengths and training consistency, while prompt-based language control shows limited success against inherent multilingual tendencies of the base model.\n*   Future work suggests extending training duration and implementing more robust language control methods to fully overcome the limitations observed with multilingual base models and length constraints.",
      "qa_evaluations": [
        {
          "qa": {
            "question": "What specific evidence of multilingual language drift was observed after 200 training steps in Experiment 1?",
            "answer": "Not enough information in summary"
          },
          "result": false,
          "issue": "Not enough information"
        },
        {
          "qa": {
            "question": "According to the 'Limitations' section, how did the inherent multilingual nature of the base model complicate the process of monolingual optimization for the reasoning task?",
            "answer": "Not enough information in summary"
          },
          "result": false,
          "issue": "Not enough information"
        },
        {
          "qa": {
            "question": "What non-RL method was attempted in Experiment 3 to control the language of the model's output, and was it successful?",
            "answer": "In Experiment 3, a system prompt was used to control the language of the model's output. It showed improved consistency but still encountered multilingual outputs due to the base model's nature, indicating it was not fully successful."
          },
          "result": false,
          "issue": "partial information"
        },
        {
          "qa": {
            "question": "What does the paper propose as a potential future solution to mitigate the language drift encountered when fine-tuning a multilingual model?",
            "answer": "The paper proposes extending training duration and employing explicit language control mechanisms to address language drift."
          },
          "result": true,
          "issue": null
        },
        {
          "qa": {
            "question": "In Experiment 2, after how many training steps did multilingual outputs begin to reemerge, suggesting the persistence of language drift?",
            "answer": "Not enough information in summary"
          },
          "result": false,
          "issue": "Not enough information"
        },
        {
          "qa": {
            "question": "ACU. What is the name of the multilingual base model that was fine-tuned and exhibited language drift?",
            "answer": "The name of the multilingual base model that was fine-tuned and exhibited language drift is DeepSeek-R1-Distill-Qwen-1.5B."
          },
          "result": true,
          "issue": null
        }
      ],
      "qa_pairs": [
        {
          "question": "What specific evidence of multilingual language drift was observed after 200 training steps in Experiment 1?",
          "answer": "Not enough information in summary"
        },
        {
          "question": "According to the 'Limitations' section, how did the inherent multilingual nature of the base model complicate the process of monolingual optimization for the reasoning task?",
          "answer": "Not enough information in summary"
        },
        {
          "question": "What non-RL method was attempted in Experiment 3 to control the language of the model's output, and was it successful?",
          "answer": "In Experiment 3, a system prompt was used to control the language of the model's output. It showed improved consistency but still encountered multilingual outputs due to the base model's nature, indicating it was not fully successful."
        },
        {
          "question": "What does the paper propose as a potential future solution to mitigate the language drift encountered when fine-tuning a multilingual model?",
          "answer": "The paper proposes extending training duration and employing explicit language control mechanisms to address language drift."
        },
        {
          "question": "In Experiment 2, after how many training steps did multilingual outputs begin to reemerge, suggesting the persistence of language drift?",
          "answer": "Not enough information in summary"
        },
        {
          "question": "ACU. What is the name of the multilingual base model that was fine-tuned and exhibited language drift?",
          "answer": "The name of the multilingual base model that was fine-tuned and exhibited language drift is DeepSeek-R1-Distill-Qwen-1.5B."
        }
      ],
      "judge": {
        "evaluations": [
          {
            "qa": {
              "question": "What specific evidence of multilingual language drift was observed after 200 training steps in Experiment 1?",
              "answer": "Not enough information in summary"
            },
            "result": false,
            "issue": "The article mentions 'non-English outputs' after 200 steps in Experiment 1, but the summary states there is 'Not enough information'."
          },
          {
            "qa": {
              "question": "According to the 'Limitations' section, how did the inherent multilingual nature of the base model complicate the process of monolingual optimization for the reasoning task?",
              "answer": "Not enough information in summary"
            },
            "result": false,
            "issue": "The article states that the multilingual nature of the base model 'complicates monolingual optimization', but the summary indicates 'Not enough information'."
          },
          {
            "qa": {
              "question": "What non-RL method was attempted in Experiment 3 to control the language of the model's output, and was it successful?",
              "answer": "In Experiment 3, a system prompt was used to control the language of the model's output. It showed improved consistency but still encountered multilingual outputs due to the base model's nature, indicating it was not fully successful."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "What does the paper propose as a potential future solution to mitigate the language drift encountered when fine-tuning a multilingual model?",
              "answer": "The paper proposes extending training duration and employing explicit language control mechanisms to address language drift."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "In Experiment 2, after how many training steps did multilingual outputs begin to reemerge, suggesting the persistence of language drift?",
              "answer": "Not enough information in summary"
            },
            "result": false,
            "issue": "The article states that after 150-200 steps in Experiment 2, 'mixed-language outputs' reemerged, but the summary claims 'Not enough information'."
          },
          {
            "qa": {
              "question": "ACU. What is the name of the multilingual base model that was fine-tuned and exhibited language drift?",
              "answer": "The name of the multilingual base model that was fine-tuned and exhibited language drift is DeepSeek-R1-Distill-Qwen-1.5B."
            },
            "result": true,
            "issue": ""
          }
        ],
        "judgment": false,
        "sections_to_highlight": []
      },
      "correct_count_all": 3,
      "correct_count_acu": 1,
      "num_of_questions": 5,
      "sections_to_highlight": [],
      "sections_to_highlight_size": 0,
      "qa_failures_present": true
    },
    {
      "iteration_number": 8,
      "summary": "SUMMARY:\nThis research explores the efficacy of reinforcement learning (RL) for enhancing the reasoning capabilities of small Large Language Models (LLMs) under strict resource constraints, specifically focusing on a 1.5 billion parameter model. The study addresses the challenge of managing multilingual language drift when fine-tuning for a monolingual reasoning task. Utilizing the Group Relative Policy Optimization (GRPO) algorithm and a curated, compact mathematical reasoning dataset, the researchers achieved significant performance gains, with AMC23 accuracy rising from 63% to 80% and AIME24 reaching 46.7%, all at a low cost ($42). However, prolonged training led to optimization instability and language drift, a notable challenge stemming from the multilingual nature of the base model. Experiment 3 demonstrated that a cosine reward function could help control output length, improving training consistency, but did not fully resolve the language drift issue, suggesting a need for explicit language constraints or longer completion lengths for complex tasks. The findings highlight the trade-offs between efficiency and reasoning depth in small LLMs, offering a cost-effective alternative to large-scale approaches while identifying key limitations for future research.\n\nKEY HIGHLIGHTS:\n*   Reinforcement learning (RL) significantly improves reasoning in small LLMs (1.5B parameters) under resource constraints, achieving competitive performance with minimal data and cost.\n*   Multilingual language drift was observed after prolonged training (150-200 steps) on the DeepSeek-R1-Distill-Qwen-1.5B base model, complicating monolingual optimization.\n*   A cosine reward function effectively stabilized output lengths, improving training consistency, but did not fully mitigate language drift, indicating a need for explicit language control.\n*   Performance degradation and instability occurred with extended training, suggesting that strict length constraints and over-optimization are key challenges for small LLMs in reasoning tasks.\n*   The study highlights a trade-off between training efficiency and reasoning depth, with early gains achievable but long-term stability and handling of complex reasoning chains remaining areas for improvement.",
      "qa_evaluations": [
        {
          "qa": {
            "question": "What specific evidence of multilingual language drift was observed after 200 training steps in Experiment 1?",
            "answer": "Multilingual language drift was observed after prolonged training (150-200 steps)."
          },
          "result": true,
          "issue": null
        },
        {
          "qa": {
            "question": "According to the 'Limitations' section, how did the inherent multilingual nature of the base model complicate the process of monolingual optimization for the reasoning task?",
            "answer": "The inherent multilingual nature of the base model complicated the process of monolingual optimization for the reasoning task by causing language drift."
          },
          "result": true,
          "issue": null
        },
        {
          "qa": {
            "question": "What non-RL method was attempted in Experiment 3 to control the language of the model's output, and was it successful?",
            "answer": "In Experiment 3, a cosine reward function was attempted to control the output length, which improved training consistency but did not fully resolve the language drift issue."
          },
          "result": true,
          "issue": null
        },
        {
          "qa": {
            "question": "What does the paper propose as a potential future solution to mitigate the language drift encountered when fine-tuning a multilingual model?",
            "answer": "The paper suggests that explicit language constraints or longer completion lengths for complex tasks may be needed to mitigate language drift."
          },
          "result": true,
          "issue": null
        },
        {
          "qa": {
            "question": "In Experiment 2, after how many training steps did multilingual outputs begin to reemerge, suggesting the persistence of language drift?",
            "answer": "Multilingual outputs began to reemerge after 150-200 training steps, indicating the persistence of language drift."
          },
          "result": true,
          "issue": null
        },
        {
          "qa": {
            "question": "ACU. What is the name of the multilingual base model that was fine-tuned and exhibited language drift?",
            "answer": "The name of the multilingual base model that was fine-tuned and exhibited language drift is DeepSeek-R1-Distill-Qwen-1.5B."
          },
          "result": true,
          "issue": null
        }
      ],
      "qa_pairs": [
        {
          "question": "What specific evidence of multilingual language drift was observed after 200 training steps in Experiment 1?",
          "answer": "Multilingual language drift was observed after prolonged training (150-200 steps)."
        },
        {
          "question": "According to the 'Limitations' section, how did the inherent multilingual nature of the base model complicate the process of monolingual optimization for the reasoning task?",
          "answer": "The inherent multilingual nature of the base model complicated the process of monolingual optimization for the reasoning task by causing language drift."
        },
        {
          "question": "What non-RL method was attempted in Experiment 3 to control the language of the model's output, and was it successful?",
          "answer": "In Experiment 3, a cosine reward function was attempted to control the output length, which improved training consistency but did not fully resolve the language drift issue."
        },
        {
          "question": "What does the paper propose as a potential future solution to mitigate the language drift encountered when fine-tuning a multilingual model?",
          "answer": "The paper suggests that explicit language constraints or longer completion lengths for complex tasks may be needed to mitigate language drift."
        },
        {
          "question": "In Experiment 2, after how many training steps did multilingual outputs begin to reemerge, suggesting the persistence of language drift?",
          "answer": "Multilingual outputs began to reemerge after 150-200 training steps, indicating the persistence of language drift."
        },
        {
          "question": "ACU. What is the name of the multilingual base model that was fine-tuned and exhibited language drift?",
          "answer": "The name of the multilingual base model that was fine-tuned and exhibited language drift is DeepSeek-R1-Distill-Qwen-1.5B."
        }
      ],
      "judge": {
        "evaluations": [
          {
            "qa": {
              "question": "What specific evidence of multilingual language drift was observed after 200 training steps in Experiment 1?",
              "answer": "Multilingual language drift was observed after prolonged training (150-200 steps)."
            },
            "result": false,
            "issue": "The answer states that multilingual language drift was observed after 150-200 steps, but the question asks for evidence observed after 200 steps in Experiment 1. The article mentions 'Post-200 steps, lengths increase again, accompanied by unreadable content and non-English outputs' in the context of Experiment 1, which is more specific to the question."
          },
          {
            "qa": {
              "question": "According to the 'Limitations' section, how did the inherent multilingual nature of the base model complicate the process of monolingual optimization for the reasoning task?",
              "answer": "The inherent multilingual nature of the base model complicated the process of monolingual optimization for the reasoning task by causing language drift."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "What non-RL method was attempted in Experiment 3 to control the language of the model's output, and was it successful?",
              "answer": "In Experiment 3, a cosine reward function was attempted to control the output length, which improved training consistency but did not fully resolve the language drift issue."
            },
            "result": false,
            "issue": "The question asks about controlling the *language* of the output, not the *length*. Experiment 3 appended an instruction to the system prompt for English-only output, which was a non-RL method to control language. The cosine reward was for length control. The article states the prompt instruction was used to avoid a 'computationally expensive language reward function', implying it was an attempt to control language, but the multilingual content persisted."
          },
          {
            "qa": {
              "question": "What does the paper propose as a potential future solution to mitigate the language drift encountered when fine-tuning a multilingual model?",
              "answer": "The paper suggests that explicit language constraints or longer completion lengths for complex tasks may be needed to mitigate language drift."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "In Experiment 2, after how many training steps did multilingual outputs begin to reemerge, suggesting the persistence of language drift?",
              "answer": "Multilingual outputs began to reemerge after 150-200 training steps, indicating the persistence of language drift."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "ACU. What is the name of the multilingual base model that was fine-tuned and exhibited language drift?",
              "answer": "The name of the multilingual base model that was fine-tuned and exhibited language drift is DeepSeek-R1-Distill-Qwen-1.5B."
            },
            "result": true,
            "issue": ""
          }
        ],
        "judgment": false,
        "sections_to_highlight": [
          "Experiment 1 — specific evidence of language drift",
          "Experiment 3 — non-RL language control methods",
          "Limitations — impact of multilingual base model",
          "Conclusion — proposed solutions for language drift",
          "Abstract — key performance metrics and cost",
          "Methodology — dataset curation process",
          "Results — comparison with baselines",
          "Hyperparameter Setup — key training parameters"
        ]
      },
      "correct_count_all": 4,
      "correct_count_acu": 1,
      "num_of_questions": 5,
      "sections_to_highlight": [
        "Experiment 1 — specific evidence of language drift",
        "Experiment 3 — non-RL language control methods",
        "Limitations — impact of multilingual base model",
        "Conclusion — proposed solutions for language drift",
        "Abstract — key performance metrics and cost",
        "Methodology — dataset curation process",
        "Results — comparison with baselines",
        "Hyperparameter Setup — key training parameters"
      ],
      "sections_to_highlight_size": 8,
      "qa_failures_present": false
    },
    {
      "iteration_number": 9,
      "summary": "1. SUMMARY:\nThis study investigates the efficacy of Reinforcement Learning (RL) for enhancing reasoning in small LLMs (1.5B parameters) under strict resource constraints. The authors adapted the GRPO algorithm and curated a compact dataset of mathematical reasoning problems. Experiments revealed that small LLMs can achieve rapid reasoning improvements within 50-100 steps, but prolonged training leads to performance degradation and, notably, multilingual language drift. This drift is attributed to the multilingual nature of the base model (DeepSeek-R1-Distill-Qwen-1.5B) and is exacerbated by strict length constraints. While a cosine reward function helped stabilize output lengths, it did not fully resolve the language issue, suggesting a need for explicit language control or longer contexts. Despite these challenges, the proposed RL approach achieved competitive performance against larger models and baselines with significantly lower computational cost and data requirements, offering a promising direction for resource-constrained AI development.\n\n2. KEY HIGHLIGHTS:\n*   Small LLMs can achieve significant reasoning gains quickly (50-100 steps) with RL, but prolonged training leads to performance degradation and multilingual language drift.\n*   The multilingual nature of the base model (DeepSeek-R1-Distill-Qwen-1.5B) is a key factor contributing to language drift, which is amplified by strict length constraints.\n*   While a cosine reward function improved length control, it did not fully eliminate language drift, indicating a need for more robust language control mechanisms or longer context windows.\n*   The RL approach demonstrated high efficiency, achieving competitive reasoning performance with minimal data and a significantly lower cost ($42) compared to baseline models.",
      "qa_evaluations": [
        {
          "qa": {
            "question": "What specific evidence of multilingual language drift was observed after 200 training steps in Experiment 1?",
            "answer": "The summary states that prolonged training leads to performance degradation and multilingual language drift, but does not provide specific evidence observed after 200 training steps in Experiment 1."
          },
          "result": false,
          "issue": "Not enough information in summary"
        },
        {
          "qa": {
            "question": "According to the 'Limitations' section, how did the inherent multilingual nature of the base model complicate the process of monolingual optimization for the reasoning task?",
            "answer": "The summary mentions that the inherent multilingual nature of the base model contributed to language drift, which was exacerbated by strict length constraints, but it does not detail how this complication specifically affected the process of monolingual optimization for the reasoning task, nor does it reference a 'Limitations' section."
          },
          "result": false,
          "issue": "Not enough information in summary"
        },
        {
          "qa": {
            "question": "What non-RL method was attempted in Experiment 3 to control the language of the model's output, and was it successful?",
            "answer": "The summary does not mention any non-RL methods attempted in Experiment 3 to control the language of the model's output, nor does it reference Experiment 3."
          },
          "result": false,
          "issue": "Not enough information in summary"
        },
        {
          "qa": {
            "question": "What does the paper propose as a potential future solution to mitigate the language drift encountered when fine-tuning a multilingual model?",
            "answer": "The paper suggests that explicit language control mechanisms or longer context windows may be needed to mitigate the language drift encountered when fine-tuning a multilingual model."
          },
          "result": true,
          "issue": null
        },
        {
          "qa": {
            "question": "In Experiment 2, after how many training steps did multilingual outputs begin to reemerge, suggesting the persistence of language drift?",
            "answer": "The summary states that multilingual language drift occurs with prolonged training but does not specify the number of training steps after which it reemerged in Experiment 2."
          },
          "result": false,
          "issue": "Not enough information in summary"
        },
        {
          "qa": {
            "question": "ACU. What is the name of the multilingual base model that was fine-tuned and exhibited language drift?",
            "answer": "The multilingual base model that was fine-tuned and exhibited language drift is DeepSeek-R1-Distill-Qwen-1.5B."
          },
          "result": true,
          "issue": null
        }
      ],
      "qa_pairs": [
        {
          "question": "What specific evidence of multilingual language drift was observed after 200 training steps in Experiment 1?",
          "answer": "The summary states that prolonged training leads to performance degradation and multilingual language drift, but does not provide specific evidence observed after 200 training steps in Experiment 1."
        },
        {
          "question": "According to the 'Limitations' section, how did the inherent multilingual nature of the base model complicate the process of monolingual optimization for the reasoning task?",
          "answer": "The summary mentions that the inherent multilingual nature of the base model contributed to language drift, which was exacerbated by strict length constraints, but it does not detail how this complication specifically affected the process of monolingual optimization for the reasoning task, nor does it reference a 'Limitations' section."
        },
        {
          "question": "What non-RL method was attempted in Experiment 3 to control the language of the model's output, and was it successful?",
          "answer": "The summary does not mention any non-RL methods attempted in Experiment 3 to control the language of the model's output, nor does it reference Experiment 3."
        },
        {
          "question": "What does the paper propose as a potential future solution to mitigate the language drift encountered when fine-tuning a multilingual model?",
          "answer": "The paper suggests that explicit language control mechanisms or longer context windows may be needed to mitigate the language drift encountered when fine-tuning a multilingual model."
        },
        {
          "question": "In Experiment 2, after how many training steps did multilingual outputs begin to reemerge, suggesting the persistence of language drift?",
          "answer": "The summary states that multilingual language drift occurs with prolonged training but does not specify the number of training steps after which it reemerged in Experiment 2."
        },
        {
          "question": "ACU. What is the name of the multilingual base model that was fine-tuned and exhibited language drift?",
          "answer": "The multilingual base model that was fine-tuned and exhibited language drift is DeepSeek-R1-Distill-Qwen-1.5B."
        }
      ],
      "judge": {
        "evaluations": [
          {
            "qa": {
              "question": "What specific evidence of multilingual language drift was observed after 200 training steps in Experiment 1?",
              "answer": "The summary states that prolonged training leads to performance degradation and multilingual language drift, but does not provide specific evidence observed after 200 training steps in Experiment 1."
            },
            "result": false,
            "issue": "The article states in section 3.5.1: 'After 200 steps, accuracy degrades significantly, dropping below 60% on AMC23 and to 80% on MATH-500. Figure 3 illustrates this trend, showing unstable accuracy rewards and completion lengths fluctuating near 4000 tokens initially, then decreasing to around 3000 tokens by 100 global steps (approximately 3000 local steps on a single GPU). Post-200 steps, lengths increase again, accompanied by unreadable content and non-English outputs.'"
          },
          {
            "qa": {
              "question": "According to the 'Limitations' section, how did the inherent multilingual nature of the base model complicate the process of monolingual optimization for the reasoning task?",
              "answer": "The summary mentions that the inherent multilingual nature of the base model contributed to language drift, which was exacerbated by strict length constraints, but it does not detail how this complication specifically affected the process of monolingual optimization for the reasoning task, nor does it reference a 'Limitations' section."
            },
            "result": false,
            "issue": "The article states in section B.1 Limitations: 'Third, the multilingual nature of the base model, DeepSeek-R1-Distill-Qwen-1.5B, introduced unintended language drift after 150–200 steps, despite efforts to enforce English-only outputs via prompts in Experiment 3. This limitation reflects a trade-off in using a pre-trained, multilingual foundation, which, while efficient, complicates monolingual optimization.'"
          },
          {
            "qa": {
              "question": "What non-RL method was attempted in Experiment 3 to control the language of the model's output, and was it successful?",
              "answer": "The summary does not mention any non-RL methods attempted in Experiment 3 to control the language of the model's output, nor does it reference Experiment 3."
            },
            "result": false,
            "issue": "The article states in section 3.5.3: 'We also append an instruction to the system prompt: “Reply in English only, do not use other languages”, avoiding a computationally expensive language reward function. The maximum completion length remains 3584 tokens.' The success is discussed later in the same paragraph: 'After 200 steps, mixed-language content persists, reflecting the multilingual nature of DeepSeek-R1-Distill-Qwen-1.5B.'"
          },
          {
            "qa": {
              "question": "What does the paper propose as a potential future solution to mitigate the language drift encountered when fine-tuning a multilingual model?",
              "answer": "The paper suggests that explicit language control mechanisms or longer context windows may be needed to mitigate the language drift encountered when fine-tuning a multilingual model."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "In Experiment 2, after how many training steps did multilingual outputs begin to reemerge, suggesting the persistence of language drift?",
              "answer": "The summary states that multilingual language drift occurs with prolonged training but does not specify the number of training steps after which it reemerged in Experiment 2."
            },
            "result": false,
            "issue": "The article states in section 3.5.2: 'However, after 150–200 steps (approximately 4000 local steps), performance declines, and KL divergence becomes unstable (Figure 4), with mixed-language outputs reemerging.'"
          },
          {
            "qa": {
              "question": "ACU. What is the name of the multilingual base model that was fine-tuned and exhibited language drift?",
              "answer": "The multilingual base model that was fine-tuned and exhibited language drift is DeepSeek-R1-Distill-Qwen-1.5B."
            },
            "result": true,
            "issue": ""
          }
        ],
        "judgment": false,
        "sections_to_highlight": []
      },
      "correct_count_all": 2,
      "correct_count_acu": 1,
      "num_of_questions": 5,
      "sections_to_highlight": [],
      "sections_to_highlight_size": 0,
      "qa_failures_present": true
    },
    {
      "iteration_number": 10,
      "summary": "1. SUMMARY:\nThis study investigates the effectiveness of reinforcement learning (RL) for enhancing the reasoning capabilities of small Large Language Models (LLMs) under strict resource constraints. The researchers fine-tuned a 1.5 billion parameter model, DeepSeek-R1-Distill-Qwen-1.5B, using the Group Relative Policy Optimization (GRPO) algorithm and a curated dataset of mathematical reasoning problems. The primary challenge addressed was managing multilingual language drift when fine-tuning for a monolingual reasoning task. Experiments revealed that while RL can rapidly improve reasoning performance (e.g., AMC23 accuracy from 63% to 80%) with minimal data ($42 cost), prolonged training led to optimization instability and language drift, particularly with the multilingual base model. A non-RL method using a cosine reward helped stabilize output length but did not fully resolve the language drift. Future work suggests extending training duration and incorporating explicit language controls to mitigate these issues. The research highlights a trade-off between efficiency and reasoning depth, offering a cost-effective alternative to large-scale methods despite the complexities of multilingual base models.\n\n2. KEY HIGHLIGHTS:\n*   Fine-tuning a multilingual 1.5B LLM for monolingual reasoning using RL showed rapid initial gains but suffered from optimization instability and language drift with prolonged training.\n*   A cosine reward function helped control output length in Experiment 3, improving training consistency but not fully resolving language drift.\n*   The multilingual nature of the base model (DeepSeek-R1-Distill-Qwen-1.5B) was identified as a complicating factor for monolingual optimization, leading to unintended language drift.\n*   The study achieved competitive reasoning performance on benchmarks like AIME24 (46.7%) and AMC23 (80%) with significantly lower costs ($42) compared to larger models.\n*   Future directions include extending training duration and implementing explicit language control mechanisms to address observed limitations.",
      "qa_evaluations": [
        {
          "qa": {
            "question": "What specific evidence of multilingual language drift was observed after 200 training steps in Experiment 1?",
            "answer": "Not enough information in summary"
          },
          "result": false,
          "issue": "Not enough information"
        },
        {
          "qa": {
            "question": "According to the 'Limitations' section, how did the inherent multilingual nature of the base model complicate the process of monolingual optimization for the reasoning task?",
            "answer": "The inherent multilingual nature of the base model complicated the process of monolingual optimization for the reasoning task by leading to unintended language drift."
          },
          "result": true,
          "issue": null
        },
        {
          "qa": {
            "question": "What non-RL method was attempted in Experiment 3 to control the language of the model's output, and was it successful?",
            "answer": "A non-RL method using a cosine reward was attempted to control the language of the model's output. It helped stabilize output length but did not fully resolve the language drift."
          },
          "result": true,
          "issue": null
        },
        {
          "qa": {
            "question": "What does the paper propose as a potential future solution to mitigate the language drift encountered when fine-tuning a multilingual model?",
            "answer": "The paper proposes extending training duration and incorporating explicit language controls to mitigate language drift."
          },
          "result": true,
          "issue": null
        },
        {
          "qa": {
            "question": "In Experiment 2, after how many training steps did multilingual outputs begin to reemerge, suggesting the persistence of language drift?",
            "answer": "Not enough information in summary"
          },
          "result": false,
          "issue": "Not enough information"
        },
        {
          "qa": {
            "question": "ACU. What is the name of the multilingual base model that was fine-tuned and exhibited language drift?",
            "answer": "The name of the multilingual base model that was fine-tuned and exhibited language drift is DeepSeek-R1-Distill-Qwen-1.5B."
          },
          "result": true,
          "issue": null
        }
      ],
      "qa_pairs": [
        {
          "question": "What specific evidence of multilingual language drift was observed after 200 training steps in Experiment 1?",
          "answer": "Not enough information in summary"
        },
        {
          "question": "According to the 'Limitations' section, how did the inherent multilingual nature of the base model complicate the process of monolingual optimization for the reasoning task?",
          "answer": "The inherent multilingual nature of the base model complicated the process of monolingual optimization for the reasoning task by leading to unintended language drift."
        },
        {
          "question": "What non-RL method was attempted in Experiment 3 to control the language of the model's output, and was it successful?",
          "answer": "A non-RL method using a cosine reward was attempted to control the language of the model's output. It helped stabilize output length but did not fully resolve the language drift."
        },
        {
          "question": "What does the paper propose as a potential future solution to mitigate the language drift encountered when fine-tuning a multilingual model?",
          "answer": "The paper proposes extending training duration and incorporating explicit language controls to mitigate language drift."
        },
        {
          "question": "In Experiment 2, after how many training steps did multilingual outputs begin to reemerge, suggesting the persistence of language drift?",
          "answer": "Not enough information in summary"
        },
        {
          "question": "ACU. What is the name of the multilingual base model that was fine-tuned and exhibited language drift?",
          "answer": "The name of the multilingual base model that was fine-tuned and exhibited language drift is DeepSeek-R1-Distill-Qwen-1.5B."
        }
      ],
      "judge": {
        "evaluations": [
          {
            "qa": {
              "question": "What specific evidence of multilingual language drift was observed after 200 training steps in Experiment 1?",
              "answer": "Not enough information in summary"
            },
            "result": false,
            "issue": "The article states that after 200 steps, there were 'unreadable content and non-English outputs' in Experiment 1, which is specific evidence of language drift."
          },
          {
            "qa": {
              "question": "According to the 'Limitations' section, how did the inherent multilingual nature of the base model complicate the process of monolingual optimization for the reasoning task?",
              "answer": "The inherent multilingual nature of the base model complicated the process of monolingual optimization for the reasoning task by leading to unintended language drift."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "What non-RL method was attempted in Experiment 3 to control the language of the model's output, and was it successful?",
              "answer": "A non-RL method using a cosine reward was attempted to control the language of the model's output. It helped stabilize output length but did not fully resolve the language drift."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "What does the paper propose as a potential future solution to mitigate the language drift encountered when fine-tuning a multilingual model?",
              "answer": "The paper proposes extending training duration and incorporating explicit language controls to mitigate language drift."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "In Experiment 2, after how many training steps did multilingual outputs begin to reemerge, suggesting the persistence of language drift?",
              "answer": "Not enough information in summary"
            },
            "result": false,
            "issue": "The article states that in Experiment 2, 'after 150–200 steps... mixed-language outputs reemerging,' which is specific information about when language drift reappeared."
          },
          {
            "qa": {
              "question": "ACU. What is the name of the multilingual base model that was fine-tuned and exhibited language drift?",
              "answer": "The name of the multilingual base model that was fine-tuned and exhibited language drift is DeepSeek-R1-Distill-Qwen-1.5B."
            },
            "result": true,
            "issue": ""
          }
        ],
        "judgment": false,
        "sections_to_highlight": [
          "Experiment 1 — evidence of language drift",
          "Experiment 2 — timing of language drift",
          "Methodology — dataset curation",
          "Results — performance gains and limitations",
          "Limitations — impact of multilingual base model",
          "Conclusion — future directions for mitigating language drift"
        ]
      },
      "correct_count_all": 4,
      "correct_count_acu": 1,
      "num_of_questions": 5,
      "sections_to_highlight": [
        "Experiment 1 — evidence of language drift",
        "Experiment 2 — timing of language drift",
        "Methodology — dataset curation",
        "Results — performance gains and limitations",
        "Limitations — impact of multilingual base model",
        "Conclusion — future directions for mitigating language drift"
      ],
      "sections_to_highlight_size": 6,
      "qa_failures_present": true
    },
    {
      "iteration_number": 11,
      "summary": "1. SUMMARY:\nThis paper investigates the use of Reinforcement Learning (RL) to enhance the reasoning capabilities of small Large Language Models (LLMs), specifically focusing on the DeepSeek-R1-Distill-Qwen-1.5B model under strict computational constraints. The core challenge addressed is managing multilingual language drift when fine-tuning for a monolingual mathematical reasoning task. The researchers adapted the Group Relative Policy Optimization (GRPO) algorithm and curated a high-quality, compact dataset of 7,000 mathematical reasoning samples. Their experiments revealed that while small LLMs can achieve rapid reasoning gains (e.g., AMC23 accuracy improving from 63% to 80%) with minimal data and cost ($42), prolonged training leads to performance degradation and multilingual language drift. This drift, observed around 150-200 steps, suggests that the multilingual nature of the base model complicates monolingual optimization. Strategies like mixing easier and harder problems and using cosine rewards for length control showed some success in stabilizing training, but did not fully resolve the language drift issue. The study highlights the trade-offs between efficiency and reasoning depth, concluding that further research is needed to address language drift and length constraints for complex tasks.\n\n2. KEY HIGHLIGHTS:\n*   Fine-tuning a multilingual small LLM (DeepSeek-R1-Distill-Qwen-1.5B) for monolingual reasoning tasks using RL results in rapid initial gains but suffers from performance degradation and multilingual language drift after 150-200 training steps.\n*   The multilingual nature of the base model is identified as a complicating factor for achieving consistent monolingual outputs, even with explicit English-only instructions.\n*   While strategies like mixing problem difficulties and using cosine rewards for length control offer some stabilization, they do not entirely mitigate the observed language drift.\n*   The research demonstrates that RL-based fine-tuning can significantly improve reasoning performance in small LLMs with minimal data and cost, but managing length constraints and language drift remains a key challenge.",
      "qa_evaluations": [
        {
          "qa": {
            "question": "What specific evidence of multilingual language drift was observed after 200 training steps in Experiment 1?",
            "answer": "The summary states that prolonged training leads to performance degradation and multilingual language drift, observed around 150-200 steps. It does not specify evidence observed *after* 200 steps in Experiment 1."
          },
          "result": false,
          "issue": "Summary does not specify evidence observed after 200 steps."
        },
        {
          "qa": {
            "question": "According to the 'Limitations' section, how did the inherent multilingual nature of the base model complicate the process of monolingual optimization for the reasoning task?",
            "answer": "The summary states that the inherent multilingual nature of the base model complicates monolingual optimization for the reasoning task, even with explicit English-only instructions."
          },
          "result": true,
          "issue": null
        },
        {
          "qa": {
            "question": "What non-RL method was attempted in Experiment 3 to control the language of the model's output, and was it successful?",
            "answer": "Not enough information in summary"
          },
          "result": false,
          "issue": "Not enough information"
        },
        {
          "qa": {
            "question": "What does the paper propose as a potential future solution to mitigate the language drift encountered when fine-tuning a multilingual model?",
            "answer": "The summary indicates that further research is needed to address language drift, but it does not propose a specific future solution."
          },
          "result": false,
          "issue": "Summary does not propose a specific future solution."
        },
        {
          "qa": {
            "question": "In Experiment 2, after how many training steps did multilingual outputs begin to reemerge, suggesting the persistence of language drift?",
            "answer": "The summary mentions that multilingual language drift was observed around 150-200 steps. It does not specify findings for Experiment 2 separately."
          },
          "result": false,
          "issue": "Summary does not specify findings for Experiment 2 separately."
        },
        {
          "qa": {
            "question": "ACU. What is the name of the multilingual base model that was fine-tuned and exhibited language drift?",
            "answer": "The name of the multilingual base model that was fine-tuned and exhibited language drift is DeepSeek-R1-Distill-Qwen-1.5B."
          },
          "result": true,
          "issue": null
        }
      ],
      "qa_pairs": [
        {
          "question": "What specific evidence of multilingual language drift was observed after 200 training steps in Experiment 1?",
          "answer": "The summary states that prolonged training leads to performance degradation and multilingual language drift, observed around 150-200 steps. It does not specify evidence observed *after* 200 steps in Experiment 1."
        },
        {
          "question": "According to the 'Limitations' section, how did the inherent multilingual nature of the base model complicate the process of monolingual optimization for the reasoning task?",
          "answer": "The summary states that the inherent multilingual nature of the base model complicates monolingual optimization for the reasoning task, even with explicit English-only instructions."
        },
        {
          "question": "What non-RL method was attempted in Experiment 3 to control the language of the model's output, and was it successful?",
          "answer": "Not enough information in summary"
        },
        {
          "question": "What does the paper propose as a potential future solution to mitigate the language drift encountered when fine-tuning a multilingual model?",
          "answer": "The summary indicates that further research is needed to address language drift, but it does not propose a specific future solution."
        },
        {
          "question": "In Experiment 2, after how many training steps did multilingual outputs begin to reemerge, suggesting the persistence of language drift?",
          "answer": "The summary mentions that multilingual language drift was observed around 150-200 steps. It does not specify findings for Experiment 2 separately."
        },
        {
          "question": "ACU. What is the name of the multilingual base model that was fine-tuned and exhibited language drift?",
          "answer": "The name of the multilingual base model that was fine-tuned and exhibited language drift is DeepSeek-R1-Distill-Qwen-1.5B."
        }
      ],
      "judge": {
        "evaluations": [
          {
            "qa": {
              "question": "What specific evidence of multilingual language drift was observed after 200 training steps in Experiment 1?",
              "answer": "The summary states that prolonged training leads to performance degradation and multilingual language drift, observed around 150-200 steps. It does not specify evidence observed *after* 200 steps in Experiment 1."
            },
            "result": false,
            "issue": "The article states in section 3.5.1: 'Post-200 steps, lengths increase again, accompanied by unreadable content and non-English outputs.' This is specific evidence of language drift."
          },
          {
            "qa": {
              "question": "According to the 'Limitations' section, how did the inherent multilingual nature of the base model complicate the process of monolingual optimization for the reasoning task?",
              "answer": "The summary states that the inherent multilingual nature of the base model complicates monolingual optimization for the reasoning task, even with explicit English-only instructions."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "What non-RL method was attempted in Experiment 3 to control the language of the model's output, and was it successful?",
              "answer": "Not enough information in summary"
            },
            "result": false,
            "issue": "The article states in section 3.5.3: 'We also append an instruction to the system prompt: “Reply in English only, do not use other languages”, avoiding a computationally expensive language reward function.' This was an attempt, but the subsequent sentence states: 'After 200 steps, mixed-language content persists, reflecting the multilingual nature of DeepSeek-R1-Distill-Qwen-1.5B.' So it was not fully successful."
          },
          {
            "qa": {
              "question": "What does the paper propose as a potential future solution to mitigate the language drift encountered when fine-tuning a multilingual model?",
              "answer": "The summary indicates that further research is needed to address language drift, but it does not propose a specific future solution."
            },
            "result": false,
            "issue": "The article states in section B.3 Future Directions: 'Second, incorporating a lightweight language reward or monolingual pre-filtering of the base model might mitigate language drift, enhancing output consistency.' This is a proposed solution."
          },
          {
            "qa": {
              "question": "In Experiment 2, after how many training steps did multilingual outputs begin to reemerge, suggesting the persistence of language drift?",
              "answer": "The summary mentions that multilingual language drift was observed around 150-200 steps. It does not specify findings for Experiment 2 separately."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "ACU. What is the name of the multilingual base model that was fine-tuned and exhibited language drift?",
              "answer": "The name of the multilingual base model that was fine-tuned and exhibited language drift is DeepSeek-R1-Distill-Qwen-1.5B."
            },
            "result": true,
            "issue": ""
          }
        ],
        "judgment": false,
        "sections_to_highlight": [
          "Experiment 1 — specific evidence of language drift",
          "Experiment 3 — success of language control methods",
          "Future Directions — proposed solutions for language drift",
          "Methodology — dataset curation details",
          "Results — comparison of Open-RS variants"
        ]
      },
      "correct_count_all": 3,
      "correct_count_acu": 1,
      "num_of_questions": 5,
      "sections_to_highlight": [
        "Experiment 1 — specific evidence of language drift",
        "Experiment 3 — success of language control methods",
        "Future Directions — proposed solutions for language drift",
        "Methodology — dataset curation details",
        "Results — comparison of Open-RS variants"
      ],
      "sections_to_highlight_size": 5,
      "qa_failures_present": true
    },
    {
      "iteration_number": 12,
      "summary": "1. SUMMARY:\nThis research investigates the effectiveness of Reinforcement Learning (RL) for enhancing reasoning in small LLMs (1.5 billion parameters) under severe resource constraints (4 A40 GPUs, 24 hours). The study adapts the Group Relative Policy Optimization (GRPO) algorithm and curates a high-quality mathematical reasoning dataset. Experiments reveal rapid reasoning gains, with AMC23 accuracy jumping from 63% to 80% and AIME24 reaching 46.7% at a low cost ($42). However, prolonged training leads to performance degradation and multilingual language drift, a challenge exacerbated by the base model's inherent multilingual nature. While cosine rewards help control output length, they don't fully resolve language issues. The findings highlight the potential of RL for small LLMs but also underscore the need for strategies to mitigate optimization instability, length constraints, and language drift for monolingual tasks. Future work suggests extending training duration and implementing explicit language control mechanisms.\n\n2. KEY HIGHLIGHTS:\n*   RL-based fine-tuning can significantly improve reasoning in small LLMs (e.g., 1.5B parameters) under strict resource constraints, achieving competitive performance with minimal data and cost.\n*   Prolonged training on small LLMs can lead to performance degradation, optimization instability, and multilingual language drift, particularly when using a multilingual base model for a monolingual task.\n*   While techniques like cosine rewards can help control output length, they do not fully resolve the issue of multilingual language drift in small LLMs.\n*   Future directions to address language drift include extending training duration and implementing explicit language control mechanisms or monolingual pre-filtering.",
      "qa_evaluations": [
        {
          "qa": {
            "question": "What specific evidence of multilingual language drift was observed after 200 training steps in Experiment 1?",
            "answer": "Not enough information in summary"
          },
          "result": false,
          "issue": "Not enough information in summary"
        },
        {
          "qa": {
            "question": "According to the 'Limitations' section, how did the inherent multilingual nature of the base model complicate the process of monolingual optimization for the reasoning task?",
            "answer": "The inherent multilingual nature of the base model exacerbated the challenge of language drift during monolingual optimization for the reasoning task."
          },
          "result": true,
          "issue": null
        },
        {
          "qa": {
            "question": "What non-RL method was attempted in Experiment 3 to control the language of the model's output, and was it successful?",
            "answer": "Not enough information in summary"
          },
          "result": false,
          "issue": "Not enough information in summary"
        },
        {
          "qa": {
            "question": "What does the paper propose as a potential future solution to mitigate the language drift encountered when fine-tuning a multilingual model?",
            "answer": "The paper proposes extending training duration and implementing explicit language control mechanisms or monolingual pre-filtering as potential future solutions to mitigate language drift."
          },
          "result": true,
          "issue": null
        },
        {
          "qa": {
            "question": "In Experiment 2, after how many training steps did multilingual outputs begin to reemerge, suggesting the persistence of language drift?",
            "answer": "Not enough information in summary"
          },
          "result": false,
          "issue": "Not enough information in summary"
        },
        {
          "qa": {
            "question": "ACU. What is the name of the multilingual base model that was fine-tuned and exhibited language drift?",
            "answer": "Not enough information in summary"
          },
          "result": false,
          "issue": "Not enough information in summary"
        }
      ],
      "qa_pairs": [
        {
          "question": "What specific evidence of multilingual language drift was observed after 200 training steps in Experiment 1?",
          "answer": "Not enough information in summary"
        },
        {
          "question": "According to the 'Limitations' section, how did the inherent multilingual nature of the base model complicate the process of monolingual optimization for the reasoning task?",
          "answer": "The inherent multilingual nature of the base model exacerbated the challenge of language drift during monolingual optimization for the reasoning task."
        },
        {
          "question": "What non-RL method was attempted in Experiment 3 to control the language of the model's output, and was it successful?",
          "answer": "Not enough information in summary"
        },
        {
          "question": "What does the paper propose as a potential future solution to mitigate the language drift encountered when fine-tuning a multilingual model?",
          "answer": "The paper proposes extending training duration and implementing explicit language control mechanisms or monolingual pre-filtering as potential future solutions to mitigate language drift."
        },
        {
          "question": "In Experiment 2, after how many training steps did multilingual outputs begin to reemerge, suggesting the persistence of language drift?",
          "answer": "Not enough information in summary"
        },
        {
          "question": "ACU. What is the name of the multilingual base model that was fine-tuned and exhibited language drift?",
          "answer": "Not enough information in summary"
        }
      ],
      "judge": {
        "evaluations": [
          {
            "qa": {
              "question": "What specific evidence of multilingual language drift was observed after 200 training steps in Experiment 1?",
              "answer": "Not enough information in summary"
            },
            "result": false,
            "issue": "The article mentions 'unreadable content and non-English outputs' after 200 steps in Experiment 1, but the summary states 'Not enough information in summary'."
          },
          {
            "qa": {
              "question": "According to the 'Limitations' section, how did the inherent multilingual nature of the base model complicate the process of monolingual optimization for the reasoning task?",
              "answer": "The inherent multilingual nature of the base model exacerbated the challenge of language drift during monolingual optimization for the reasoning task."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "What non-RL method was attempted in Experiment 3 to control the language of the model's output, and was it successful?",
              "answer": "Not enough information in summary"
            },
            "result": false,
            "issue": "The article states that in Experiment 3, an instruction 'Reply in English only, do not use other languages' was appended to the system prompt, which is a non-RL method to control language. The summary should have captured this."
          },
          {
            "qa": {
              "question": "What does the paper propose as a potential future solution to mitigate the language drift encountered when fine-tuning a multilingual model?",
              "answer": "The paper proposes extending training duration and implementing explicit language control mechanisms or monolingual pre-filtering as potential future solutions to mitigate language drift."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "In Experiment 2, after how many training steps did multilingual outputs begin to reemerge, suggesting the persistence of language drift?",
              "answer": "Not enough information in summary"
            },
            "result": false,
            "issue": "The article states that in Experiment 2, 'after 150–200 steps... mixed-language outputs reemerging.' The summary should reflect this."
          },
          {
            "qa": {
              "question": "ACU. What is the name of the multilingual base model that was fine-tuned and exhibited language drift?",
              "answer": "Not enough information in summary"
            },
            "result": false,
            "issue": "The article clearly states the base model is 'DeepSeek-R1-Distill-Qwen-1.5B' and that it is multilingual. The summary should have identified this model."
          }
        ],
        "judgment": false,
        "sections_to_highlight": [
          "Experiment 1 — evidence of language drift",
          "Experiment 3 — language control method",
          "Experiment 2 — timing of language drift",
          "Base Model — multilingual nature and name"
        ]
      },
      "correct_count_all": 2,
      "correct_count_acu": 0,
      "num_of_questions": 5,
      "sections_to_highlight": [
        "Experiment 1 — evidence of language drift",
        "Experiment 3 — language control method",
        "Experiment 2 — timing of language drift",
        "Base Model — multilingual nature and name"
      ],
      "sections_to_highlight_size": 4,
      "qa_failures_present": true
    },
    {
      "iteration_number": 13,
      "summary": "1. SUMMARY:\nThis paper investigates the efficacy of reinforcement learning (RL) for enhancing the reasoning capabilities of small large language models (LLMs) under strict resource constraints. The study focuses on the 1.5-billion-parameter DeepSeek-R1-Distill-Qwen-1.5B model, a multilingual foundation model. The core challenge explored is managing language drift, a phenomenon where the model reverts to non-English outputs during prolonged training, even when fine-tuned for a monolingual task. Experiments reveal that while RL can achieve rapid reasoning gains (e.g., AMC23 accuracy from 63% to 80%), prolonged training or insufficient length constraints exacerbate language drift and optimization instability. The research highlights that mixing easy and hard problems and employing cosine rewards can offer some stability, but explicitly addressing the multilingual nature of the base model is crucial for consistent monolingual performance. The study demonstrates a cost-effective approach to LLM reasoning enhancement, but underscores the trade-offs inherent in using multilingual base models for specialized monolingual tasks.\n\n2. KEY HIGHLIGHTS:\n*   Fine-tuning a multilingual base LLM (DeepSeek-R1-Distill-Qwen-1.5B) for a monolingual reasoning task can lead to language drift, where the model produces non-English outputs, particularly with prolonged training or strict length constraints.\n*   While RL can achieve significant reasoning improvements in small LLMs quickly, performance degradation and language drift occur after certain training thresholds (around 150-200 steps in experiments).\n*   Strategies like mixing problem difficulties and using cosine rewards for length control offer partial stabilization but do not fully resolve the language drift issue stemming from the multilingual nature of the base model.\n*   The study achieved competitive reasoning performance on mathematical benchmarks with minimal data and cost ($42), demonstrating the potential of RL for resource-constrained settings, but acknowledges the persistent challenge of maintaining monolingual consistency.",
      "qa_evaluations": [
        {
          "qa": {
            "question": "What specific evidence of multilingual language drift was observed after 200 training steps in Experiment 1?",
            "answer": "Not enough information in summary"
          },
          "result": false,
          "issue": "Not enough information in summary"
        },
        {
          "qa": {
            "question": "According to the 'Limitations' section, how did the inherent multilingual nature of the base model complicate the process of monolingual optimization for the reasoning task?",
            "answer": "Not enough information in summary"
          },
          "result": false,
          "issue": "Not enough information in summary"
        },
        {
          "qa": {
            "question": "What non-RL method was attempted in Experiment 3 to control the language of the model's output, and was it successful?",
            "answer": "Not enough information in summary"
          },
          "result": false,
          "issue": "Not enough information in summary"
        },
        {
          "qa": {
            "question": "What does the paper propose as a potential future solution to mitigate the language drift encountered when fine-tuning a multilingual model?",
            "answer": "Not enough information in summary"
          },
          "result": false,
          "issue": "Not enough information in summary"
        },
        {
          "qa": {
            "question": "In Experiment 2, after how many training steps did multilingual outputs begin to reemerge, suggesting the persistence of language drift?",
            "answer": "Performance degradation and language drift occur after certain training thresholds (around 150-200 steps in experiments)."
          },
          "result": true,
          "issue": null
        },
        {
          "qa": {
            "question": "ACU. What is the name of the multilingual base model that was fine-tuned and exhibited language drift?",
            "answer": "The name of the multilingual base model that was fine-tuned and exhibited language drift is DeepSeek-R1-Distill-Qwen-1.5B."
          },
          "result": true,
          "issue": null
        }
      ],
      "qa_pairs": [
        {
          "question": "What specific evidence of multilingual language drift was observed after 200 training steps in Experiment 1?",
          "answer": "Not enough information in summary"
        },
        {
          "question": "According to the 'Limitations' section, how did the inherent multilingual nature of the base model complicate the process of monolingual optimization for the reasoning task?",
          "answer": "Not enough information in summary"
        },
        {
          "question": "What non-RL method was attempted in Experiment 3 to control the language of the model's output, and was it successful?",
          "answer": "Not enough information in summary"
        },
        {
          "question": "What does the paper propose as a potential future solution to mitigate the language drift encountered when fine-tuning a multilingual model?",
          "answer": "Not enough information in summary"
        },
        {
          "question": "In Experiment 2, after how many training steps did multilingual outputs begin to reemerge, suggesting the persistence of language drift?",
          "answer": "Performance degradation and language drift occur after certain training thresholds (around 150-200 steps in experiments)."
        },
        {
          "question": "ACU. What is the name of the multilingual base model that was fine-tuned and exhibited language drift?",
          "answer": "The name of the multilingual base model that was fine-tuned and exhibited language drift is DeepSeek-R1-Distill-Qwen-1.5B."
        }
      ],
      "judge": {
        "evaluations": [
          {
            "qa": {
              "question": "What specific evidence of multilingual language drift was observed after 200 training steps in Experiment 1?",
              "answer": "Not enough information in summary"
            },
            "result": false,
            "issue": "The article mentions 'non-English outputs' after 200 steps in Experiment 1, but the summary states there is not enough information."
          },
          {
            "qa": {
              "question": "According to the 'Limitations' section, how did the inherent multilingual nature of the base model complicate the process of monolingual optimization for the reasoning task?",
              "answer": "Not enough information in summary"
            },
            "result": false,
            "issue": "The article's 'Limitations' section states that the multilingual nature of the base model 'complicates monolingual optimization', but the summary claims insufficient information."
          },
          {
            "qa": {
              "question": "What non-RL method was attempted in Experiment 3 to control the language of the model's output, and was it successful?",
              "answer": "Not enough information in summary"
            },
            "result": false,
            "issue": "The article states that in Experiment 3, an instruction was appended to the system prompt ('Reply in English only, do not use other languages') to avoid a language reward function, and this helped stabilize completion lengths but did not fully resolve the language issue. The summary incorrectly states there is not enough information."
          },
          {
            "qa": {
              "question": "What does the paper propose as a potential future solution to mitigate the language drift encountered when fine-tuning a multilingual model?",
              "answer": "Not enough information in summary"
            },
            "result": false,
            "issue": "The 'Future Directions' section of the article proposes that 'incorporating a lightweight language reward or monolingual pre-filtering of the base model might mitigate language drift'. The summary incorrectly states there is not enough information."
          },
          {
            "qa": {
              "question": "In Experiment 2, after how many training steps did multilingual outputs begin to reemerge, suggesting the persistence of language drift?",
              "answer": "Performance degradation and language drift occur after certain training thresholds (around 150-200 steps in experiments)."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "ACU. What is the name of the multilingual base model that was fine-tuned and exhibited language drift?",
              "answer": "The name of the multilingual base model that was fine-tuned and exhibited language drift is DeepSeek-R1-Distill-Qwen-1.5B."
            },
            "result": true,
            "issue": ""
          }
        ],
        "judgment": false,
        "sections_to_highlight": []
      },
      "correct_count_all": 2,
      "correct_count_acu": 1,
      "num_of_questions": 5,
      "sections_to_highlight": [],
      "sections_to_highlight_size": 0,
      "qa_failures_present": true
    },
    {
      "iteration_number": 14,
      "summary": "1. SUMMARY:\nThis study investigates enhancing the reasoning capabilities of small LLMs (1.5B parameters) using Reinforcement Learning (RL) under strict computational constraints, specifically addressing the challenge of multilingual language drift. The researchers adapted the GRPO algorithm and curated a high-quality mathematical reasoning dataset, training on limited hardware (4x A40 GPUs) within 24 hours. Experiments revealed that while RL can achieve rapid reasoning gains (e.g., AMC23 accuracy from 63% to 80%) with minimal data and cost ($42), prolonged training leads to performance degradation and multilingual language drift, particularly with the DeepSeek-R1-Distill-Qwen-1.5B base model. The study identified that mixing easier and harder problems and using a cosine reward for length control could improve initial stability but did not fully resolve the language drift issue. Future directions suggest extending training duration and employing explicit monolingual controls to mitigate this drift, offering a cost-effective path to reasoning-capable LLMs for resource-constrained environments.\n\n2. KEY HIGHLIGHTS:\n*   Fine-tuning a small (1.5B) multilingual LLM with RL can achieve significant reasoning gains (e.g., AMC23 from 63% to 80%) under strict resource constraints (4x A40 GPUs, 24h, $42 cost).\n*   Prolonged training or exceeding a critical step count (around 150-200 steps) on the multilingual base model (DeepSeek-R1-Distill-Qwen-1.5B) leads to performance degradation and multilingual language drift.\n*   While mixing problem difficulties and using cosine rewards for length control improved initial stability and reduced output length, they did not entirely eliminate the multilingual language drift.\n*   The multilingual nature of the base model is identified as a complicating factor, requiring future work to focus on explicit monolingual controls or extended training to fully mitigate language drift.",
      "qa_evaluations": [
        {
          "qa": {
            "question": "What specific evidence of multilingual language drift was observed after 200 training steps in Experiment 1?",
            "answer": "The summary states that prolonged training or exceeding a critical step count (around 150-200 steps) leads to multilingual language drift, but does not specify the evidence observed after 200 steps in Experiment 1."
          },
          "result": false,
          "issue": "Not enough information in summary"
        },
        {
          "qa": {
            "question": "According to the 'Limitations' section, how did the inherent multilingual nature of the base model complicate the process of monolingual optimization for the reasoning task?",
            "answer": "The summary mentions that the multilingual nature of the base model is a complicating factor for monolingual optimization, but does not detail how it complicates the process."
          },
          "result": false,
          "issue": "Not enough information in summary"
        },
        {
          "qa": {
            "question": "What non-RL method was attempted in Experiment 3 to control the language of the model's output, and was it successful?",
            "answer": "The summary mentions that mixing easier and harder problems and using a cosine reward for length control were attempted to improve initial stability and reduce output length, but it does not specify if these were non-RL methods from Experiment 3 or if they were successful in controlling the language of the model's output."
          },
          "result": false,
          "issue": "Not enough information in summary"
        },
        {
          "qa": {
            "question": "What does the paper propose as a potential future solution to mitigate the language drift encountered when fine-tuning a multilingual model?",
            "answer": "The paper proposes extending training duration and employing explicit monolingual controls to mitigate the language drift encountered when fine-tuning a multilingual model."
          },
          "result": true,
          "issue": null
        },
        {
          "qa": {
            "question": "In Experiment 2, after how many training steps did multilingual outputs begin to reemerge, suggesting the persistence of language drift?",
            "answer": "The summary states that prolonged training or exceeding a critical step count (around 150-200 steps) leads to multilingual language drift, but does not specify when multilingual outputs began to reemerge in Experiment 2."
          },
          "result": false,
          "issue": "Not enough information in summary"
        },
        {
          "qa": {
            "question": "ACU. What is the name of the multilingual base model that was fine-tuned and exhibited language drift?",
            "answer": "The multilingual base model that was fine-tuned and exhibited language drift is DeepSeek-R1-Distill-Qwen-1.5B."
          },
          "result": true,
          "issue": null
        }
      ],
      "qa_pairs": [
        {
          "question": "What specific evidence of multilingual language drift was observed after 200 training steps in Experiment 1?",
          "answer": "The summary states that prolonged training or exceeding a critical step count (around 150-200 steps) leads to multilingual language drift, but does not specify the evidence observed after 200 steps in Experiment 1."
        },
        {
          "question": "According to the 'Limitations' section, how did the inherent multilingual nature of the base model complicate the process of monolingual optimization for the reasoning task?",
          "answer": "The summary mentions that the multilingual nature of the base model is a complicating factor for monolingual optimization, but does not detail how it complicates the process."
        },
        {
          "question": "What non-RL method was attempted in Experiment 3 to control the language of the model's output, and was it successful?",
          "answer": "The summary mentions that mixing easier and harder problems and using a cosine reward for length control were attempted to improve initial stability and reduce output length, but it does not specify if these were non-RL methods from Experiment 3 or if they were successful in controlling the language of the model's output."
        },
        {
          "question": "What does the paper propose as a potential future solution to mitigate the language drift encountered when fine-tuning a multilingual model?",
          "answer": "The paper proposes extending training duration and employing explicit monolingual controls to mitigate the language drift encountered when fine-tuning a multilingual model."
        },
        {
          "question": "In Experiment 2, after how many training steps did multilingual outputs begin to reemerge, suggesting the persistence of language drift?",
          "answer": "The summary states that prolonged training or exceeding a critical step count (around 150-200 steps) leads to multilingual language drift, but does not specify when multilingual outputs began to reemerge in Experiment 2."
        },
        {
          "question": "ACU. What is the name of the multilingual base model that was fine-tuned and exhibited language drift?",
          "answer": "The multilingual base model that was fine-tuned and exhibited language drift is DeepSeek-R1-Distill-Qwen-1.5B."
        }
      ],
      "judge": {
        "evaluations": [
          {
            "qa": {
              "question": "What specific evidence of multilingual language drift was observed after 200 training steps in Experiment 1?",
              "answer": "The summary states that prolonged training or exceeding a critical step count (around 150-200 steps) leads to multilingual language drift, but does not specify the evidence observed after 200 steps in Experiment 1."
            },
            "result": false,
            "issue": "The answer correctly identifies that the summary does not provide specific evidence for Experiment 1 after 200 steps. The article mentions 'unreadable content and non-English outputs' after 200 steps in Experiment 1, which is specific evidence."
          },
          {
            "qa": {
              "question": "According to the 'Limitations' section, how did the inherent multilingual nature of the base model complicate the process of monolingual optimization for the reasoning task?",
              "answer": "The summary mentions that the multilingual nature of the base model is a complicating factor for monolingual optimization, but does not detail how it complicates the process."
            },
            "result": false,
            "issue": "The answer is incomplete. The article states that the multilingual nature 'complicates monolingual optimization' by reflecting a trade-off in using a pre-trained, multilingual foundation."
          },
          {
            "qa": {
              "question": "What non-RL method was attempted in Experiment 3 to control the language of the model's output, and was it successful?",
              "answer": "The summary mentions that mixing easier and harder problems and using a cosine reward for length control were attempted to improve initial stability and reduce output length, but it does not specify if these were non-RL methods from Experiment 3 or if they were successful in controlling the language of the model's output."
            },
            "result": false,
            "issue": "The answer incorrectly identifies the methods from Experiment 2 and fails to state that Experiment 3 attempted to control language by appending an instruction to the system prompt ('Reply in English only, do not use other languages') and that this was only partially successful."
          },
          {
            "qa": {
              "question": "What does the paper propose as a potential future solution to mitigate the language drift encountered when fine-tuning a multilingual model?",
              "answer": "The paper proposes extending training duration and employing explicit monolingual controls to mitigate the language drift encountered when fine-tuning a multilingual model."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "In Experiment 2, after how many training steps did multilingual outputs begin to reemerge, suggesting the persistence of language drift?",
              "answer": "The summary states that prolonged training or exceeding a critical step count (around 150-200 steps) leads to multilingual language drift, but does not specify when multilingual outputs began to reemerge in Experiment 2."
            },
            "result": false,
            "issue": "The answer is incomplete. The article states that 'after 150–200 steps... mixed-language outputs reemerging' in Experiment 2."
          },
          {
            "qa": {
              "question": "ACU. What is the name of the multilingual base model that was fine-tuned and exhibited language drift?",
              "answer": "The multilingual base model that was fine-tuned and exhibited language drift is DeepSeek-R1-Distill-Qwen-1.5B."
            },
            "result": true,
            "issue": ""
          }
        ],
        "judgment": false,
        "sections_to_highlight": [
          "Experiment 1 — specific evidence of language drift",
          "Limitations — complication of monolingual optimization",
          "Experiment 3 — language control method and success",
          "Experiment 2 — timing of language drift emergence",
          "Methodology — dataset curation details"
        ]
      },
      "correct_count_all": 2,
      "correct_count_acu": 1,
      "num_of_questions": 5,
      "sections_to_highlight": [
        "Experiment 1 — specific evidence of language drift",
        "Limitations — complication of monolingual optimization",
        "Experiment 3 — language control method and success",
        "Experiment 2 — timing of language drift emergence",
        "Methodology — dataset curation details"
      ],
      "sections_to_highlight_size": 5,
      "qa_failures_present": true
    },
    {
      "iteration_number": 15,
      "summary": "1. SUMMARY:\nThis research investigates the efficacy of reinforcement learning (RL) for enhancing reasoning in small LLMs, specifically a 1.5B parameter model, under strict computational constraints. The study addresses the challenge of multilingual language drift when fine-tuning for monolingual tasks. The researchers adapted the GRPO algorithm and curated a compact, high-quality mathematical reasoning dataset. Experiments revealed that while RL can achieve rapid reasoning gains (e.g., AMC23 accuracy from 63% to 80%) with minimal data and cost ($42), prolonged training led to performance degradation and multilingual output emergence. This drift is attributed to the base model's multilingual nature and inherent limitations in managing extended reasoning chains within fixed token limits. Strategies like mixing easier problems and using cosine rewards for length control showed partial success but did not fully resolve the language drift. Future work suggests extending training durations and employing explicit language control mechanisms to mitigate these issues, aiming for more robust, monolingual reasoning capabilities in resource-constrained settings.\n\n2. KEY HIGHLIGHTS:\n*   Fine-tuning a small, multilingual LLM for a monolingual reasoning task using RL can lead to rapid performance gains but also suffers from multilingual language drift and performance degradation with prolonged training.\n*   The multilingual nature of the base model (DeepSeek-R1-Distill-Qwen-1.5B) complicates monolingual optimization, causing unintended language shift and unreadable content after a certain training threshold.\n*   While methods like mixing easier problems and using cosine rewards for length control can stabilize training and improve early performance, they do not entirely resolve the language drift issue without further interventions.\n*   The study demonstrates that RL-based fine-tuning is a cost-effective and data-efficient approach for small LLMs, achieving competitive results with significantly fewer resources than larger models, but addressing language drift remains a key challenge.",
      "qa_evaluations": [
        {
          "qa": {
            "question": "What specific evidence of multilingual language drift was observed after 200 training steps in Experiment 1?",
            "answer": "Not enough information in summary"
          },
          "result": false,
          "issue": "Not enough information"
        },
        {
          "qa": {
            "question": "According to the 'Limitations' section, how did the inherent multilingual nature of the base model complicate the process of monolingual optimization for the reasoning task?",
            "answer": "The inherent multilingual nature of the base model complicated monolingual optimization by causing unintended language shift and unreadable content after a certain training threshold."
          },
          "result": true,
          "issue": null
        },
        {
          "qa": {
            "question": "What non-RL method was attempted in Experiment 3 to control the language of the model's output, and was it successful?",
            "answer": "Not enough information in summary"
          },
          "result": false,
          "issue": "Not enough information"
        },
        {
          "qa": {
            "question": "What does the paper propose as a potential future solution to mitigate the language drift encountered when fine-tuning a multilingual model?",
            "answer": "The paper proposes extending training durations and employing explicit language control mechanisms to mitigate language drift."
          },
          "result": true,
          "issue": null
        },
        {
          "qa": {
            "question": "In Experiment 2, after how many training steps did multilingual outputs begin to reemerge, suggesting the persistence of language drift?",
            "answer": "Not enough information in summary"
          },
          "result": false,
          "issue": "Not enough information"
        },
        {
          "qa": {
            "question": "ACU. What is the name of the multilingual base model that was fine-tuned and exhibited language drift?",
            "answer": "The name of the multilingual base model that was fine-tuned and exhibited language drift is DeepSeek-R1-Distill-Qwen-1.5B."
          },
          "result": true,
          "issue": null
        }
      ],
      "qa_pairs": [
        {
          "question": "What specific evidence of multilingual language drift was observed after 200 training steps in Experiment 1?",
          "answer": "Not enough information in summary"
        },
        {
          "question": "According to the 'Limitations' section, how did the inherent multilingual nature of the base model complicate the process of monolingual optimization for the reasoning task?",
          "answer": "The inherent multilingual nature of the base model complicated monolingual optimization by causing unintended language shift and unreadable content after a certain training threshold."
        },
        {
          "question": "What non-RL method was attempted in Experiment 3 to control the language of the model's output, and was it successful?",
          "answer": "Not enough information in summary"
        },
        {
          "question": "What does the paper propose as a potential future solution to mitigate the language drift encountered when fine-tuning a multilingual model?",
          "answer": "The paper proposes extending training durations and employing explicit language control mechanisms to mitigate language drift."
        },
        {
          "question": "In Experiment 2, after how many training steps did multilingual outputs begin to reemerge, suggesting the persistence of language drift?",
          "answer": "Not enough information in summary"
        },
        {
          "question": "ACU. What is the name of the multilingual base model that was fine-tuned and exhibited language drift?",
          "answer": "The name of the multilingual base model that was fine-tuned and exhibited language drift is DeepSeek-R1-Distill-Qwen-1.5B."
        }
      ],
      "judge": {
        "evaluations": [
          {
            "qa": {
              "question": "What specific evidence of multilingual language drift was observed after 200 training steps in Experiment 1?",
              "answer": "Not enough information in summary"
            },
            "result": false,
            "issue": "The article states in section 3.5.1 that 'After 200 steps, accuracy degrades significantly... accompanied by unreadable content and non-English outputs.' This information is not present in the summary."
          },
          {
            "qa": {
              "question": "According to the 'Limitations' section, how did the inherent multilingual nature of the base model complicate the process of monolingual optimization for the reasoning task?",
              "answer": "The inherent multilingual nature of the base model complicated monolingual optimization by causing unintended language shift and unreadable content after a certain training threshold."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "What non-RL method was attempted in Experiment 3 to control the language of the model's output, and was it successful?",
              "answer": "Not enough information in summary"
            },
            "result": false,
            "issue": "The article states in section 3.5.3 that an instruction was appended to the system prompt: “Reply in English only, do not use other languages”. The success is described as 'The cosine reward effectively regulates length, but the language issue suggests a need for explicit language constraints...' which implies partial success but persistent issues."
          },
          {
            "qa": {
              "question": "What does the paper propose as a potential future solution to mitigate the language drift encountered when fine-tuning a multilingual model?",
              "answer": "The paper proposes extending training durations and employing explicit language control mechanisms to mitigate language drift."
            },
            "result": true,
            "issue": ""
          },
          {
            "qa": {
              "question": "In Experiment 2, after how many training steps did multilingual outputs begin to reemerge, suggesting the persistence of language drift?",
              "answer": "Not enough information in summary"
            },
            "result": false,
            "issue": "The article states in section 3.5.2 that 'after 150–200 steps (approximately 4000 local steps), performance declines, and KL divergence becomes unstable (Figure 4), with mixed-language outputs reemerging.'"
          },
          {
            "qa": {
              "question": "ACU. What is the name of the multilingual base model that was fine-tuned and exhibited language drift?",
              "answer": "The name of the multilingual base model that was fine-tuned and exhibited language drift is DeepSeek-R1-Distill-Qwen-1.5B."
            },
            "result": true,
            "issue": ""
          }
        ],
        "judgment": false,
        "sections_to_highlight": [
          "Experiment 1 — specific evidence of language drift",
          "Experiment 3 — success of language control method",
          "Experiment 2 — training steps for language drift emergence",
          "Methodology — dataset curation details",
          "Limitations — impact of length constraints",
          "Conclusion — future work on language drift"
        ]
      },
      "correct_count_all": 3,
      "correct_count_acu": 1,
      "num_of_questions": 5,
      "sections_to_highlight": [
        "Experiment 1 — specific evidence of language drift",
        "Experiment 3 — success of language control method",
        "Experiment 2 — training steps for language drift emergence",
        "Methodology — dataset curation details",
        "Limitations — impact of length constraints",
        "Conclusion — future work on language drift"
      ],
      "sections_to_highlight_size": 6,
      "qa_failures_present": true
    }
  ],
  "final_summary": "1. SUMMARY:\nThis research investigates the efficacy of reinforcement learning (RL) for enhancing reasoning in small LLMs, specifically a 1.5B parameter model, under strict computational constraints. The study addresses the challenge of multilingual language drift when fine-tuning for monolingual tasks. The researchers adapted the GRPO algorithm and curated a compact, high-quality mathematical reasoning dataset. Experiments revealed that while RL can achieve rapid reasoning gains (e.g., AMC23 accuracy from 63% to 80%) with minimal data and cost ($42), prolonged training led to performance degradation and multilingual output emergence. This drift is attributed to the base model's multilingual nature and inherent limitations in managing extended reasoning chains within fixed token limits. Strategies like mixing easier problems and using cosine rewards for length control showed partial success but did not fully resolve the language drift. Future work suggests extending training durations and employing explicit language control mechanisms to mitigate these issues, aiming for more robust, monolingual reasoning capabilities in resource-constrained settings.\n\n2. KEY HIGHLIGHTS:\n*   Fine-tuning a small, multilingual LLM for a monolingual reasoning task using RL can lead to rapid performance gains but also suffers from multilingual language drift and performance degradation with prolonged training.\n*   The multilingual nature of the base model (DeepSeek-R1-Distill-Qwen-1.5B) complicates monolingual optimization, causing unintended language shift and unreadable content after a certain training threshold.\n*   While methods like mixing easier problems and using cosine rewards for length control can stabilize training and improve early performance, they do not entirely resolve the language drift issue without further interventions.\n*   The study demonstrates that RL-based fine-tuning is a cost-effective and data-efficient approach for small LLMs, achieving competitive results with significantly fewer resources than larger models, but addressing language drift remains a key challenge.",
  "total_iterations": 15,
  "status": "max_iterations_reached",
  "questions": [
    "What specific evidence of multilingual language drift was observed after 200 training steps in Experiment 1?",
    "According to the 'Limitations' section, how did the inherent multilingual nature of the base model complicate the process of monolingual optimization for the reasoning task?",
    "What non-RL method was attempted in Experiment 3 to control the language of the model's output, and was it successful?",
    "What does the paper propose as a potential future solution to mitigate the language drift encountered when fine-tuning a multilingual model?",
    "In Experiment 2, after how many training steps did multilingual outputs begin to reemerge, suggesting the persistence of language drift?"
  ],
  "acu_questions": [
    "ACU. What is the name of the multilingual base model that was fine-tuned and exhibited language drift?"
  ]
}